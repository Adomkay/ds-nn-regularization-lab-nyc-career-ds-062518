{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization and Optimization Lab\n",
    "\n",
    "## Objective\n",
    "\n",
    "In this lab, we'll gain experience detecting and dealing with a ANN model that is overfitting using various regularization and hyperparameter tuning techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "In this lab, we'll work with a large dataset of customer complaints to a bank, with the goal of predicting what product the customer is complaining about based on the text of their complaint.  There are 7 different possible products that we can predict, making this a multi-class classification task. \n",
    "\n",
    "\n",
    "#### Preprocessing our Data Set\n",
    "We'll start by preprocessing our dataset by tokenizing the complaints and limiting the number of words we consider to reduce dimensionality. \n",
    "\n",
    "#### Building our Tuning our Model\n",
    "Once we have preprocessed our data set, we'll build a model and explore the various ways that we can reduce overfitting using the following strategies:\n",
    "- Early stopping to minimize the discrepancy between train and test accuracy.\n",
    "- L1 and L2 regularization.\n",
    "- Dropout regularization.\n",
    "- Using more data.\n",
    "\n",
    "\n",
    "**_Let's Get Started!_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing the Bank Complaints Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Import the libraries and take a sample\n",
    "\n",
    "Run the cell below to import everything we'll need for this lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn import preprocessing\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import random\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in the cell below, import our data into a DataFrame.  The data is currently stored in `Bank_complaints.csv`.\n",
    "Then, `.describe()` the dataset to get a feel for what we're working with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>60000</td>\n",
       "      <td>60000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>7</td>\n",
       "      <td>59724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I am filing this complaint because Experian ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>11404</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Product                       Consumer complaint narrative\n",
       "count          60000                                              60000\n",
       "unique             7                                              59724\n",
       "top     Student loan  I am filing this complaint because Experian ha...\n",
       "freq           11404                                                 26"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to speed things up during the development process (and also to give us the ability to see how adding more data affects our model performance), we're going to work with a sample of our dataset rather than the whole thing.  The entire dataset consists of 60,000 rows--we're going to build a model using only 10,000 items randomly sampled from this.\n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Get a random sample of `10000` items from our dataset (HINT: use the `df` object's `.sample()` method to make this easy)\n",
    "* Reset the indexes on these samples to `range(10000)`, so that the indices for our rows are sequential and make sense.\n",
    "* Store our labels, which are found in `\"Product\"`, in a different variable.\n",
    "* Store the data, found in `\"Consumer complaint narrative`, in the variable `complaints`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(10000)\n",
    "df.index = range(10000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Tokenizing the Complaints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll only keep 2,000 most common words and use one-hot encoding to quickly vectorize our dataset from text into a format that a neural network can work with. \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Create a `Tokenizer()` object, and set the `num_words` parameter to `2000`.\n",
    "* Call the tokenizer object's `fit_on_texts()` method and pass in our `complaints` variable we created above. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll create some text sequences by calling the `tokenizer` object's `.texts_to_sequences()` method and feeding in our `complaints` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(complaints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll convert our text data from text to a vectorized matrix.  \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Call the `tokenizer` object's `.texts_to_matrix` method, passing in our `complaints` variable, as well as setting the `mode` parameter equal to `'binary'`.\n",
    "* Store the tokenizer's `.word_index` in the appropriate variable.\n",
    "* Check the `np.shape()` of our `one_hot_results`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2000)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results) # Expected Results (10000, 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 One-hot Encoding of the Products Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've tokenized and encoded our text data, we still need to one-hot encode our label data.  \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "\n",
    "* Create a `LabelEncoder` object, which can found inside the `preprocessing` module.\n",
    "* `fit` the label encoder we just created to `product`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check what classes our label encoder found.  Run the cell below to examine a list of classes that `product` contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bank account or service',\n",
       " 'Checking or savings account',\n",
       " 'Consumer Loan',\n",
       " 'Credit card',\n",
       " 'Credit reporting',\n",
       " 'Mortgage',\n",
       " 'Student loan']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " list(le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll need to transform `product` into a numeric vector.  \n",
    "\n",
    "In the cell below, use the label encoder's `.transform` method on `product` to create an integer encoded version of our labels. \n",
    "\n",
    "Then, access `product_cat` to see an example of how the labels are now encoded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 4, 4, ..., 5, 2, 5])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_cat = le.transform(product) \n",
    "product_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to go from integer encoding to one-hot encoding.  Use the `to_categorical` method from keras to do this easily in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_onehot = to_categorical(product_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0., ...,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  1.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  1.,  0.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's check the shape of our one-hot encoded labels to make sure everything worked correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 7)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(product_onehot) # Expected Output: (10000, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Train - test split\n",
    "\n",
    "Now, we'll split our data into training and testing sets.  \n",
    "\n",
    "\n",
    "We'll accomplish this by generating a random list of 1500 different indices between 1 and 10000.  Then, we'll slice these rows and store them as our test set, and delete them from the training set (it's very important to remember to remove them from the training set!)\n",
    "\n",
    "Run the cell below to create a set of random indices for our test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index = random.sample(range(1,10000), 1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now:\n",
    "\n",
    "* Slice the `test_index` rows from `one_hot_results` and store them in `test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = one_hot_results[test_index]\n",
    "\n",
    "# This line returns a version of our one_hot_results that has every item with an index in test_index removed\n",
    "train = np.delete(one_hot_results, test_index, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll need to repeat the splitting process on our labels, making sure that we use the same indices we used to split our data. \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Slice `test_index` from `product_onehot`\n",
    "* Use `np.delete` to remove `test_index` items from `product_onehot` (the syntax is exactly the same above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_test = product_onehot[test_index]\n",
    "label_train = np.delete(product_onehot, test_index, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's examine the shape everything we just did to make sure that the dimensions match up.  \n",
    "\n",
    "In the cell below, use `np.shape` to check the shape of:\n",
    "\n",
    "* `label_test`\n",
    "* `label_train`\n",
    "* `test`\n",
    "* `train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 7)\n",
      "(8500, 7)\n",
      "(1500, 2000)\n",
      "(8500, 2000)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(label_test)) # Expected Output: (1500, 7)\n",
    "print(np.shape(label_train)) # Expected Output: (8500, 7)\n",
    "print(np.shape(test)) # Expected Output: (1500, 2000)\n",
    "print(np.shape(train)) # Expected Output: (8500, 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Running the model using a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Creating the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture we mentioned that in deep learning, we generally keep aside a validation set, which is used during hyperparameter tuning. Then when we have made the final model decision, the test set is used to define the final model perforance. \n",
    "\n",
    "In this example, let's take the first 1000 cases out of the training set to become the validation set. You should do this for both `train` and `label_train`.\n",
    "\n",
    "Run the cell below to create our validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "val = train[:1000]\n",
    "train_final = train[1000:]\n",
    "label_val = label_train[:1000]\n",
    "label_train_final = label_train[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Creating, compiling and running the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rebuild a fully connected (Dense) layer network with relu activations in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we used 2 hidden with 50 units in the first layer and 25 in the second, both with a `relu` activation function. Because we are dealing with a multiclass problem (classifying the complaints into 7 classes), we use a use a softmax classifyer in order to output 7 class probabilities per case.\n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Import `Sequential` from the appropriate module in keras.\n",
    "* Import `Dense` from the appropriate module in keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, build a model with the following specifications in the cell below:\n",
    "\n",
    "* An input layer of shape `(2000,)`\n",
    "* Hidden layer 1: Dense, 50 neurons, relu activation \n",
    "* Hidden layer 2: Dense, 25 neurons, relu activation\n",
    "* Output layer: Dense, 7 neurons, softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, `compile` the model with the following settings:\n",
    "\n",
    "* Optimizer is `\"SGD\"`\n",
    "* Loss is `'categorical_crossentropy'`\n",
    "* metrics is `['accuracy']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, Train the model for 120 epochs in mini-batches of 256 samples. Also pass in `(val, label_val)` to the `validation_data` parameter, so that we see how our model does on the test set after every epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.9645 - acc: 0.1343 - val_loss: 1.9444 - val_acc: 0.1570\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.9377 - acc: 0.1693 - val_loss: 1.9272 - val_acc: 0.1870\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.9227 - acc: 0.1903 - val_loss: 1.9152 - val_acc: 0.2120\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.9109 - acc: 0.2092 - val_loss: 1.9046 - val_acc: 0.2360\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.8994 - acc: 0.2263 - val_loss: 1.8940 - val_acc: 0.2420\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.8874 - acc: 0.2441 - val_loss: 1.8820 - val_acc: 0.2590\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.8741 - acc: 0.2587 - val_loss: 1.8684 - val_acc: 0.2730\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.8587 - acc: 0.2777 - val_loss: 1.8526 - val_acc: 0.2870\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.8410 - acc: 0.2939 - val_loss: 1.8339 - val_acc: 0.3100\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.8202 - acc: 0.3123 - val_loss: 1.8125 - val_acc: 0.3280\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.7963 - acc: 0.3359 - val_loss: 1.7873 - val_acc: 0.3490\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.7683 - acc: 0.3631 - val_loss: 1.7577 - val_acc: 0.3750\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.7361 - acc: 0.3897 - val_loss: 1.7236 - val_acc: 0.4100\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.6990 - acc: 0.4267 - val_loss: 1.6855 - val_acc: 0.4340\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.6577 - acc: 0.4636 - val_loss: 1.6421 - val_acc: 0.4650\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.6122 - acc: 0.4969 - val_loss: 1.5955 - val_acc: 0.4950\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.5633 - acc: 0.5251 - val_loss: 1.5466 - val_acc: 0.5130\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.5117 - acc: 0.5519 - val_loss: 1.4969 - val_acc: 0.5390\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.4581 - acc: 0.5788 - val_loss: 1.4428 - val_acc: 0.5570\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.4040 - acc: 0.5999 - val_loss: 1.3900 - val_acc: 0.5750\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.3500 - acc: 0.6169 - val_loss: 1.3382 - val_acc: 0.5960\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.2972 - acc: 0.6348 - val_loss: 1.2872 - val_acc: 0.6150\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.2455 - acc: 0.6480 - val_loss: 1.2404 - val_acc: 0.6270\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.1962 - acc: 0.6623 - val_loss: 1.1923 - val_acc: 0.6460\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1496 - acc: 0.6733 - val_loss: 1.1483 - val_acc: 0.6550\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.1063 - acc: 0.6807 - val_loss: 1.1091 - val_acc: 0.6580\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.0655 - acc: 0.6875 - val_loss: 1.0740 - val_acc: 0.6660\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0281 - acc: 0.6959 - val_loss: 1.0403 - val_acc: 0.6670\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9933 - acc: 0.7053 - val_loss: 1.0066 - val_acc: 0.6860\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9611 - acc: 0.7121 - val_loss: 0.9799 - val_acc: 0.6820\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9314 - acc: 0.7209 - val_loss: 0.9537 - val_acc: 0.6890\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9039 - acc: 0.7251 - val_loss: 0.9289 - val_acc: 0.6990\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8788 - acc: 0.7319 - val_loss: 0.9097 - val_acc: 0.7010\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8552 - acc: 0.7340 - val_loss: 0.8869 - val_acc: 0.7100\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8338 - acc: 0.7395 - val_loss: 0.8684 - val_acc: 0.7120\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8138 - acc: 0.7425 - val_loss: 0.8517 - val_acc: 0.7200\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7950 - acc: 0.7455 - val_loss: 0.8387 - val_acc: 0.7260\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7777 - acc: 0.7520 - val_loss: 0.8216 - val_acc: 0.7280\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.7614 - acc: 0.7567 - val_loss: 0.8089 - val_acc: 0.7320\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.7463 - acc: 0.7589 - val_loss: 0.7945 - val_acc: 0.7340\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.7318 - acc: 0.7633 - val_loss: 0.7858 - val_acc: 0.7380\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.7180 - acc: 0.7668 - val_loss: 0.7740 - val_acc: 0.7360\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.7056 - acc: 0.7705 - val_loss: 0.7663 - val_acc: 0.7400\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.6934 - acc: 0.7755 - val_loss: 0.7594 - val_acc: 0.7400\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.6827 - acc: 0.7757 - val_loss: 0.7475 - val_acc: 0.7420\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.6712 - acc: 0.7801 - val_loss: 0.7375 - val_acc: 0.7430\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.6612 - acc: 0.7829 - val_loss: 0.7326 - val_acc: 0.7400\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.6511 - acc: 0.7864 - val_loss: 0.7284 - val_acc: 0.7450\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.6421 - acc: 0.7880 - val_loss: 0.7181 - val_acc: 0.7440\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.6333 - acc: 0.7887 - val_loss: 0.7143 - val_acc: 0.7480\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.6245 - acc: 0.7937 - val_loss: 0.7071 - val_acc: 0.7470\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.6164 - acc: 0.7968 - val_loss: 0.7025 - val_acc: 0.7490\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.6086 - acc: 0.7963 - val_loss: 0.6970 - val_acc: 0.7490\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.6013 - acc: 0.8009 - val_loss: 0.6912 - val_acc: 0.7570\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.5936 - acc: 0.8043 - val_loss: 0.6905 - val_acc: 0.7530\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.5873 - acc: 0.8064 - val_loss: 0.6839 - val_acc: 0.7570\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.5796 - acc: 0.8061 - val_loss: 0.6785 - val_acc: 0.7550\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.5729 - acc: 0.8119 - val_loss: 0.6762 - val_acc: 0.7600\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.5665 - acc: 0.8111 - val_loss: 0.6723 - val_acc: 0.7530\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.5602 - acc: 0.8151 - val_loss: 0.6712 - val_acc: 0.7570\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.5546 - acc: 0.8161 - val_loss: 0.6684 - val_acc: 0.7580\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.5485 - acc: 0.8181 - val_loss: 0.6624 - val_acc: 0.7630\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.5428 - acc: 0.8227 - val_loss: 0.6627 - val_acc: 0.7680\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.5376 - acc: 0.8212 - val_loss: 0.6583 - val_acc: 0.7610\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.5320 - acc: 0.8243 - val_loss: 0.6541 - val_acc: 0.7590\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.5265 - acc: 0.8265 - val_loss: 0.6519 - val_acc: 0.7600\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.5209 - acc: 0.8285 - val_loss: 0.6505 - val_acc: 0.7670\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.5159 - acc: 0.8283 - val_loss: 0.6555 - val_acc: 0.7530\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.5117 - acc: 0.8308 - val_loss: 0.6446 - val_acc: 0.7610\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.5065 - acc: 0.8325 - val_loss: 0.6455 - val_acc: 0.7620\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.5019 - acc: 0.8335 - val_loss: 0.6426 - val_acc: 0.7730\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.4972 - acc: 0.8355 - val_loss: 0.6388 - val_acc: 0.7660\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.4927 - acc: 0.8364 - val_loss: 0.6386 - val_acc: 0.7710\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.4882 - acc: 0.8391 - val_loss: 0.6369 - val_acc: 0.7700\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.4834 - acc: 0.8397 - val_loss: 0.6380 - val_acc: 0.7640\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.4798 - acc: 0.8407 - val_loss: 0.6344 - val_acc: 0.7750\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.4751 - acc: 0.8445 - val_loss: 0.6332 - val_acc: 0.7670\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.4712 - acc: 0.8444 - val_loss: 0.6320 - val_acc: 0.7740\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.4669 - acc: 0.8459 - val_loss: 0.6289 - val_acc: 0.7700\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.4629 - acc: 0.8472 - val_loss: 0.6305 - val_acc: 0.7670\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.4590 - acc: 0.8488 - val_loss: 0.6276 - val_acc: 0.7690\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.4548 - acc: 0.8508 - val_loss: 0.6274 - val_acc: 0.7750\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.4511 - acc: 0.8532 - val_loss: 0.6291 - val_acc: 0.7750\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.4477 - acc: 0.8531 - val_loss: 0.6246 - val_acc: 0.7740\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.4440 - acc: 0.8551 - val_loss: 0.6218 - val_acc: 0.7750\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.4402 - acc: 0.8552 - val_loss: 0.6265 - val_acc: 0.7700\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.4368 - acc: 0.8576 - val_loss: 0.6310 - val_acc: 0.7670\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.4332 - acc: 0.8576 - val_loss: 0.6252 - val_acc: 0.7750\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.4293 - acc: 0.8576 - val_loss: 0.6185 - val_acc: 0.7760\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.4260 - acc: 0.8611 - val_loss: 0.6279 - val_acc: 0.7720\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.4227 - acc: 0.8620 - val_loss: 0.6165 - val_acc: 0.7750\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.4193 - acc: 0.8637 - val_loss: 0.6191 - val_acc: 0.7760\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.4157 - acc: 0.8644 - val_loss: 0.6165 - val_acc: 0.7760\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.4126 - acc: 0.8659 - val_loss: 0.6161 - val_acc: 0.7810\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.4095 - acc: 0.8672 - val_loss: 0.6171 - val_acc: 0.7790\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.4061 - acc: 0.8665 - val_loss: 0.6165 - val_acc: 0.7730\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.4029 - acc: 0.8701 - val_loss: 0.6142 - val_acc: 0.7780\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.3997 - acc: 0.8712 - val_loss: 0.6153 - val_acc: 0.7800\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.3966 - acc: 0.8719 - val_loss: 0.6137 - val_acc: 0.7770\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.3938 - acc: 0.8725 - val_loss: 0.6133 - val_acc: 0.7820\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.3910 - acc: 0.8740 - val_loss: 0.6158 - val_acc: 0.7770\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.3876 - acc: 0.8748 - val_loss: 0.6152 - val_acc: 0.7780\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.3847 - acc: 0.8747 - val_loss: 0.6164 - val_acc: 0.7770\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.3822 - acc: 0.8759 - val_loss: 0.6144 - val_acc: 0.7780\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.3787 - acc: 0.8800 - val_loss: 0.6116 - val_acc: 0.7780\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.3764 - acc: 0.8793 - val_loss: 0.6144 - val_acc: 0.7820\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.3732 - acc: 0.8801 - val_loss: 0.6118 - val_acc: 0.7750\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.3704 - acc: 0.8817 - val_loss: 0.6132 - val_acc: 0.7820\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.3680 - acc: 0.8813 - val_loss: 0.6122 - val_acc: 0.7790\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.3650 - acc: 0.8828 - val_loss: 0.6146 - val_acc: 0.7820\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.3626 - acc: 0.8840 - val_loss: 0.6143 - val_acc: 0.7800\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.3595 - acc: 0.8856 - val_loss: 0.6136 - val_acc: 0.7810\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.3571 - acc: 0.8873 - val_loss: 0.6124 - val_acc: 0.7830\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.3546 - acc: 0.8869 - val_loss: 0.6126 - val_acc: 0.7770\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.3516 - acc: 0.8895 - val_loss: 0.6152 - val_acc: 0.7790\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.3495 - acc: 0.8908 - val_loss: 0.6122 - val_acc: 0.7810\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.3465 - acc: 0.8913 - val_loss: 0.6125 - val_acc: 0.7880\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.3439 - acc: 0.8932 - val_loss: 0.6128 - val_acc: 0.7800\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.3416 - acc: 0.8932 - val_loss: 0.6152 - val_acc: 0.7840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.3393 - acc: 0.8939 - val_loss: 0.6119 - val_acc: 0.7850\n"
     ]
    }
   ],
   "source": [
    "model_val = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary `history` contains four entries this time: one per metric that was being monitored during training and during validation.\n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Store the model's `.history` inside of `model_val_dict`\n",
    "* Check what `keys()` this dictionary contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_val_dict = model_val.history\n",
    "model_val_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get the final results on the training and testing sets using `model.evaluate()` on `train_final` and `label_train_final`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 42us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also use this function to get the results on our testing set.  Call the function again, but this time on `test` and `label_test`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 38us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, check the contents of each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.33576024494171142, 0.89600000000000002]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train # Expected Results: [0.33576024494171142, 0.89600000000000002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.72006658554077152, 0.74333333365122478]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test # Expected Results: [0.72006658554077152, 0.74333333365122478]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the results. Let's include the training and the validation loss in the same plot. We'll do the same thing for the training and validation accuracy.\n",
    "\n",
    "Run the cell below to visualize a plot of our training and validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3Xl4VOX1wPHvyYQAshN2khBWBUKAEIEIQhBqwV2rrSjiguWHtdaltaIVpdrWra1Iq7VIQVEUF1q17iUmLGWTfZU9QFhD2NdkkvP7Y26mA2QZkkxmJjmf55nHuXfee+fcuXhP3uW+V1QVY4wxBiAi2AEYY4wJHZYUjDHGeFlSMMYY42VJwRhjjJclBWOMMV6WFIwxxnhZUjCVRkRcInJcROIqsmyoE5F3RGS88z5VRNb6U7YM3xOw30xEskQktaL3a0KPJQVTLOcCU/gqEJFTPsu3X+j+VDVfVeuq6o6KLFsWInKpiCwTkWMi8r2IDAnE95xLVTNUtWtF7EtE5onIXT77DuhvZqoHSwqmWM4Fpq6q1gV2ANf6rJt+bnkRiaz8KMvsNeBToD5wFbAruOEYExosKZgyE5Hficj7IvKeiBwDRohIiogsFJHDIrJHRCaKSA2nfKSIqIjEO8vvOJ9/6fzFvkBE2l5oWefzYSKyUUSOiMhfROS/vn9FF8ENbFePraq6vpRj3SQiQ32Wo0TkoIgkikiEiHwkInud484Qkc7F7GeIiGT6LPcSkRXOMb0H1PT5LFpEvhCRbBE5JCL/FpHWzmcvACnA607NbUIRv1lD53fLFpFMEXlcRMT57F4RmS0iLzsxbxWRK0v6DXziquWciz0isktE/iwiUc5nzZyYDzu/zxyf7Z4Qkd0ictSpnaX6832mcllSMOV1I/Au0AB4H8/F9kGgCdAPGAr8Xwnb3waMAxrjqY08e6FlRaQZ8AHwqPO924DepcS9GPiTiHQvpVyh94DhPsvDgN2quspZ/gzoCLQA1gBvl7ZDEakJfAJMwXNMnwA3+BSJAN4A4oA2QB7wCoCqPgYsAMY4NbeHiviK14CLgHbAFcAoYKTP55cBq4Fo4GXgH6XF7HgKSAYSgZ54zvPjzmePAluBpnh+i3HOsXbF8+8gSVXr4/n9rJkrBFlSMOU1T1X/raoFqnpKVb9T1UWq6lbVrcAkYGAJ23+kqktUNQ+YDvQoQ9lrgBWq+onz2cvAgeJ2IiIj8FzIRgCfi0iis36YiCwqZrN3gRtEpJazfJuzDufY31TVY6p6GhgP9BKROiUcC04MCvxFVfNUdQawvPBDVc1W1X85v+tR4A+U/Fv6HmMN4MfAWCeurXh+lzt8im1R1Smqmg+8BcSISBM/dn87MN6Jbz/wjM9+84BWQJyq5qrqbGe9G6gFdBWRSFXd5sRkQowlBVNeO30XROQSEfncaUo5iueCUdKFZq/P+5NA3TKUbeUbh3pmecwqYT8PAhNV9QvgfuAbJzFcBswqagNV/R7YAlwtInXxJKJ3wTvq50WnCeYosNnZrLQLbCsgS8+elXJ74RsRqSMik0Vkh7Pfb/3YZ6FmgMt3f8771j7L5/6eUPLvX6hlCft93llOE5EtIvIogKpuAH6J59/DfqfJsYWfx2IqkSUFU17nTrP7dzzNJx2cZoKnAAlwDHuAmMIFp928dfHFicTzlyuq+gnwGJ5kMAKYUMJ2hU1IN+KpmWQ660fi6ay+Ak8zWofCUC4kbofvcNJfA22B3s5vecU5ZUua4ng/kI+n2cl33xXRob6nuP2q6lFVfVhV4/E0hT0mIgOdz95R1X54jskFPFcBsZgKZknBVLR6wBHghNPZWlJ/QkX5DEgSkWvFMwLqQTxt2sX5EBgvIt1EJAL4HsgFauNp4ijOe3jawkfj1BIc9YAzQA6eNvzf+xn3PCBCRH7udBLfAiSds9+TwCERicaTYH3tw9NfcB6nGe0j4A8iUtfplH8YeMfP2EryHvCUiDQRkaZ4+g3eAXDOQXsnMR/Bk5jyRaSziAxy+lFOOa/8CojFVDBLCqai/RK4EziGp9bwfqC/UFX3AT8B/oznwtweT9v8mWI2eQGYhmdI6kE8tYN78VzsPheR+sV8TxawBOiLp2O70FRgt/NaC8z3M+4zeGodPwUOATcBH/sU+TOemkeOs88vz9nFBGC4M9Lnz0V8xc/wJLttwGw8/QbT/ImtFL8FVuLppF4FLOJ/f/VfjKeZ6zjwX+AVVZ2HZ1TVi3j6evYCjYAnKyAWU8HEHrJjqhoRceG5QN+sqnODHY8x4cRqCqZKEJGhItLAaZ4Yh6fPYHGQwzIm7FhSMFVFfzzj4w/guTfiBqd5xhhzAaz5yBhjjJfVFIwxxniF0wRmADRp0kTj4+ODHYYxxoSVpUuXHlDVkoZqA2GYFOLj41myZEmwwzDGmLAiIttLL2XNR8YYY3wELCmISKyIpIvIehFZKyIPFlFGnCl4N4vIKhFJKmpfxhhjKkcgm4/cwC9VdZmI1AOWish/VHWdT5lheKYb7gj0Af7m/NcYY0wQBCwpqOoePBNnoarHRGQ9nknKfJPC9cA0Z5bIhc5DQVo62xpjQkBeXh5ZWVmcPn062KEYP9SqVYuYmBhq1KhRpu0rpaPZeRJUTzxzpPhqzdlTL2c5685KCiIyGs8kZMTFhf1z3I0JK1lZWdSrV4/4+HicB7eZEKWq5OTkkJWVRdu2bUvfoAgB72h25p6fCTzkPCjkrI+L2OS8u+lUdZKqJqtqctOmpY6oMsZUoNOnTxMdHW0JIQyICNHR0eWq1QU0KThPf5oJTFfVfxZRJAuI9VmOwTORmTEmhFhCCB/lPVeBHH0keJ75ul5Vi5rWFzxTF490RiH1BY4Eqj9h7/G9PPTVQ+Tm5wZi98YYUyUEsqbQD89zW68QkRXO6yoRGSMiY5wyX+CZxGwzngeU/yxQwczbMY9XFr3CL7/+ZaC+whgTADk5OfTo0YMePXrQokULWrdu7V3OzfXvj7y7776bDRs2lFjm1VdfZfr06RURMv3792fFihUVsq/KFsjRR/Mo5XGEzqij+wMVg6/W9VrTL7Yff/3ur/SJ6cOIxBGV8bXGmHKKjo72XmDHjx9P3bp1+dWvfnVWGVVFVYmIKPrv3KlTp5b6PfffXymXopBXLe5oXrBzAYOnDWZh1kIiJIJRn45ixd7wzOLGGI/NmzeTkJDAmDFjSEpKYs+ePYwePZrk5GS6du3KM8884y1b+Je72+2mYcOGjB07lu7du5OSksL+/fsBePLJJ5kwYYK3/NixY+nduzcXX3wx8+d7HqZ34sQJfvSjH9G9e3eGDx9OcnJyqTWCd955h27dupGQkMATTzwBgNvt5o477vCunzhxIgAvv/wyXbp0oXv37owYEZw/XMNu7qOyyMjMIDc/l3zNxyUuIiWSlH+k8JvLf8OTA+yJgMb466GvHqrwP6h6tOjBhKETyrTtunXrmDp1Kq+//joAzz//PI0bN8btdjNo0CBuvvlmunTpctY2R44cYeDAgTz//PM88sgjTJkyhbFjx563b1Vl8eLFfPrppzzzzDN89dVX/OUvf6FFixbMnDmTlStXkpRU8iQMWVlZPPnkkyxZsoQGDRowZMgQPvvsM5o2bcqBAwdYvXo1AIcPHwbgxRdfZPv27URFRXnXVbZqUVNIjU8lyhWFS1y4Ily41c1p92nGpY8jZXIK49LHsWDngmCHaYy5QO3bt+fSSy/1Lr/33nskJSWRlJTE+vXrWbdu3Xnb1K5dm2HDhgHQq1cvMjMzi9z3TTfddF6ZefPmceuttwLQvXt3unbtWmJ8ixYt4oorrqBJkybUqFGD2267jTlz5tChQwc2bNjAgw8+yNdff02DBg0A6Nq1KyNGjGD69OllvvmsvKpFTSElNoW0kWlkZGaw48gO3lj2hvezhbsWsnDXQl6Y9wKzRs5iQJsBQYzUmNBW1r/oA6VOnTre95s2beKVV15h8eLFNGzYkBEjRhQ5Xj8qKsr73uVy4Xa7i9x3zZo1zytzoQ8lK658dHQ0q1at4ssvv2TixInMnDmTSZMm8fXXXzN79mw++eQTfve737FmzRpcLtcFfWd5VYuaAngSw+OXP87I7iO9tYbIiEginJ8gryCPoe8M5fdzf3/BJ94YE3xHjx6lXr161K9fnz179vD1119X+Hf079+fDz74AIDVq1cXWRPx1bdvX9LT08nJycHtdjNjxgwGDhxIdnY2qsott9zCb3/7W5YtW0Z+fj5ZWVlcccUVvPTSS2RnZ3Py5MkKP4bSVIuagi/fWkP0RdE89NVDnHGfoYACTrlP8eS3TzJ56WR+2OGH3Nn9TlJiU4IdsjHGD0lJSXTp0oWEhATatWtHv379Kvw7HnjgAUaOHEliYiJJSUkkJCR4m36KEhMTwzPPPENqaiqqyrXXXsvVV1/NsmXLGDVqFKqKiPDCCy/gdru57bbbOHbsGAUFBTz22GPUq1evwo+hNGH3jObk5GStyIfsLNi5gPEZ45m1bRYFWoAgqDPTRgQRDO82nPsvvd+Sg6m21q9fT+fOnYMdRkhwu9243W5q1arFpk2buPLKK9m0aRORkaH193VR50xElqpqcmnbVpvmo+KkxKYwPnU8NV01vR3RhU1KBRQwffV0Br45kLk75gY5UmNMsB0/fpx+/frRvXt3fvSjH/H3v/895BJCeVWtoymjopqUTrtPe2sMeQV53DjjRm7rdhvDE4ZbrcGYaqphw4YsXbo02GEElCUFR0psivdi361ZN6atnMbUFVNxF7gREXJO5fCXxX/h9SWvk3FnBpfFXRbkiI0xpuJV++ajoqTEpvC3a/5G+p3pPDvoWe7teS8u8QwLyyvI44b3b2DW1llBjtIYYyqeJYUSnDuMtbCvIftkNle+fSUz180McoTGGFOxLCn4obDPYUi7IUSI5ydTlBH/HMF9n99nd0MbY6oMSwp+OneUUpQrijP5Z3h9yesMemuQJQZjAiQ1NfW8G9EmTJjAz35W8kz7devWBWD37t3cfPPNxe67tCHuEyZMOOsmsquuuqpC5iUaP348f/zjH8u9n4pmSeECFNYYnh30LPf0uMdbaziTf4YpK6YEOTpjqqbhw4czY8aMs9bNmDGD4cOH+7V9q1at+Oijj8r8/ecmhS+++IKGDRuWeX+hLpBPXpsiIvtFZE0xnzcQkX+LyEoRWSsidwcqlopU1HQZAFOXT2XS0klBjs6Y0LBg5wKem/tchdSgb775Zj777DPOnDkDQGZmJrt376Z///4cP36cwYMHk5SURLdu3fjkk0/O2z4zM5OEhAQATp06xa233kpiYiI/+clPOHXqlLfcfffd5512++mnnwZg4sSJ7N69m0GDBjFo0CAA4uPjOXDgAAB//vOfSUhIICEhwTvtdmZmJp07d+anP/0pXbt25corrzzre4qyYsUK+vbtS2JiIjfeeCOHDh3yfn+XLl1ITEz0TsQ3e/Zs70OGevbsybFjx8r82xap8OEUFf0CBgBJwJpiPn8CeMF53xQ4CESVtt9evXppqJi/Y76O+fcYjXo2ShmPMh59f/X7wQ7LmAq1bt26Cyo/f8d8rf272ur6rUtr/662zt8xv9wxXHXVVfrxxx+rqupzzz2nv/rVr1RVNS8vT48cOaKqqtnZ2dq+fXstKChQVdU6deqoquq2bdu0a9euqqr6pz/9Se+++25VVV25cqW6XC797rvvVFU1JydHVVXdbrcOHDhQV65cqaqqbdq00ezsbG8shctLlizRhIQEPX78uB47dky7dOmiy5Yt023btqnL5dLly5erquott9yib7/99nnH9PTTT+tLL72kqqrdunXTjIwMVVUdN26cPvjgg6qq2rJlSz19+rSqqh46dEhVVa+55hqdN2+eqqoeO3ZM8/Lyztt3UecMWKJ+XLsDVlNQ1TnOhb7YIkA951nOdZ2yRU9XGKJSYlOIaxBHfkG+d919n99H9onsIEZlTHD5Pr8kNz+XjMyMcu/TtwnJt+lIVXniiSdITExkyJAh7Nq1i3379hW7nzlz5ngfXpOYmEhiYqL3sw8++ICkpCR69uzJ2rVrS53sbt68edx4443UqVOHunXrctNNNzF3rmfmg7Zt29KjRw+g5Om5wfN8h8OHDzNw4EAA7rzzTubMmeON8fbbb+edd97x3jndr18/HnnkESZOnMjhw4cr/I7qYPYp/BXoDOwGVgMPqmpBUQVFZLSILBGRJdnZoXXB9X1WQ01XTY7mHuXSNy5l7nabFsNUT77/T0S5okiNTy33Pm+44QbS0tJYtmwZp06d8j7cZvr06WRnZ7N06VJWrFhB8+bNi5wu25fn79Czbdu2jT/+8Y+kpaWxatUqrr766lL3oyXMG1c47TaUPD13aT7//HPuv/9+li5dSq9evXC73YwdO5bJkydz6tQp+vbty/fff1+mfRcnmEnhh8AKoBXQA/iriNQvqqCqTlLVZFVNbtq0aWXGWCrfzueJwyYSIRFsP7KdK6ZdYSOSTLXk+/9E2si0CpkWpm7duqSmpnLPPfec1cF85MgRmjVrRo0aNUhPT2f79u0l7mfAgAFMnz4dgDVr1rBq1SrAM+12nTp1aNCgAfv27ePLL7/0blOvXr0i2+0HDBjAxx9/zMmTJzlx4gT/+te/uPzyyy/42Bo0aECjRo28tYy3336bgQMHUlBQwM6dOxk0aBAvvvgihw8f5vjx42zZsoVu3brx2GOPkZycXOFJIZjTXNwNPO+0dW0WkW3AJcDiIMZUJoVTZDw39zlvU5K7wM3rS163eZJMteQ7bUxFGT58ODfddNNZI5Fuv/12rr32WpKTk+nRoweXXHJJifu47777uPvuu0lMTKRHjx707t0b8DxFrWfPnnTt2vW8abdHjx7NsGHDaNmyJenp6d71SUlJ3HXXXd593HvvvfTs2bPEpqLivPXWW4wZM4aTJ0/Srl07pk6dSn5+PiNGjODIkSOoKg8//DANGzZk3LhxpKen43K56NKli/cpchUloFNni0g88JmqJhTx2d+Afao6XkSaA8uA7qp6oKR9VvTU2RVpwc4FDJ42mNz8XAq0gNqRtXmgzwNcf/H1lhxM2LKps8NPeabODlhNQUTeA1KBJiKSBTwN1ABQ1deBZ4E3RWQ1IMBjpSWEUOc726q7wM1TGU/xwn9fYOKiiRVWjTbGmEAKWFJQ1RLvLFHV3cCVgfr+YPFtSip8YM8Z9xkyMjMsKRhjQp7d0RwgqfGp1IqsBXge1pPYPLGULYwJXYFsZjYVq7znypJCgBQ2JT3U5yEiIyJ5a+VbwQ7JmDKpVasWOTk5lhjCgKqSk5NDrVq1yrwPe8hOABU2JR3PPc7k5ZN5dfGr3N/7/mCHZcwFiYmJISsri1C7R8gUrVatWsTExJR5e0sKAbZg5wKmr/aMi37gywfo3qI7/eP6BzkqY/xXo0YN2rZtG+wwTCWx5qMAK7zlHzzPYJiwcEKQIzLGmOJZUggw31v+IySC9Mx0juceD3ZYxhhTJEsKAeZ7y//rV7/OwVMHuWHGDTYFhjEmJFlSqASFz2BIaJaAS1ykbUtj8LTBlhiMMSHHkkIlysjMQPEM6zvtPl0hUwobY0xFsqRQiVLjU6np8kypqyg9W/QMckTGGHM2SwqVqLB/4cE+DwIwe/vsIEdkjDFns/sUKlnhDW3rstfxpwV/YmCbgQztODTYYRljDGA1haBYsHMBc7fPJa8gj2tnXGsdzsaYkGFJIQgyMjPIK8gDPA/j+c/W/wQ5ImOM8bCkEASFN7RFiOfnP3bm/Ef9GWNMMAQsKYjIFBHZLyJrSiiTKiIrRGStiFSbXlffG9q6NOnCP7//p/cxnsYYE0yBrCm8CRTbgyoiDYHXgOtUtStwSwBjCTkpsSk8cfkTPHvFs2w9tJWZ62cGOyRjjAlcUlDVOcDBEorcBvxTVXc45fcHKpZQdv3F19O6Xmse/eZR63A2xgRdMPsUOgGNRCRDRJaKyMggxhI0i3ctZv+J/ew4uoNBbw2yxGCMCapgJoVIoBdwNfBDYJyIdCqqoIiMFpElIrKkqj3oIyMzgwItACA3P9emvjDGBFUwk0IW8JWqnlDVA8AcoHtRBVV1kqomq2py06ZNKzXIQCsciSQIitKtebdgh2SMqcaCmRQ+AS4XkUgRuQjoA6wPYjxBUTgS6eG+DwOwbM+yIEdkjKnOAjbNhYi8B6QCTUQkC3gaqAGgqq+r6noR+QpYBRQAk1W12OGrVVnh1BfrD6znb0v+xtj+Y4lyRQU7LGNMNRSwpKCqw/0o8xLwUqBiCDdD2g7hy81f8uJ/X+TJAU8GOxxjTDVkdzSHiAU7F/BkuicRPJ3xtI1CMsYEhSWFEJGRmUFufi4ABVrAh+s+DHJExpjqyJJCiCgcheQSFwC7ju4KckTGmOrInqcQIgpHIWVkZpC2LY1vtn7DqbxT1K5RO9ihGWOqEasphJCU2BQev/xxxg0Yx+HTh/lg7QfBDskYU81YUghBA9oMoE2DNjyV/pR1OBtjKpUlhRC0MGshu4/tZsfRHVwx7QpLDMaYSmNJIQT5zod0xn3G5kMyxlQaSwoh6Nz5kFJiU4IdkjGmmrCkEIIKRyLd1eMuAPYc2xPcgIwx1YYlhRCVEpvC5OsmE98wnsnLJwc7HGNMNWFJIYRFSASjeo7i223fsvng5mCHY4ypBiwphLjEZokIwrOznw12KMaYasCSQghbsHMBt868FUWZtmoac3fMDXZIxpgqzpJCCPOdJA9g8lLrWzDGBJYlhRB27iR52w5vC3JExpiqLmBJQUSmiMh+ESnxaWoicqmI5IvIzYGKJVwVDk19dtCz3N7tdubvnM/e43uDHZYxpgoLZE3hTWBoSQVExAW8AHwdwDjCmu8kefmaz1sr3gp2SMaYKixgSUFV5wAHSyn2ADAT2B+oOKqKi5tcTPfm3Xlp/kvM3zE/2OEYY6qooPUpiEhr4EbgdT/KjhaRJSKyJDs7O/DBhaAFOxewPns9OadybJI8Y0zABLOjeQLwmKrml1ZQVSeparKqJjdt2rQSQgs9GZkZ5Ds/VW5+rk2SZ4wJiGA+eS0ZmCEiAE2Aq0TEraofBzGmkFU4Eum0+zSK0qtVr2CHZIypgoJWU1DVtqoar6rxwEfAzywhFK9wJNJ9l94HwJaDW4IckTGmKgrkkNT3gAXAxSKSJSKjRGSMiIwJ1HdWdSmxKfx12F/p3rw7/1j+j2CHY4ypggLWfKSqwy+g7F2BiqOqERHuTbqXB758gAe+eIDbut1mz1swxlQYu6M5DHVs3BGAV797lcHTBttIJGNMhbGkEIaW7VkGgKI2EskYU6EsKYSh1PhUarpqAp5nLqTGpwY3IGNMlWFJIQylxKbw7chvaXpRUzpFd7I+BWNMhbGkEKYui7uMJy5/grXZa1m1b1WwwzHGVBGWFMLYHYl3EOWK4o2lbwQ7FGNMFWFJIYxFXxTNwDYDeWPZG3y79dtgh2OMqQIsKYSxBTsXMGf7HM7kn2Ho9KE2NNUYU26WFMJYRmYG7gI3AHkFeTY01RhTbpYUwljhJHkRzmlsUbdFkCMyxoQ7SwphrHCSvCcHPElNV00WZFnzkTGmfII5dbapACmxKaTEprDr2C7eXvk2Leu2ZGiHoXbvgjGmTKymUEVcFnsZp/NP8+ycZ20+JGNMmVlSqCL2Hd8H2HxIxpjysaRQRRR2OgNERkTafEjGmDIJ5EN2pojIfhFZU8znt4vIKuc1X0S6ByqW6iAlNoVvRnxDnRp1SG6VbH0Kxpgy8SspiEh7EanpvE8VkV+ISMNSNnsTGFrC59uAgaqaCDwLTPInFlO8gfEDeSTlEebvnM/WQ1uDHY4xJgz5W1OYCeSLSAfgH0Bb4N2SNlDVOcDBEj6fr6qHnMWFQIyfsZgSjEkeQ4REcOe/7rTOZmPMBfM3KRSoqhu4EZigqg8DLSswjlHAl8V9KCKjRWSJiCzJzs6uwK+terYf3g7AvJ3zbBSSMeaC+ZsU8kRkOHAn8JmzrkZFBCAig/AkhceKK6Oqk1Q1WVWTmzZtWhFfW2VlZGagKABn3GdsFJIx5oL4mxTuBlKA36vqNhFpC7xT3i8XkURgMnC9quaUd3/m7KeyKUr/Nv2DHJExJpz4lRRUdZ2q/kJV3xORRkA9VX2+PF8sInHAP4E7VHVjefZl/qdw6osR3UagKHuP7Q12SMaYMCKqWnohkQzgOjzTYqwAsoHZqvpICdu8B6QCTYB9wNM4TU6q+rqITAZ+BGx3NnGranJpsSQnJ+uSJUtKjbm6yy/I55JXL6Fx7cYsHLUQEQl2SMaYIBKRpf5cY/1tPmqgqkeBm4CpqtoLGFLSBqo6XFVbqmoNVY1R1X+o6uuq+rrz+b2q2khVezivUoM1/nNFuHik7yMs3rWYMZ+PsQ5nY4xf/E0KkSLSEvgx/+toNiHukiaXADBp6SQbiWSM8Yu/SeEZ4Gtgi6p+JyLtgE2BC8tUhIVZCxE8zUY2H5Ixxh9+TZ2tqh8CH/osb8XTH2BCWGp8KrUia3HKfcq7bIwxJfF3mosYEfmXM5fRPhGZKSJ2B3KIKxyJNCh+EPmaz/tr37cmJGNMifxtPpoKfAq0AloD/3bWmRCXEpvCry/7NQCvLHrF+haMMSXyNyk0VdWpqup2Xm8CdmtxmFi+d7n1LRhj/OJvUjggIiNExOW8RgB2B3KYSI1PpWZkzbOWjTGmKP4mhXvwDEfdC+wBbsYz9YUJAymxKXw78ltS26SSr/nUiaoT7JCMMSHKrzuai9xQ5CFVnVDB8ZTK7mguu0OnDtH2lbZ0b9Gdoe2Hkhqfag/jMaaaqOg7motS7BQXJjQ1qt2IW7rcwpztc3gy/UnrdDbGnKc8ScEm0wlDreq3AqBAC6zT2RhznvIkhbK1O5mgGtp+KDUiPI/CcEW4rNPZGHOWEpOCiBwTkaNFvI7huWfBhJmU2BRmjZxFdO1oGtdqTNq2NGtCMsZ4lZgUVLWeqtYv4lVPVf2aIsOEngFtBvDoZY+y98Renkp/yvoWjDFe5Wk+MmEsX/MBz9PZrG/BGFMoYElBRKY4cyWtKeZzEZGJIrJZRFaJSFKgYjE/MiOlAAAa/klEQVTnGxQ/yPvYThGxvgVjDBDYmsKbwNASPh8GdHReo4G/BTAWc46U2BTS70ynT+s+5BfksyFnA8/Nfc6akYyp5sp885pfOxeJBz5T1YQiPvs7kKGq7znLG4BUVd1T0j7t5rWKdejUIdq90o6juUcRhChXFGkj0+ymNmOqmMq4ea28WgM7fZaznHWmEjWq3Ygh7YZQoAXka771LxhTzQUzKRR181uR1RYRGS0iS0RkSXZ2doDDqn4e7vswEeL5p1DDVYPoi6KtKcmYaiqYSSELiPVZjgF2F1VQVSeparKqJjdtajN2V7TL4i7jnZvewSUuujbpykNfPcS49HE2VNWYaiiYSeFTYKQzCqkvcKS0/gQTOMMThvPMoGdYuncpZ9xnrCnJmGoqkENS3wMWABeLSJaIjBKRMSIyxinyBbAV2Ay8AfwsULEY/zx62aN0bNyRAgpwiQtXhIsdR3ZYbcGYaiSgo48CwUYfBdbyPcu59I1LadOwDbuO7sJd4LYRScZUAeEw+siEoJ4te/LMoGfYemgrefl51oxkTDVjScGc57F+j9G9eXdvM1KUK8pGJBlTTdikduY8rggXn9z6CV1e60J07Wge7/84D331ELn5udaUZEwVZzUFU6Q2Ddsw5bop7Dy6k3dXv0tufq41JRlTDVhSMMX6ScJP+PmlP2feznm4Ilw2IsmYasCSginRH6/8I71b9yYyIpJbutyCILyx7A27sc2YKsqSgilRzciafHDzB9SOrE3atjTcBW5vM9K0ldOs89mYKsY6mk2p2jRsw8wfz2TwtMEo6m1Gmrpiqt3HYEwVYzUF45eB8QP5+zV/p0ALuLTVpdzT456zag3W+WxM1WBJwfhtVNIoHun7CAt3LaSGqwZRrijrfDamirFpLswFyS/I59aZt/LRuo8YP3A8e4/vtWYkY8KATXNhAsIV4eLtG99mYJuB/H7u78nNz7XOZ2OqEKspmDI5fPowA6YOYEPOBgTBXeDGFeE66/09Pe5hZPeRVnMwJgRYTcEEVMNaDZk1chbtG7VHEEYljTqv8/nvS/9u9zMYE2YsKZgya1anGd/e+S1tGrbh3dXv0q15N6JcUYjzpFVFrUnJmDBjzUem3HYf283gaYPZfng7fxj8BzYc2ODtfPZtUrKOaGOCJySaj0RkqIhsEJHNIjK2iM/jRCRdRJaLyCoRuSqQ8ZjAaFWvFbPvms3FTS7msVmPcWX7K0m/M51nBz17XpOS1RqMCW0BqymIiAvYCPwAyAK+A4ar6jqfMpOA5ar6NxHpAnyhqvEl7ddqCqHr0KlDXPXuVSzetZjXrnqN/0v+PxbsXMDgaYPJzc+1WoMxQRQKNYXewGZV3aqqucAM4PpzyihQ33nfANgdwHhMgDWq3YhZd8xiWIdhjPl8DE+lP0XfmL6kjUw7r9Zwxn2G8RnjrcZgTIgJZE3hZmCoqt7rLN8B9FHVn/uUaQl8AzQC6gBDVHVpEfsaDYwGiIuL67V9+/aAxGwqhrvAzf/9+/+YsmIKtybcyj+u+wcX1bjIW2s44z5DAQVESAQ1XTWZMHQCOSdzSI1PtZqDMQESCjUFKWLduRloOPCmqsYAVwFvi8h5ManqJFVNVtXkpk2bBiBUU5EiIyKZfN1knhv8HO+veZ8BUweQdTSLlNgU0kamMaTdECIkggIt4Iz7DD//4ueMSx9H6lup3PfZfVZ7MCaIApkUsoBYn+UYzm8eGgV8AKCqC4BaQJMAxmQqiYgwtv9YPh3+KRtzNtJrUi/St6WTEpvC+NTx1HTVxCUuIiIiyNf88+5tmLR0knVIGxMEgWw+isTT0TwY2IWno/k2VV3rU+ZL4H1VfVNEOgNpQGstISjraA4/67PXc9MHN7ExZyPPDX6ORy97lIVZC8nIzCD6omge+uohTrtPo05FMoIIXBEuCrTA7ow2poL423wU0PsUnCGmEwAXMEVVfy8izwBLVPVTZ8TRG0BdPE1Lv1bVb0rapyWF8HTszDFGfTqKD9d9yFUdr2Lq9VNpVqcZAAt2LmDaymneextEhAItoEALABCEWpG1rO/BmHIIiaQQCJYUwpeq8up3r/Krb35Fw1oNefOGNxnaYaj38wU7F/hVe4hyRVmCMOYCWVIwIWv1vtUMnzmctdlr+Vnyz3jxBy9SJ6rOWWVKqj1Y85IxF86Sgglpp/JO8Ztvf8PLC1+mY+OOvHXDW0Ve1M+tPeTm51rzkjFlYEnBhIX0benc9cld7Dyyk4f6PsTvrvgdF9W4qMiy/jYv+dYeADIyMyxRmGrPkoIJG8fOHGPsrLG8tuQ12jVqx4QfTuCaTtcgUtStLh7+dE7XcNWw5zsY47CkYMLO7MzZ3Pf5faw/sJ5hHYYxYegEOkV3KnGbkmoPvlN4Fy77NjNFXxRtzU2m2rCkYMJSXn4ef1n8F8ZnjOe0+zQP9nmQcQPHUb9m/VK3Pbf2UDgBX25+7nnNTPkF+cVOtQHW5GSqHksKJqztPb6XJ9KeYOoKz/0MT/R/gtG9RlO7Ru1Sty2sPRRe4EtqZoLz+yOsyclURZYUTJXw3a7v+PWsX5ORmUHLui15csCT/DTpp9Rw1big/ZzbzOQ7KV/hPEwFWnBBTU5gNQoTPiwpmColIzODp9KfYu6OuXSK7sTzg5/nhktuKLEzuji+CaLwQl843NXfJqfIiMgiaxSFsVqiMKHGkoKpclSVzzZ+xmOzHmP9gfV0b96dsf3HckuXW3BFuMq17wttciqqRlHcaCfgrH37kzR847HkYiqCJQVTZbkL3Lyz6h1e+O8LfH/ge9o3as/j/R/nju53EOWKqrDvKanJqbCm4Fuj8CdRlJQ0iqq52BPqTEWxpGCqvAIt4OPvP+YPc//A0j1Lia0fyy/6/IJRPUfRqHajCv2uc5uciqpRFNX05JsoSkoaefl5RfZxRBDBkHZDGJ86HqDIGKxGYfxhScFUG6rKN1u+4fn/Pk9GZgYX1biIEd1GcH/v+0lsnhjw7y+p6enc2kFptQsouQ/DN3n49mv4ThJYVNIobV1hQqmoZitr/go9lhRMtbRq3yomLprI9NXTOe0+Tb/Yftx/6f38qMuPKrRpqTTnJorSkobvxb7wvomZ62Yya9usIkdFwdkJ5UITSVHft3zPcm9c585EW3gMpdVSzr1XpLgZbS1pVD5LCqZaO3jqIG+ueJPXvnuNLYe20LxOc0b3Gs09Pe8hvmF8sMMrMmmc+5d74TOtfUdFFVdT8KczvLh1hQnFXeAudi6pkr67sH+kZ8uefs1JVViusM+kPDWcwnWlNaWV9nsXta2/iau4PwAqYiBBRSbPkEgKIjIUeAXPQ3Ymq+rzRZT5MTAez0N2VqrqbSXt05KCuRAFWsA3W77hr4v/yhebvgBgSLsh3NPzHq6/+Hq/boYLJn8vZiV1hpdWU/DtwwBP4ii8kPtTSylqG3/XlaeGc25iKq4Tv7SaWUkJrrTEVVJTob8DCfzZd0U8QyToSUFEXHgex/kDPM9r/g4YrqrrfMp0xPOM5itU9ZCINFPV/SXt15KCKavth7fz5oo3mbJiCjuO7KB+zfrc3Plmbut2G6nxqeUe1hpsxXWGl7bu3Ps0zr0olnQhLep+jqJqBcXVHspTw/G3E7+0PpzSElxJicvf0WclDSTwZ9/nPmSqLCPSQiEppADjVfWHzvLjAKr6nE+ZF4GNqjrZ3/1aUjDlVaAFzM6czdur3ubDdR9yPPc4Leq24Cddf8Kd3e+kR4seZbopLpwV1UzhTy3F3/6D4sqVp4ZTVE2hLBfp0i7CF3qfir8DCS5k375JyiUunh30LI9f/vgFneNQSAo3A0NV9V5n+Q6gj6r+3KfMx3hqE/3wNDGNV9WvitjXaGA0QFxcXK/t27cHJGZT/ZzKO8Xnmz7n3dXv8vmmz8nNzyWhWQI3XnIj13S6huRWyURIRLDDDGnlbR8vaw3H30780ppzSktwJSWusjRX+U7A6O++z23OCteawi3AD89JCr1V9QGfMp8BecCPgRhgLpCgqoeL26/VFEygHDx1kPfXvM+7a95l/s75FGgBreu15vZut3NH9ztIaJYQ7BCNH8rS8VvUtv4mrrJ0bJdl3+XtdA6FpOBP89HrwEJVfdNZTgPGqup3xe3XkoKpDDknc/hy85e8v/Z9vtz0JfmaT5emXbjxkhu58ZIbSWqZVO2amEx4C4WkEImnaWgwsAtPR/NtqrrWp8xQPJ3Pd4pIE2A50ENVc4rbryUFU9n2n9jPB2s/4J/r/8ns7bMp0AJi6sdwXafruPbia0mNT6VWZK1gh2lMiYKeFJwgrgIm4OkvmKKqvxeRZ4AlqvqpeP7U+hMwFMgHfq+qM0rapyUFE0wHTh7g842f88mGT/h6y9eczDtJnRp1GNJuCFe2v5IftPsBHRp3sFqECTkhkRQCwZKCCRWn3adJ35bOvzf+m883fc6OIzsAiG8Yz1UdrmJYx2GkxqdSN6pukCM1xpKCMZVKVdlyaAv/2fIfvtryFWlb0ziRd4IaETXoF9ePIW2HMDB+IJe2upSakTWDHa6phiwpGBNEZ9xnmLdjHt9s+Yavt3zNyn0rAagdWZv+cf35QbsfMLjdYHq06GFDXk2lsKRgTAjJOZnD3B1zSd+WTtq2NNZme8ZbNK7dmEHxgxjSbgiD2w62/ggTMJYUjAlhu4/t5ttt35K2LY1ZW2eRdTQLgNj6sQxoM4DL4y6nT0wfujbtesHPozamKJYUjAkTqsqmg5tI25pGemY6c3fMZe/xvQBEuaLo0aIHA+IGMKjtIPrF9qNBrQZBjtiEI0sKxoSpwk7rJbuXsHT3UhbuWsiirEXkFeQhCJc0uYS+MX1JiUkhJTaFLk27WL+EKZUlBWOqkJN5J1mYtZD5O+ezaNciFuxcQM4pzz2e9aLqcWnrS+ndqjd9YvrQp3UfWtZrGeSITaixpGBMFaaqbD64mQVZC1iUtYjFuxezcu9K8gryAIhrEMdlsZfRL7YfPVv0pGuzrjSs1TDIUZtgsqRgTDVz2n2a5XuWe2oSWQv4747/suvYLu/ncQ3i6NO6DykxKSS3SiaxeaL1T1QjlhSMqeZUlayjWazat4q12WtZtmcZC7MWsv3I/6aeb9eoHcmtkklumUyvVr3o2aInjWo3CmLUJlAsKRhjirT3+F6W71nOir0rWLZ3Gd/t+u6sRNGmQRu6t+hOYrNEerbsSa+WvYhrEGf3T4Q5SwrGGL9ln8hm+V5Poli+dzkr965kQ84G7xPBmlzUhMTmiXRr1o1uzbrRuWlnOjfpbLWKMGJJwRhTLqfyTrF6/2rv0NjV+1ezNnstJ/NOesu0rteabs27kdgs0VO7aJ5Ip+hORLmighi5KYq/SSGyMoIxxoSf2jVq07t1b3q37u1dl1+QT+bhTNYfWM/a/WtZk72G1ftWk7Y1zTvyySUuOjTuQJemXejatCsJzRJIbJ5Ix+iOREbYJSfUWU3BGFNuefl5bMjZwKp9q1iXvY71B9azLnsdm3I2ka/5ANSKrEVCswQ6N+nMJU0u8b7aN2pvM8dWgpBoPnKerPYKnofsTFbV54spdzPwIXCpqpZ4xbekYEz4OO0+zfcHvmfVvlWs3LuSlfs8fRWFcz2Bp2bRvnF7Ojfp7K1ZdG3WlU7RneyJdhUo6M1HIuICXgV+AGQB34nIp6q67pxy9YBfAIsCFYsxJjhqRdaiR4se9GjRA7r/b/3x3ONszNnI+uz1rD/gvLLX89nGz7w1C0GIaxBHp+hOdGzckU7RnbikySV0adqFmPoxNhoqQALZwNcb2KyqWwFEZAZwPbDunHLPAi8CvwpgLMaYEFI3qi5JLZNIapl01vrc/Fw25mxkzf41bMzZyMacjWzI2cA7q9/h6JmjZ23fKboTF0df7E0WF0dfTMfojvaku3IKZFJoDez0Wc4C+vgWEJGeQKyqfiYilhSMqeaiXFEkNEsgoVnCWetVlf0n9vP9ge//119xcBMLsxYyY80MlP81g7eo24IOjTvQvlF7z6txe9o1akf7Ru1pWqdpZR9S2AlkUiiqbuc9cyISAbwM3FXqjkRGA6MB4uLiKig8Y0y4EBGa121O87rNGRg/8KzPTrtPsylnE98f+J5NBzex6eAmthzcwn+2/oe3jr11Vtno2tF0btqZTo07eZNFmwZtiGsQR8t6LW22WQLY0SwiKcB4Vf2hs/w4gKo+5yw3ALYAx51NWgAHgetK6my2jmZjjL9O5Z0i83AmWw9t9fRhOP0Xmw9u9j6zolCdGnXo0rQLXZp2IbZ+LDH1Y4hvGE+n6E7ENYjDFeEK0lFUjKCPPhKRSGAjMBjYBXwH3Kaqa4spnwH8ykYfGWMqw4ncE2w7vI0dR3aQeTiTDQc2sCZ7DRsObGDP8T3eu7kBarpq0rZRW9o3ak98w3hi68cS2yCW9o3a0zG6I41rNw7ikfgn6KOPVNUtIj8HvsYzJHWKqq4VkWeAJar6aaC+2xhjSlMnqk6R/RcA7gI3e4/vZeuhrWw4sIGNORvZcmgLWw5tYd6OeRw5c+Ss8g1rNSSuQRxxDeJo17AdHRp3oEPjDrRp2IbY+rHUq1mvsg6r3OzmNWOMuUDHzhxjx5EdbDm0hc0HN7Pl4BZ2Ht3J9iPb2XJwCyfyTpxVvlGtRsQ2iKVNgzZ0aNyBjo070rZRW1rVa0Wreq2Irh0d8CG2Qa8pGGNMVVWvZj26NutK12Zdz/tMVdl3Yh9bDm5hx5EdbD+ynZ1HdrLj6A62Hd7GrK2zOOU+ddY29WvWp0PjDrRr1I74BvHeGkZM/Rhi6sfQtE7TSusEt6RgjDEVSERoUbcFLeq2oB/9zvu8QAvYfWw3O47sYPex3WQdzWLLwS1sPrSZVftW8e8N/+ZM/pmztqkRUYPW9VvzQO8HeCTlkYDGb0nBGGMqUYREeGsARSm8JyPraNbZr2NZtKjbIuDxWVIwxpgQ4ntPRq9WvSr9++1ODWOMMV6WFIwxxnhZUjDGGONlScEYY4yXJQVjjDFelhSMMcZ4WVIwxhjjZUnBGGOMV9hNiCci2cD2C9ysCXAgAOEEgx1LaLJjCV1V6XjKcyxtVLXUR8+FXVIoCxFZ4s/sgOHAjiU02bGErqp0PJVxLNZ8ZIwxxsuSgjHGGK/qkhQmBTuACmTHEprsWEJXVTqegB9LtehTMMYY45/qUlMwxhjjB0sKxhhjvKp0UhCRoSKyQUQ2i8jYYMdzIUQkVkTSRWS9iKwVkQed9Y1F5D8issn5b6Ngx+ovEXGJyHIR+cxZbisii5xjeV9EooIdo79EpKGIfCQi3zvnKCVcz42IPOz8G1sjIu+JSK1wOTciMkVE9ovIGp91RZ4H8ZjoXA9WiUhS8CI/XzHH8pLzb2yViPxLRBr6fPa4cywbROSHFRVHlU0KIuICXgWGAV2A4SLSJbhRXRA38EtV7Qz0Be534h8LpKlqRyDNWQ4XDwLrfZZfAF52juUQMCooUZXNK8BXqnoJ0B3PcYXduRGR1sAvgGRVTQBcwK2Ez7l5Exh6zrrizsMwoKPzGg38rZJi9NebnH8s/wESVDUR2Ag8DuBcC24FujrbvOZc88qtyiYFoDewWVW3qmouMAO4Psgx+U1V96jqMuf9MTwXndZ4juEtp9hbwA3BifDCiEgMcDUw2VkW4ArgI6dIOB1LfWAA8A8AVc1V1cOE6bnB81je2iISCVwE7CFMzo2qzgEOnrO6uPNwPTBNPRYCDUWkZeVEWrqijkVVv1FVt7O4ECh8sPP1wAxVPaOq24DNeK555VaVk0JrYKfPcpazLuyISDzQE1gENFfVPeBJHECz4EV2QSYAvwYKnOVo4LDPP/hwOj/tgGxgqtMcNllE6hCG50ZVdwF/BHbgSQZHgKWE77mB4s9DuF8T7gG+dN4H7FiqclKQItaF3fhbEakLzAQeUtWjwY6nLETkGmC/qi71XV1E0XA5P5FAEvA3Ve0JnCAMmoqK4rS3Xw+0BVoBdfA0s5wrXM5NScL235yI/AZPk/L0wlVFFKuQY6nKSSELiPVZjgF2BymWMhGRGngSwnRV/aezel9hldf57/5gxXcB+gHXiUgmnma8K/DUHBo6TRYQXucnC8hS1UXO8kd4kkQ4npshwDZVzVbVPOCfwGWE77mB4s9DWF4TRORO4Brgdv3fjWUBO5aqnBS+Azo6oyii8HTKfBrkmPzmtLn/A1ivqn/2+ehT4E7n/Z3AJ5Ud24VS1cdVNUZV4/Gch29V9XYgHbjZKRYWxwKgqnuBnSJysbNqMLCOMDw3eJqN+orIRc6/ucJjCctz4yjuPHwKjHRGIfUFjhQ2M4UqERkKPAZcp6onfT76FLhVRGqKSFs8neeLK+RLVbXKvoCr8PTYbwF+E+x4LjD2/niqg6uAFc7rKjxt8WnAJue/jYMd6wUeVyrwmfO+nfMPeTPwIVAz2PFdwHH0AJY45+djoFG4nhvgt8D3wBrgbaBmuJwb4D08fSF5eP56HlXcecDT5PKqcz1YjWfEVdCPoZRj2Yyn76DwGvC6T/nfOMeyARhWUXHYNBfGGGO8qnLzkTHGmAtkScEYY4yXJQVjjDFelhSMMcZ4WVIwxhjjZUnBGIeI5IvICp9Xhd2lLCLxvrNfGhOqIksvYky1cUpVewQ7CGOCyWoKxpRCRDJF5AURWey8Ojjr24hImjPXfZqIxDnrmztz3690Xpc5u3KJyBvOswu+EZHaTvlfiMg6Zz8zgnSYxgCWFIzxVfuc5qOf+Hx2VFV7A3/FM28Tzvtp6pnrfjow0Vk/EZitqt3xzIm01lnfEXhVVbsCh4EfOevHAj2d/YwJ1MEZ4w+7o9kYh4gcV9W6RazPBK5Q1a3OJIV7VTVaRA4ALVU1z1m/R1WbiEg2EKOqZ3z2EQ/8Rz0PfkFEHgNqqOrvROQr4Die6TI+VtXjAT5UY4plNQVj/KPFvC+uTFHO+LzP5399elfjmZOnF7DUZ3ZSYyqdJQVj/PMTn/8ucN7PxzPrK8DtwDznfRpwH3ifS12/uJ2KSAQQq6rpeB5C1BA4r7ZiTGWxv0iM+Z/aIrLCZ/krVS0cllpTRBbh+UNquLPuF8AUEXkUz5PY7nbWPwhMEpFReGoE9+GZ/bIoLuAdEWmAZxbPl9XzaE9jgsL6FIwphdOnkKyqB4IdizGBZs1HxhhjvKymYIwxxstqCsYYY7wsKRhjjPGypGCMMcbLkoIxxhgvSwrGGGO8/h96z7iknuxinAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a10fcbbe0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "loss_values = model_val_dict['loss']\n",
    "val_loss_values = model_val_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'g.', label='Validation loss')\n",
    "\n",
    "plt.title('Training & validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting! \n",
    "\n",
    "Run the cell below to visualize a plot of our training and validation accuracy>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a10fcba90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = model_val_dict['acc'] \n",
    "val_acc_values = model_val_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'r.', label='Validation acc')\n",
    "plt.title('Training & validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe an interesting pattern here: although the training accuracy keeps increasing when going through more epochs, and the training loss keeps decreasing, the validation accuracy and loss seem to be reaching a status quo around the 60th epoch. This means that we're actually **overfitting** to the train data when we do as many epochs as we were doing. Luckily, you learned how to tackle overfitting in the previous lecture! For starters, it does seem clear that we are training too long. So let's stop training at the 60th epoch first (so-called \"early stopping\") before we move to more advanced regularization techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Early stopping\n",
    "\n",
    "Now that we know that the model starts to overfit around epoch 60, we can just retrain the model from scratch, but this time only up to 60 epochs! This will help us with our overfitting problem.  This method is called **_Early Stopping_**.\n",
    "\n",
    "In the cell below: \n",
    "\n",
    "* Recreate the exact model we did above. \n",
    "* Compile the model with the exact same hyperparameters.\n",
    "* Fit the model with the exact same hyperparameters, with the exception of `epochs`.  This time, set epochs to `60` instead of `120`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/60\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.9474 - acc: 0.1571 - val_loss: 1.9435 - val_acc: 0.1520\n",
      "Epoch 2/60\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.9276 - acc: 0.1855 - val_loss: 1.9287 - val_acc: 0.1740\n",
      "Epoch 3/60\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.9151 - acc: 0.2003 - val_loss: 1.9178 - val_acc: 0.1900\n",
      "Epoch 4/60\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.9042 - acc: 0.2157 - val_loss: 1.9082 - val_acc: 0.2110\n",
      "Epoch 5/60\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.8935 - acc: 0.2276 - val_loss: 1.8985 - val_acc: 0.2100\n",
      "Epoch 6/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.8820 - acc: 0.2324 - val_loss: 1.8879 - val_acc: 0.2190\n",
      "Epoch 7/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.8692 - acc: 0.2432 - val_loss: 1.8753 - val_acc: 0.2330\n",
      "Epoch 8/60\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.8545 - acc: 0.2583 - val_loss: 1.8607 - val_acc: 0.2420\n",
      "Epoch 9/60\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.8370 - acc: 0.2779 - val_loss: 1.8427 - val_acc: 0.2600\n",
      "Epoch 10/60\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.8157 - acc: 0.2988 - val_loss: 1.8211 - val_acc: 0.2800\n",
      "Epoch 11/60\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.7902 - acc: 0.3256 - val_loss: 1.7941 - val_acc: 0.2970\n",
      "Epoch 12/60\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.7597 - acc: 0.3467 - val_loss: 1.7621 - val_acc: 0.3160\n",
      "Epoch 13/60\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.7252 - acc: 0.3693 - val_loss: 1.7272 - val_acc: 0.3430\n",
      "Epoch 14/60\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.6870 - acc: 0.3891 - val_loss: 1.6876 - val_acc: 0.3860\n",
      "Epoch 15/60\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.6449 - acc: 0.4115 - val_loss: 1.6456 - val_acc: 0.4110\n",
      "Epoch 16/60\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.6002 - acc: 0.4411 - val_loss: 1.6004 - val_acc: 0.4350\n",
      "Epoch 17/60\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.5528 - acc: 0.4664 - val_loss: 1.5536 - val_acc: 0.4580\n",
      "Epoch 18/60\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.5034 - acc: 0.5005 - val_loss: 1.5058 - val_acc: 0.4810\n",
      "Epoch 19/60\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.4532 - acc: 0.5309 - val_loss: 1.4577 - val_acc: 0.5000\n",
      "Epoch 20/60\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.4030 - acc: 0.5643 - val_loss: 1.4104 - val_acc: 0.5270\n",
      "Epoch 21/60\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.3531 - acc: 0.5813 - val_loss: 1.3617 - val_acc: 0.5490\n",
      "Epoch 22/60\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.3030 - acc: 0.6000 - val_loss: 1.3143 - val_acc: 0.5760\n",
      "Epoch 23/60\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.2545 - acc: 0.6184 - val_loss: 1.2685 - val_acc: 0.5880\n",
      "Epoch 24/60\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.2084 - acc: 0.6359 - val_loss: 1.2259 - val_acc: 0.6080\n",
      "Epoch 25/60\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.1644 - acc: 0.6488 - val_loss: 1.1849 - val_acc: 0.6180\n",
      "Epoch 26/60\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.1232 - acc: 0.6579 - val_loss: 1.1482 - val_acc: 0.6180\n",
      "Epoch 27/60\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.0848 - acc: 0.6679 - val_loss: 1.1123 - val_acc: 0.6380\n",
      "Epoch 28/60\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0491 - acc: 0.6779 - val_loss: 1.0802 - val_acc: 0.6410\n",
      "Epoch 29/60\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.0157 - acc: 0.6860 - val_loss: 1.0511 - val_acc: 0.6500\n",
      "Epoch 30/60\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9847 - acc: 0.6944 - val_loss: 1.0248 - val_acc: 0.6460\n",
      "Epoch 31/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9563 - acc: 0.6968 - val_loss: 0.9986 - val_acc: 0.6610\n",
      "Epoch 32/60\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9298 - acc: 0.7023 - val_loss: 0.9766 - val_acc: 0.6630\n",
      "Epoch 33/60\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9051 - acc: 0.7107 - val_loss: 0.9544 - val_acc: 0.6650\n",
      "Epoch 34/60\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8822 - acc: 0.7156 - val_loss: 0.9364 - val_acc: 0.6730\n",
      "Epoch 35/60\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8617 - acc: 0.7184 - val_loss: 0.9160 - val_acc: 0.6810\n",
      "Epoch 36/60\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8418 - acc: 0.7257 - val_loss: 0.8994 - val_acc: 0.6830\n",
      "Epoch 37/60\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8230 - acc: 0.7293 - val_loss: 0.8870 - val_acc: 0.6820\n",
      "Epoch 38/60\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8064 - acc: 0.7321 - val_loss: 0.8689 - val_acc: 0.6820\n",
      "Epoch 39/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.7903 - acc: 0.7356 - val_loss: 0.8580 - val_acc: 0.6970\n",
      "Epoch 40/60\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7754 - acc: 0.7400 - val_loss: 0.8441 - val_acc: 0.6970\n",
      "Epoch 41/60\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7612 - acc: 0.7420 - val_loss: 0.8332 - val_acc: 0.6960\n",
      "Epoch 42/60\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7479 - acc: 0.7476 - val_loss: 0.8246 - val_acc: 0.6960\n",
      "Epoch 43/60\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.7353 - acc: 0.7528 - val_loss: 0.8160 - val_acc: 0.6970\n",
      "Epoch 44/60\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.7238 - acc: 0.7540 - val_loss: 0.8040 - val_acc: 0.6990\n",
      "Epoch 45/60\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.7125 - acc: 0.7568 - val_loss: 0.7961 - val_acc: 0.6980\n",
      "Epoch 46/60\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7016 - acc: 0.7597 - val_loss: 0.7880 - val_acc: 0.7120\n",
      "Epoch 47/60\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.6914 - acc: 0.7628 - val_loss: 0.7814 - val_acc: 0.7100\n",
      "Epoch 48/60\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.6822 - acc: 0.7655 - val_loss: 0.7753 - val_acc: 0.7090\n",
      "Epoch 49/60\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.6732 - acc: 0.7649 - val_loss: 0.7660 - val_acc: 0.7120\n",
      "Epoch 50/60\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.6639 - acc: 0.7701 - val_loss: 0.7605 - val_acc: 0.7200\n",
      "Epoch 51/60\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.6556 - acc: 0.7720 - val_loss: 0.7547 - val_acc: 0.7180\n",
      "Epoch 52/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.6474 - acc: 0.7771 - val_loss: 0.7495 - val_acc: 0.7130\n",
      "Epoch 53/60\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.6391 - acc: 0.7792 - val_loss: 0.7462 - val_acc: 0.7180\n",
      "Epoch 54/60\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.6321 - acc: 0.7821 - val_loss: 0.7385 - val_acc: 0.7230\n",
      "Epoch 55/60\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.6245 - acc: 0.7841 - val_loss: 0.7358 - val_acc: 0.7290\n",
      "Epoch 56/60\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.6178 - acc: 0.7859 - val_loss: 0.7290 - val_acc: 0.7210\n",
      "Epoch 57/60\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.6108 - acc: 0.7884 - val_loss: 0.7257 - val_acc: 0.7230\n",
      "Epoch 58/60\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.6039 - acc: 0.7925 - val_loss: 0.7221 - val_acc: 0.7230\n",
      "Epoch 59/60\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.5975 - acc: 0.7963 - val_loss: 0.7191 - val_acc: 0.7290\n",
      "Epoch 60/60\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.5920 - acc: 0.7959 - val_loss: 0.7141 - val_acc: 0.7270\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "final_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=60,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as we did before, get our results using `model.evaluate()` on the appropriate variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 43us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 38us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.58606486314137773, 0.79826666669845581]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train  # Expected Output: [0.58606486314137773, 0.79826666669845581]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.74768974288304646, 0.71333333365122475]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test # [0.58606486314137773, 0.79826666669845581]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've significantly reduced the variance, so this is already pretty good! Our test set accuracy is slightly worse, but this model will definitely be more robust than the 120 epochs one we fitted before.\n",
    "\n",
    "Now, let's see what else we can do to improve the result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. L2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's include L2 regularization. You can easily do this in keras adding the argument `kernel_regulizers.l2` and adding a value for the regularization parameter lambda between parentheses.\n",
    "\n",
    "In the cell below: \n",
    "\n",
    "* Recreate the same model we did before.\n",
    "* In our two hidden layers (but not our output layer), add in the parameter `kernel_regularizer=regularizers.l2(0.005)` to add L2 regularization to each hidden layer.  \n",
    "* Compile the model with the same hyperparameters as we did before. \n",
    "* Fit the model with the same hyperparameters as we did before, but this time for `120` epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 2.6015 - acc: 0.1676 - val_loss: 2.5892 - val_acc: 0.1930\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 2.5767 - acc: 0.2169 - val_loss: 2.5672 - val_acc: 0.2190\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 2.5536 - acc: 0.2487 - val_loss: 2.5429 - val_acc: 0.2480\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 2.5269 - acc: 0.2745 - val_loss: 2.5138 - val_acc: 0.2790\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 2.4960 - acc: 0.2951 - val_loss: 2.4799 - val_acc: 0.2980\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 2.4603 - acc: 0.3204 - val_loss: 2.4415 - val_acc: 0.3160\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 2.4199 - acc: 0.3377 - val_loss: 2.3999 - val_acc: 0.3420\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 2.3764 - acc: 0.3565 - val_loss: 2.3555 - val_acc: 0.3670\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 2.3299 - acc: 0.3792 - val_loss: 2.3097 - val_acc: 0.3950\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 2.2818 - acc: 0.4084 - val_loss: 2.2628 - val_acc: 0.4250\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 2.2335 - acc: 0.4352 - val_loss: 2.2190 - val_acc: 0.4500\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 2.1862 - acc: 0.4617 - val_loss: 2.1722 - val_acc: 0.4710\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 2.1406 - acc: 0.4896 - val_loss: 2.1300 - val_acc: 0.4900\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 2.0968 - acc: 0.5068 - val_loss: 2.0893 - val_acc: 0.5140\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 2.0547 - acc: 0.5357 - val_loss: 2.0491 - val_acc: 0.5350\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 2.0145 - acc: 0.5595 - val_loss: 2.0113 - val_acc: 0.5400\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.9759 - acc: 0.5764 - val_loss: 1.9764 - val_acc: 0.5450\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.9382 - acc: 0.5924 - val_loss: 1.9425 - val_acc: 0.5720\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.9022 - acc: 0.6065 - val_loss: 1.9090 - val_acc: 0.5790\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.8673 - acc: 0.6212 - val_loss: 1.8785 - val_acc: 0.5970\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.8335 - acc: 0.6332 - val_loss: 1.8487 - val_acc: 0.6070\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.8005 - acc: 0.6488 - val_loss: 1.8179 - val_acc: 0.6090\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.7687 - acc: 0.6548 - val_loss: 1.7890 - val_acc: 0.6240\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.7375 - acc: 0.6645 - val_loss: 1.7626 - val_acc: 0.6340\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.7069 - acc: 0.6729 - val_loss: 1.7369 - val_acc: 0.6410\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.6786 - acc: 0.6815 - val_loss: 1.7106 - val_acc: 0.6430\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.6509 - acc: 0.6863 - val_loss: 1.6836 - val_acc: 0.6580\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.6240 - acc: 0.6933 - val_loss: 1.6630 - val_acc: 0.6680\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.5983 - acc: 0.7023 - val_loss: 1.6382 - val_acc: 0.6670\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.5739 - acc: 0.7079 - val_loss: 1.6200 - val_acc: 0.6660\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.5505 - acc: 0.7137 - val_loss: 1.5959 - val_acc: 0.6720\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.5280 - acc: 0.7181 - val_loss: 1.5751 - val_acc: 0.6820\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.5060 - acc: 0.7241 - val_loss: 1.5564 - val_acc: 0.6850\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.4856 - acc: 0.7263 - val_loss: 1.5370 - val_acc: 0.6830\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.4662 - acc: 0.7305 - val_loss: 1.5215 - val_acc: 0.6900\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.4472 - acc: 0.7335 - val_loss: 1.5066 - val_acc: 0.6940\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.4290 - acc: 0.7372 - val_loss: 1.4904 - val_acc: 0.6940\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.4119 - acc: 0.7441 - val_loss: 1.4745 - val_acc: 0.6940\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.3952 - acc: 0.7436 - val_loss: 1.4623 - val_acc: 0.6980\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.3799 - acc: 0.7509 - val_loss: 1.4474 - val_acc: 0.7050\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.3641 - acc: 0.7553 - val_loss: 1.4339 - val_acc: 0.7080\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.3499 - acc: 0.7575 - val_loss: 1.4251 - val_acc: 0.7090\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.3362 - acc: 0.7620 - val_loss: 1.4142 - val_acc: 0.7030\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.3224 - acc: 0.7617 - val_loss: 1.4000 - val_acc: 0.7110\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.3095 - acc: 0.7659 - val_loss: 1.3939 - val_acc: 0.7140\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.2972 - acc: 0.7669 - val_loss: 1.3805 - val_acc: 0.7080\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.2847 - acc: 0.7717 - val_loss: 1.3723 - val_acc: 0.7250\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.2738 - acc: 0.7720 - val_loss: 1.3621 - val_acc: 0.7150\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.2626 - acc: 0.7763 - val_loss: 1.3590 - val_acc: 0.7180\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.2522 - acc: 0.7764 - val_loss: 1.3478 - val_acc: 0.7240\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.2414 - acc: 0.7809 - val_loss: 1.3365 - val_acc: 0.7210\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.2311 - acc: 0.7827 - val_loss: 1.3328 - val_acc: 0.7220\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.2215 - acc: 0.7851 - val_loss: 1.3220 - val_acc: 0.7210\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.2122 - acc: 0.7908 - val_loss: 1.3175 - val_acc: 0.7260\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.2028 - acc: 0.7897 - val_loss: 1.3074 - val_acc: 0.7240\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.1938 - acc: 0.7936 - val_loss: 1.3039 - val_acc: 0.7270\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.1853 - acc: 0.7940 - val_loss: 1.2958 - val_acc: 0.7270\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.1765 - acc: 0.7977 - val_loss: 1.2907 - val_acc: 0.7260\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.1686 - acc: 0.7997 - val_loss: 1.2831 - val_acc: 0.7320\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.1605 - acc: 0.8037 - val_loss: 1.2807 - val_acc: 0.7290\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.1528 - acc: 0.8019 - val_loss: 1.2746 - val_acc: 0.7350\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.1451 - acc: 0.8052 - val_loss: 1.2687 - val_acc: 0.7380\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.1379 - acc: 0.8095 - val_loss: 1.2622 - val_acc: 0.7380\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.1306 - acc: 0.8100 - val_loss: 1.2550 - val_acc: 0.7380\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.1231 - acc: 0.8107 - val_loss: 1.2521 - val_acc: 0.7410\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.1163 - acc: 0.8129 - val_loss: 1.2464 - val_acc: 0.7360\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.1093 - acc: 0.8164 - val_loss: 1.2437 - val_acc: 0.7440\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1028 - acc: 0.8148 - val_loss: 1.2386 - val_acc: 0.7370\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0959 - acc: 0.8196 - val_loss: 1.2376 - val_acc: 0.7450\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0898 - acc: 0.8195 - val_loss: 1.2289 - val_acc: 0.7330\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0836 - acc: 0.8225 - val_loss: 1.2248 - val_acc: 0.7400\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0767 - acc: 0.8243 - val_loss: 1.2227 - val_acc: 0.7470\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0712 - acc: 0.8241 - val_loss: 1.2156 - val_acc: 0.7380\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0648 - acc: 0.8257 - val_loss: 1.2120 - val_acc: 0.7460\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.0590 - acc: 0.8257 - val_loss: 1.2117 - val_acc: 0.7460\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.0532 - acc: 0.8287 - val_loss: 1.2066 - val_acc: 0.7460\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.0475 - acc: 0.8283 - val_loss: 1.2009 - val_acc: 0.7430\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0422 - acc: 0.8332 - val_loss: 1.1997 - val_acc: 0.7480\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0364 - acc: 0.8303 - val_loss: 1.1970 - val_acc: 0.7510\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0309 - acc: 0.8345 - val_loss: 1.1906 - val_acc: 0.7470\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0254 - acc: 0.8331 - val_loss: 1.1858 - val_acc: 0.7470\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0200 - acc: 0.8381 - val_loss: 1.1863 - val_acc: 0.7550\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.0149 - acc: 0.8373 - val_loss: 1.1796 - val_acc: 0.7550\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.0094 - acc: 0.8376 - val_loss: 1.1758 - val_acc: 0.7510\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0050 - acc: 0.8396 - val_loss: 1.1742 - val_acc: 0.7520\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9995 - acc: 0.8423 - val_loss: 1.1736 - val_acc: 0.7530\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9946 - acc: 0.8409 - val_loss: 1.1710 - val_acc: 0.7550\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9896 - acc: 0.8453 - val_loss: 1.1676 - val_acc: 0.7580\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9849 - acc: 0.8447 - val_loss: 1.1608 - val_acc: 0.7540\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9799 - acc: 0.8459 - val_loss: 1.1584 - val_acc: 0.7540\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9754 - acc: 0.8483 - val_loss: 1.1563 - val_acc: 0.7530\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9706 - acc: 0.8488 - val_loss: 1.1537 - val_acc: 0.7600\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9661 - acc: 0.8495 - val_loss: 1.1531 - val_acc: 0.7590\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9615 - acc: 0.8500 - val_loss: 1.1500 - val_acc: 0.7570\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9567 - acc: 0.8513 - val_loss: 1.1487 - val_acc: 0.7580\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9525 - acc: 0.8541 - val_loss: 1.1443 - val_acc: 0.7560\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9483 - acc: 0.8541 - val_loss: 1.1409 - val_acc: 0.7600\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9438 - acc: 0.8544 - val_loss: 1.1375 - val_acc: 0.7540\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9392 - acc: 0.8559 - val_loss: 1.1354 - val_acc: 0.7600\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9353 - acc: 0.8569 - val_loss: 1.1315 - val_acc: 0.7580\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9310 - acc: 0.8576 - val_loss: 1.1316 - val_acc: 0.7600\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9265 - acc: 0.8584 - val_loss: 1.1293 - val_acc: 0.7590\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9222 - acc: 0.8581 - val_loss: 1.1258 - val_acc: 0.7600\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9184 - acc: 0.8596 - val_loss: 1.1237 - val_acc: 0.7580\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9146 - acc: 0.8609 - val_loss: 1.1266 - val_acc: 0.7650\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9106 - acc: 0.8609 - val_loss: 1.1204 - val_acc: 0.7570\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9064 - acc: 0.8629 - val_loss: 1.1168 - val_acc: 0.7640\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9025 - acc: 0.8632 - val_loss: 1.1130 - val_acc: 0.7640\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8984 - acc: 0.8640 - val_loss: 1.1173 - val_acc: 0.7600\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8947 - acc: 0.8660 - val_loss: 1.1105 - val_acc: 0.7630\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8909 - acc: 0.8691 - val_loss: 1.1066 - val_acc: 0.7550\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8871 - acc: 0.8677 - val_loss: 1.1075 - val_acc: 0.7620\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8832 - acc: 0.8697 - val_loss: 1.1056 - val_acc: 0.7630\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8795 - acc: 0.8707 - val_loss: 1.1029 - val_acc: 0.7590\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8759 - acc: 0.8713 - val_loss: 1.1021 - val_acc: 0.7650\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8720 - acc: 0.8716 - val_loss: 1.0997 - val_acc: 0.7600\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8683 - acc: 0.8720 - val_loss: 1.0973 - val_acc: 0.7650\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8647 - acc: 0.8736 - val_loss: 1.0943 - val_acc: 0.7620\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8611 - acc: 0.8756 - val_loss: 1.0928 - val_acc: 0.7620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8578 - acc: 0.8736 - val_loss: 1.0936 - val_acc: 0.7620\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "random.seed(123)\n",
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu',kernel_regularizer=regularizers.l2(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(Dense(25, kernel_regularizer=regularizers.l2(0.005), activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L2_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see how regularization has affected our model results.  \n",
    "\n",
    "Run the cell below to get the model's `.history`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2_model_dict = L2_model.history\n",
    "L2_model_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the training accuracy as well as the validation accuracy for both the L2 and the model without regularization (for 120 epochs).\n",
    "\n",
    "Run the cell below to visualize our training and validation accuracy both with and without L2 regularization, so that we can compare them directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzsnXd4FUXXwH8nvZCQQhISSAhdqpQgSFFBkCIq1hcUARui2MWCvvoK9o4VRRQrIuJHEVAQEClKSei9Q0IKgfSee+98f8xNSAMCclNgfs+zz727O7t7du/eOTNnzpwjSikMBoPBYABwqm4BDAaDwVBzMErBYDAYDMUYpWAwGAyGYoxSMBgMBkMxRikYDAaDoRijFAwGg8FQjFEKNQQRcRaRLBGJOJ9lazoi8r2IvGT/fpWIbK9M2XO4zgXzzAxVz79592obRimcI/YKpmixiUhuifU7zvZ8SimrUqqOUurI+Sx7LohIFxHZICKZIrJLRPo64jplUUotV0q1OR/nEpFVIjKqxLkd+swuBso+0xLbW4nIPBFJFpEUEflNRJpXg4iG84BRCueIvYKpo5SqAxwBriux7Yey5UXEpeqlPGc+BeYBvsAg4Gj1imM4FSLiJCLV/T+uC8wBWgIhwCZgdlUKUFP/XzXk9zkrapWwtQkReUVEfhKRH0UkExguIpeLyBoRSRORBBH5UERc7eVdRESJSKR9/Xv7/t/sLfZ/RKTx2Za17x8oIntEJF1EPhKR1RW1+EpgAQ4rzQGl1M4z3OteERlQYt3N3mJsb/9TzBKRRPt9LxeRVqc4T18ROVRivbOIbLLf04+Ae4l9gSKy0N46TRWRX0WkgX3fm8DlwGf2ntukCp6Zn/25JYvIIREZLyJi33eviPwlIu/bZT4gItec5v7/ay+TKSLbReT6Mvvvt/e4MkVkm4hcat/eSETm2GU4LiIf2Le/IiJflzi+mYioEuurRORlEfkHyAYi7DLvtF9jv4jcW0aGm+zPMkNE9onINSIyTETWlin3jIjMOtW9VoRSao1S6iulVIpSqhB4H2gjInUreFY9ReRoyYpSRG4VkQ32791E91IzRCRJRN6u6JpF74qIPCciicAX9u3Xi8hm+++2SkTaljgmqsT7NENEfpaTpst7RWR5ibKl3pcy1z7lu2ffX+73OZvnWd0YpeBYbgSmo1tSP6Er20eBekAPYABw/2mOvx14AQhA90ZePtuyIhIMzASesl/3IHDZGeReB7xbVHlVgh+BYSXWBwLxSqkt9vX5QHOgPrAN+O5MJxQRd2Au8BX6nuYCQ0oUcUJXBBFAI6AQ+ABAKfUM8A8wxt5ze6yCS3wKeAFNgD7APcCIEvu7A1uBQHQl9+VpxN2D/j3rAq8C00UkxH4fw4D/Anege143ASmiW7YLgH1AJBCO/p0qy53A3fZzxgFJwLX29fuAj0SkvV2G7ujn+CTgB/QGDmNv3UtpU89wKvH7nIErgDilVHoF+1ajf6srS2y7Hf0/AfgIeFsp5Qs0A06noBoCddDvwIMi0gX9TtyL/t2+AubaGynu6Pudin6ffqH0+3Q2nPLdK0HZ36f2oJQyy79cgENA3zLbXgGWneG4ccDP9u8ugAIi7evfA5+VKHs9sO0cyt4NrCyxT4AEYNQpZBoORKPNRnFAe/v2gcDaUxxzCZAOeNjXfwKeO0XZenbZvUvI/pL9e1/gkP17HyAWkBLHrisqW8F5o4DkEuurSt5jyWcGuKIVdIsS+8cCS+zf7wV2ldjnaz+2XiXfh23AtfbvS4GxFZTpBSQCzhXsewX4usR6M/1XLXVvL55BhvlF10UrtLdPUe4LYIL9ewfgOOB6irKlnukpykQA8cCtpynzBjDF/t0PyAEa2tf/Bl4EAs9wnb5AHuBW5l7+V6bcfrTC7gMcKbNvTYl3715geUXvS9n3tJLv3ml/n5q8mJ6CY4ktuSIil4jIArspJQOYiK4kT0Viie856FbR2ZYNKymH0m/t6VoujwIfKqUWoivKxfYWZ3dgSUUHKKV2of9814pIHWAw9pafaK+ft+zmlQx0yxhOf99FcsfZ5S3icNEXEfEWkakicsR+3mWVOGcRwYBzyfPZvzcosV72ecIpnr+IjCphskhDK8kiWcLRz6Ys4WgFaK2kzGUp+24NFpG1os12acA1lZAB4Bt0LwZ0g+AnpU1AZ429V7oY+EAp9fNpik4HbhZtOr0Z3dgoeifvAloDu0VknYgMOs15kpRSBSXWGwHPFP0O9ucQiv5dwyj/3sdyDlTy3Tunc9cEjFJwLGVD0H6ObkU2U7p7/CK65e5IEtDdbABERChd+ZXFBd2KRik1F3gGrQyGA5NOc1yRCelGYJNS6pB9+wh0r6MP2rzSrEiUs5HbTknb7NNAY+Ay+7PsU6bs6cL/HgOs6Eqk5LnPekBdRJoAk4EH0K1bP2AXJ+8vFmhawaGxQCMRca5gXzbatFVE/QrKlBxj8ESbWV4HQuwyLK6EDCilVtnP0QP9+52T6UhEAtHvySyl1JunK6u0WTEB6E9p0xFKqd1KqaFoxf0u8IuIeJzqVGXWY9G9Hr8Si5dSaiYVv0/hJb5X5pkXcaZ3ryLZag1GKVQtPmgzS7bowdbTjSecL+YDnUTkOrsd+1Eg6DTlfwZeEpF29sHAXUAB4Amc6s8JWikMBEZT4k+Ovud84AT6T/dqJeVeBTiJyEP2Qb9bgU5lzpsDpNorpBfLHJ+EHi8oh70lPAt4TUTqiB6UfxxtIjhb6qArgGS0zr0X3VMoYirwtIh0FE1zEQlHj3mcsMvgJSKe9ooZtPfOlSISLiJ+wLNnkMEdcLPLYBWRwcDVJfZ/CdwrIr1FD/w3FJGWJfZ/h1Zs2UqpNWe4lquIeJRYXO0DyovR5tL/nuH4In5EP/PLKTFuICJ3ikg9pZQN/V9RgK2S55wCjBXtUi323/Y6EfFGv0/OIvKA/X26Gehc4tjNQHv7e+8J/O801znTu1erMUqhankSGAlkonsNPzn6gkqpJOA/wHvoSqgpsBFdUVfEm8C3aJfUFHTv4F70n3iBiPie4jpx6LGIbpQeMJ2GtjHHA9vRNuPKyJ2P7nXcB6SiB2jnlCjyHrrnccJ+zt/KnGISMMxuRnivgks8iFZ2B4G/0GaUbysjWxk5twAfosc7EtAKYW2J/T+in+lPQAbwf4C/UsqCNrO1QrdwjwC32A/7He3SudV+3nlnkCENXcHORv9mt6AbA0X7/0Y/xw/RFe2flG4lfwu0pXK9hClAbonlC/v1OqEVT8n5O2GnOc90dAv7D6VUaontg4Cdoj323gH+U8ZEdEqUUmvRPbbJ6HdmD7qHW/J9GmPfdxuwEPv/QCm1A3gNWA7sBlac5lJnevdqNVLaZGu40LGbK+KBW5RSK6tbHkP1Y29JHwPaKqUOVrc8VYWIxACTlFL/1tvqgsL0FC4CRGSAiNS1u+W9gB4zWFfNYhlqDmOB1Re6QhAdRiXEbj66B92rW1zdctU0auQsQMN5pyfwA9ruvB0YYu9OGy5yRCQO7Wd/Q3XLUgW0QpvxvNHeWDfbzauGEhjzkcFgMBiKMeYjg8FgMBRT68xH9erVU5GRkdUthsFgMNQqYmJijiulTueODtRCpRAZGUl0dHR1i2EwGAy1ChE5fOZSxnxkMBgMhhIYpWAwGAyGYoxSMBgMBkMxDlUK9klTu0Un9SgXv0V0kpGlIrJFdPKVsgGrDAaDwVCFOEwp2MMpfIIOktYaHYemdZli7wDfKqXao8NIv+4oeQwGg8FwZhzZU7gM2Kd0OscCYAblZ022RichAR2k62KYVWkwGAw1FkcqhQaUTjQRR/k4/pvRSTZARzD0sYeiNRgMBkM14Mh5ChUlUSkbU2Mc8LHoJPIr0ElOLOVOJDIaHaefiIhalQPbYDAYKo/NBnv3ws6dUGhPgJeXBykpkJoKgwdDVJRDRXCkUoijdMz2huiQzcUopeLRcfKxp3G8WVWQ7FspNQUdx52oqCgTrMlgMNReMjJg1SrYvh3274cjRyA9XW8/eBCys099bP36tVoprAea27NaHQWGolPvFSMi9YAUe5al8cBXDpTHYDAYHItScOwY7NihK/2tW/V3iwW8vCArC2JiwGpPzR0YCI0agb8/hIRA797QsSO0aweenrqMu7ve7+cHzhVlbz2/OEwpKKUsIvIQsAidJP0rpdR2EZkIRCul5gFXAa+LiEKbj8Y6Sh6DwWA4Z2w2iI3VFXx2tq7gXVx0y37PHt3iP3wYDh2CtLSTxwUEQNu24Ourj/P0hPHjoU8fXfn7+VXbLZ2KWhc6OyoqSpnYRwaD4byzbh18/DGsXAk+Prp1np8PycmQmAg5ORUf5+kJTZroFn+jRnDJJdC6tV5CQ0EqGl6tekQkRil1RttTrQuIZzAYDJXGZoOjR3Vr/sgRXcGnpGgzj7Mz5ObqFv6uXXpwt04dGDgQCgr0wK6vLzRtqm35LVvqit7PTyuI/HytBBo2BKfz58h5IucE245tIyYhhuj4aHItuXQO7UxUWBRdG3TF39P/vF2rIoxSMBgMtROrVQ/Y7tgBcXGQkKAr/CJPndRUOHFCe++UxNVVV+JWq7bXN2oEkZHw4IMwYoRWBOdIobUQFycXxN47KLQWsiN5B0fSj5CYlUhSdhIpuSmk5qWSXZBNvjWffEs+OYU5ZBdmE5cRx7HsY8XnC/cNx9PVkzm75gDw4YAPebjrw+csX2UwSsFgMNRMrFbdit+3T9vsDx3SdvyAAD2YO326VgagW/316+t9/v7QrJn+DAzULf2WLXXlHxSkewPnyaRjUzb2ntjLHwf+YM6uOSw/tBwvVy+aBjTF1cmVLUlbyLeWznxbx60O/h7+eLt54+7sjpuzG95u3oTWCaVDSAfaBLehdVBrOtbvSEidEAAy8jPYkLCBZgHNzovcp8OMKRgMhuohNxeSknTFvm2b9tQ5fFi38I8f10qgoOBkeTc37cVjs2kl0L+/btn37KkVggM8c5KykohJiGFH8g5yC3PJt+aTnpdOck4y8ZnxbErcRGZBJgCt6rViUPNB5FvyOZB2gDxLHp3qd6JzWGeaBTSjfp36BHsH4+bsdt7lrAxmTMFgMFQfSmkvnLKNzqNH4aef4Mcf4cCB0vt8ffWAbUAAXHop3HgjtGihW/1Nm54ctM3MPFm+kuQW5nIi9wTJ2cmk5KYgIni4eJBTmMOeE3vYc2IP+1L2sT91P4lZiSilsClbcYVfhJM44evuS5BXECF1Qhhx6QiiwqLoHt6dFoEtzuVJ1TiMUjAYDOdOYiJER+sWvtWq7ffR0bBihe4FVISTE/TtC3ffrSv60FA9gBsRUSmzTr6XO/tT96OOKRSKQmshBdYCMvIz2Juylz0n9nAo7RBHM48SnxlPSm4KeZa8057T29WbZgHNaB3Umn5N+uEkeuA43DecLg260C64HT7uPrg4XfhV5oV/hwaD4dxRSnvtrF2rK/oVK/SArrOzVgLHj5c/Jjwc+vXTfvguZaoYb2+49lpt7jkDVpuVrIIskrKTOJB6gD0n9rD04FKWHlhKduGpZ/16u3oT6RdJeN1w2ge3J9ArEH8PfwK9AgnyCiLQKxClFPnWfNyc3WgR2ILQOqHFg8MXO0YpGAwGXcHHxekB3aLYOzt3wsaN2o0TdIXevbu24dtsWmG0aqXDLjRrpm3+Li7ax/8MFaxN2dh2bBsrDq9gffx6UnNTSc9PJy0vjdTcVFLzUskqyCp3XKRfJCMvHUn38O7FtnlXZ1c9WGtv7Yf5hJkK/l9glILBcLFgs8GyZTBjhrbLu7joWbZ792oPn5KDul5eusK/9lro0kUvHTpod87TkFuYizsKJ4QCawEzts1gcvRk0vPS8ff0x93ZnbiMOI6kHyn2ygnzCSPEOwRfd18i/SLpWL8j/h7+1PWoi6+7L/W86tHEvwlN/ZtSv059U+E7GKMUDIYLjaLWfkqK9tNPStK2/2XLdFgGPz9tvrFYtJ9+ixa68m/eXA/oNm2qTUCVmJB1IucEMQkxrDy8kj8O/MH6+PW4OLnQ2K8x6fnpJGYl0jqoNW2C25CSm0JuYS4d6nfghpY30C6kHVc2upJGfo2q4KEYKotRCgZDbSUjQ5t3Dh3Sdv79+2HpUl3xl8TJSQdba9MGXn1Ve/V4eJzytPmWfNztCuFI+hFeWfEK3235Di9XL4K8gvBw8SDfmk9mfiZHM4/qS4gTXRt0ZXzP8RRaC9mfuh+bsjG682j6N+1vWve1CKMUDIaajNWqQzRs3ap9+WNjdau/KBBbSZdPPz+48koYN04P8hZN5AoMLOfDn1OYw+fRn7MhcQN9IvvQv1l/Vh1ZxQdrP+Dv2L8J8Q6haUBTouP1nKA72t2Bp4snyTnJ5FnycHdxx8vVi9b1WhMVFkVUWBR1PepW5ZMxOAgzec1gqG4KCnSlHxOjW/zu7nr7unWwfLmezAW6xR8Wpk0/4eHQqRN07qzNP/Xr64HgCkjITGDh3oXsT92Pl6sXBdYCpsRMISk7CX8Pf1LzUovLNvFvwm2tb+NY9jH2puyldVBrnuv1HBF1TXKr2o6ZvGYw1FTy83Vlv2wZrF4N69eXHuQtolEjberp1UtP5mrVqtjsU2gtZG/KXrYd20ZG+p+4ZLpQaC1kz4k97Di+g5TcFACyCrLYdmwbAM7ijFXpOP69I3sz89aZ9IroxabETSw5sISW9VpybfNrcXZyfMx+Q83F9BQMBkeRnQ1ffQUffqijarZoAXXramWQmak9eTp31m6eXbtq185GjbSCsFjAx4d8Sz57U/YiCArFysMrmbNbx9gpsJZXJO7O7lxS7xKCvYMREVycXOgZ3pPBLQbTNrgtFpuFAmsB3m4V9yoMFy6mp2AwVCXZ2TqEw5EjsGWLntW7aJH2AOreXSuEPXt0Nq7//AeGDNGJVuzZtZRSbD22lbgDi4or9S/XfMBbf79FfGapLLY0D2jOg1EP0im0E+1C2hHoGYhVWXESJxr4NDhtS9/V2RVX59O7lRouboxSMBjOhtxciI+HevV0S3/mTPj8c1izpnS5iAi45hp46CHo0aPCU+Vb8omJ/ZulB5YyY/sMdiTvKFfmikZX8MbVb+Dm7IZN2ehQvwOX1LvEePMYHIZRCgZDZbDZ4Pvv4ZlntPdPSS65BCZMgMaNdcKVNm0gOBiA1NxUvl3zAXtO7CGjIIOM/IziGbt7T+wtnsDVM6Innw76lLbBbdl9YjcHUw/Sv1l/rmh0RVXfqeEix6FKQUQGAB+gczRPVUq9UWZ/BPAN4Gcv86xSaqEjZTIYTovFor1+1qzR8X5OnNDeQEePwubN2vb/8suQnq6jgPbti+rVCyu24mBpSili4qP5auNXfLP5G3IKcwjwDKCuu56h6+/pT/OA5gxoOoAeET3oHt6dYO/gYhF6NepVXXdvMDhOKYiIM/AJ0A+IA9aLyDylVMk+8n+BmUqpySLSGlgIRDpKJoPhlPz9t+4JzJp1MtZPZKR2AU1L03F9vv4a7ryzeKZvel463235js8+G8vuE7tpVa8VbYLbsO7oOg6kHsDN2Y3b293Oo10fpUP9DtV2awbD2eDInsJlwD6l1AEAEZkB3ACUVAoKKAqKXhcoPaJmMJxvbDY9J6CgQAdu27cP3nhDu4Z6ecHgwXDLLdoNtEwkzwJrAfN3z+GP/X8QnRDN5sTNFNoK6RLWhce6Psb25O2sPrKaVkGteK7nc9zY6kYCPAOq6UYNhnPDkUqhARBbYj0O6FqmzEvAYhF5GPAG+lZ0IhEZDYwGiIgwk2gM58CGDTB1KsydqweKSxIRod1G77pLp2pEh23+Y9/v7Dq+i+yCbI5mHmXm9pmcyD1BXfe6dA7rzOPdHue2NrfROaxzNdyQweAYHKkUKnKPKDspYhjwtVLqXRG5HPhORNoqpWylDlJqCjAF9DwFh0hruLBQSscG2rsXXnsNZs/WPYEBA+D663Xoh8xM8PQk+arLeHPde5xY+jANfRpisVn4fuv3xGXEFZ/Ow8WD61pcx90d76Zfk35mgpfhgsWRSiEOCC+x3pDy5qF7gAEASql/RMQDqAccc6BchgsNm02PCcydq+MD7dypcwNY9exdfH21d9Bjj5VK4ZhdkM3UDVP532ejyC7MJsQ7hISsBJRSXNP0Gib1n0Tvxr2p41an2vLqGi4SDhyAe+7R81a++043WopQSjs2xMdDUJBeHIgjlcJ6oLmINAaOAkOB28uUOQJcDXwtIq0ADyDZgTIZLhQKCmDVKvjtN/j5Z50O0s1Np3Xs0UMPEgcG6vkEgwdj8fNl27FtbDm4hc2Jm1kdu5qYhBgsNgv9m/bn/f7v0yqoFRabhdzCXHzcfar7Dg2gvcEyMnRwv7MlIwP++ksHCTyLfM7F112+XCcPiow8uX3TJu2R1quXft8KC/V7mJqqY1E1socBT03VvdToaD2G1bcv3Hxz+eRDViv88IOezyKiQ6B06aIdHg4c0HNgVq/W82MAPvsM7r//7J/FWeDQMBciMgiYhHY3/Uop9aqITASilVLz7B5HXwB10Kalp5VSi093ThPm4iJnzx6YNEm3prKy9ASyq6+GO+6AG27Qg8clyC7IZtqmabz7z7scSjsE6FAQUWFRXNnoSq5peg1XNLrCTAaraRQWwrffavffw4f1uE+XLjBmjK5gQbegly+HP//UlW9sLFx1FVx3nR5DevttPaM8IEBHjh0yBI4d0/kl/PygQQMdSyohQS9F8ad274Yvv9Qtcy8v/b6NHAn/+x+8+aa+rq+vdk9ev157pxVRt67OU52ff3Kbp6eu1Pv3hxde0PcTHa2P3bBBh0Dp1Uu/04mJOt5VQoI+NiJCr0dEaE+4rl31fJhzoLJhLkzsI0PNpaAAvvkG5s3Tf6rMTP1HcnWF22/Xf5Y+fYoHh23KxsK9C/lo3UdsStyExWYhpzCHPEse3cO7M6bzGKLComge2Lz2J2C32SqVBKeYFSt0ZXfvvSdbq2lpOgdDx46nP9eePTBlysmKzttbV1DBwbrSjY/XFe9995VTylitehB//XpdrqBAh/kYMUK3tJcs0TkgYmP1/ix7Cs6UFF1BdumiK/OtW/U9xMfr44cOhbfegn/+0bK3aaO9xVatOtmqvvZa7TwwbRosWFD5ZyWix55GjNDOCUuX6ns9dkw/v8GDYf58fe0uXfQYVYMGuoLfskW/j2FhutcQFQWhofDpp/Df/+p3GLQy6thRH3/55XDrrSfDm8fHw+TJ0K2blsP5/IxfGaVgqL1kZsL06fD667pV1by5NgN5eOj8wGPHQkgIB1MP8t4/77E9eTv51vziNI8NfBowqPkg3JzdcHd256ZWN9EjouJQE7WSl1+Gd97RS1Elv2uXDrnh56croW7ddHht0JXX1VfryvLWW3UluWaNrvTi47WJ5P77dW8rNPTkdXJz9W/w5pt63a58yczULfkinJy0kgoMhKee0rbxevW0CWXYMB0DqkkTPds7O1uHCPf01C3uvDx93shIfW1fX30/rq762MGDTyqxvDytCF57TSuohg11RTt8+Mmw4Tk5uudQv74ONlhEdLQeaypSZmlp+t5zc/W20NCTiYfq1i2ekY7NBu+9p3sOr74KN9107r9bQoI2Z7VpoyPeulRtw6SySgGlVK1aOnfurAwXIBaLUkuXKjVypFJeXkqBUl27KvXbb0rZbEoppWLTY9Xve39XU2OmqpGzRyrnCc7K7WU31ePLHurqb65WQ2YMUTO2zlAFloLqvZeSbN+u7+nNN/U9no5//lFq0aLTl/n8c/1sQkP15403KjV8uFJOTnq9aPHwUGriRKU2b1YqMFCpZs2UeuklXS4iQikRpS65RKmPP1aqZ8+Tx3XpotTYsUr17q2Ur6/edscdSiUknJTBalUqOVnfW3y8vq9165QaNEiXd3JSqlcvfU1XV6WmTCl9Dxs3KvXww0o9+qj+zQvO8vfav1+pX35RKjf37I67yEGb7c9Yx1Z7JX+2i1EKFxj79yv15JNKhYXp19HHR6n77lPxi35RWxI2qz3H96iFexaqa3+4VslLongJxUsoj1c81CMLH1Gx6bHVfQcVs2ePrkxFlHJz0/d2xRVKHTlSvmx+vlLPPqvLurjoCrYkNptSKSlKTZ+uK9yBA/Ux77yjK11PT6WeekqpY8d0ZR0To9Rtt52s6IOClNq3T5/rt9+Uql9fqfvvVyo7++Q1duxQ6tVXtSL29NTK4YEHlPrzz7O7740blXrhBaUuvVSpJk2UWrny7I6vRrILstXYBWNVsw+bqVWHVxVvX35wubrhxxvUwO8HqoHfD1RP/P6E2pW8SymlVFZ+lpq7a65acWhFqXPlFeap1UdWq5+3/6w+XvuxWhe3TtnsjZvF+xarzp93Vn2/7atWHl6pbDabmr1ztoqaEqUu+fgSNW7ROLX0wFK1IX5DuSU5O/mc76+ySsGYjwxVT2qqdiGdNk3PH3BygkGDULffzqr2fryx8UMW7i0dAivEO4TRnUfTr0k/wuuGE+YTVvVuollZ2jacl6cD4xVlSCsiL097nLz3nh4k9fDQXiVPPQULF2qzl1LahhwVpc0U8fHapLBli7Z/L1miTSsbNmib+zPPaI+TIjt5ly7aPFJkLjl4UJtfKnJTXLxY2/MnTChtSlGqvBeMA0nJTTmjW69N2fhk3SdEJ0QTVieMBr4NaB/Sno71O+Lj7kNuYS7Hso8RUicEDxdt5tmYsJEftv5Am6A2jOwwEidxIrsgm/fXvE9SVhJRYVG0rNeS3cd3Ex0fTXp+OmE+YQR7B5OWl0Z8ZjxKKTqFdiKibgTPLHmGncd3EuIdQkpuCh8N/IhDaYd4c/WbhPqE0sCnATZlY3PSZiw2Cx3qd2DX8V3kWfIA+G+v/zKh9wS2JG3h9l9uZ+fxnaXusVNoJ8J9w5m7ey5N/ZuSVZBFUnYSDXwacDTzKM0CmtHEvwl/HvyTQlthuWcEMPnayYyJGnNOv4MZUzDULHJzdcKZzz7TcwkA/PzIuWcEP1zhz6K8baw9upa4jDiCvIJ4+LKHaRXUijxLHr7uvgxoNsBxSiAuDh58UNt4778frrhCK6svvtCm/EnYAAAgAElEQVT286goXel+/vnJuEgdOmhXwoQEfU/LlunBUdDK4MEH4emnISTk5HX279fjAOvWaSVgsWjFEBEBL72k7dV//qnt/3feqeVatkzb+jt31oOZAweWH8x1MPmWfObtnkeuRSumZgHNuLzh5af02MouyGZj4kb+OvQX8/bMY93RdQR7B3NPx3sY3n44vu7aPdTHzQdfd1+OZh5l5JyRLDu4rLhCLqoUBcHX3Zf0/HQAXJxcaBfcDidxIiYhBidxwqZsdGvYjVGXjuK1Va9xJP0I3q7eZBdmF8tUx60OAZ4BJGQmUGgrxEmcCPEOwWKzkJyjf9PQOqF8M+QbosKiGPbLMBbtXwTAvR3v5f0B71PHTY+pJGYlMm3jNBbsXUBUWBSDWwxmxrYZfLnxS7o17MaGhA0EegbyzjXv0Da4Lf4e/vy651cmR09mX8o+xvccz9M9nsambExeP5nf9v3G8PbDGd5+OC5OLmTkZ/B37N/kW0p4MNlpH9Kexv7G+6gURinUMqxW+OgjPWB57Jh2qRs8mIT2TXitcBlTd/1AniWPxn6N6dawG30a99FJ4l09q0a+v//WlXF2tm6hJydr5WCx6AHYiAg9MJqervMjTJigy9x9Nxw/rs8REKA9oRo31oOWAwaUHrCtiPx8fY2K8io/9ZRWHm5u2utn5Mjzf98lRbHksyZuDU38mxBeN7zUvoTMBG6aeRNr4krni2gT1Ia7OtyFq7Mr8ZnxxUtcRhx7U/Ziswcl6NqgKwObDWRD4gbm75lfvL0Ib1dvFApB+HDgh9zV4S4UiqSsJDYmbiQ6Pppj2ccI8wkjyCuIg2kHiY6PJi0vrbginb9nPuMWjyM5J5m2wW35dNCndA/vzp4Te9h9YjctA1vSIrAFzk7O2JSN9Lx0fNx9cHFyQSlFXEYc245to2vDrsWxqqw2K5PWTKJ5YHOub3n9GZ+hUopP1n/CY78/xuAWg5l6/VTqedUrV8ambNU2G94oBUP1s3OnNomsXat9y194AXr14sdtMxg9fzSF1kKGtx/O490ep01wm/N//SKPHHd33cq2WrUXysaNWgkA7NihvXTmztVeTrNnw8qVJzOjFXnWpKaWnmWamAjvvw/t2ukAekWeK+eD/HytfIYMgcsuO23RfSn78PfwJ9DrpGxKqQpTdbo6u+IkTsVllhxYwhcbvuD3fb+TWZCJkzhxbfNrGd5+OD5uPmTkZ/DE4idIz0tnynVT6NawG0oplh9azmcxnxEdr/+Hrk6uxeaVUJ9Q2gW3IyosissaXFYqJHhseixLDizBqqwopUjPTyc+M56cwhyevPxJmgc2P+dHlpKbwsrDKxnUfFC1ZpbLyM/Ax82nRs57MUrBUD0cP67t5/Pnw7x5qDp1WP3UUBZF+eHrUZftydv5ZvM39AjvwfSbpxNR9ywCHCp7PKP4eF2pd+igW/VWq678v/lGm1ZCQ7Vf+/Ll5c/h7a1nnvr76/WwMO1qeC4zZs8z/8T+Q1xGHLe0vqW4Uvlyw5esPbqWjwZ+hLvLyTEMq83KW6vf4sXlLxJaJ5TFdy7mknqXEJseyy0/38K6o+vKnd/fw59BzQdxRaMr+H7L96w8spJg72BuaHkDA5oNICY+hi83fklSdlLxMZF+kcwdOpf2Ie3Lne9w2mG83bwJ8AwoVjaGmotRCoaq4/hxPRtzzhw9echmg/r1ybm2P/dHxfN90h/FRQXh2Z7PMrH3xFNPIEtJ0dP8U1P1emqqNuHExJzcBrpiHzhQhx7YsUP7wru5nYwRc999uqfi6alt/zab7g2cp8lA50J2QTZLDixhwd4F+Hv488TlTxDsHVxserAqKzdeciOfXvspE5ZP4LOYzwC4p+M9fHHdF4gIsemx3Dn7Tv46/BdDLhnCP7H/YLFZeLXPq7y4/EVyC3N5vNvjxQOyRew+sZsFexdwPOc4YT5hPN/ree7peE8pZVNgLWBL0hasNh03qk1wm2JbuqF2Y5SCwfHk5Wnvlldf1S34du1gyBCO97mcr5w38+7a90nPS+ftfm8z9rKx5BbmYlXW4oFGdu3Sg7T/93/avBMVpc8zc6Y+dxGurvrcnTtDy5a6dS+i4x4tWKAnKv3vfzq2zNnM8nUAR9KPMGfXHH7d8yvrj64vZ0PPteRisVnwdfclqyALd2d3Lg+/nGUHl3F9y+vp3rA7L/z5AgqFxWbhmR7P4CROvL7qdT4c8CEhdUK4f/79WGwWPh74MSMuHcGB1ANc8/01HEg9QPOA5swdOpdWQa0qlM9qs7I9eTvNA5pX3biNoUZglILBMRw5ogPQrV2rww4kJcHgwSQ+9yizXfYyb888/tj/B1Zl5YpGV/DpoE9PjhccP65t9uvW6bAHmzfrCn/QoJO9ARHteTN6tFYAoMucavZnFbtXFmGxWVi8fzFN/JvQMrAlOYU5vLziZd79510sNgut6rWid2TvUq1w0CG4r258Nb0a9eJQ2iFeXvEyM7bN4NkezzKh9wScxImNCRsZv3Q8Iy8dybB2w7ApGzf9dBPzds9DoejaoCs/3PQDTQOaFp83MSuRbzd/y+jOo/Hz8Kvqx2GoBRilYDi/KKXTUT7yiPbXj4zEdtll/D2wDS/Inyw/tByApv5Nua3NbYzqMIoWgS10RMn167X75s8/60HUwEDdK+jTB0aNOhlSwGrVJh7X6hsoLEmBtYD0PO0K6SRO+Hv64yROrIlbwwMLHmBT4iYAmgc0J8+SR2xGLHd1uIvxPcef1aBpgbXgjO62mfmZDPtlGFFhUTzf6/lqHUw11E4qqxRqeVQwQ5WwcaP2o583j7yel/Pjo31YLPtZeXglRw/PJKJuBK/2eZWbWt1Ey8CWiM2mA3q9P1CH/wUd0+a++/TSrl3FrXtn57O298/fMx8vVy/6NO5Tbp9SivXx62ns15gg71PHoF99ZDVvrn4Tq7ISFRpFmE8Yfxz4g0X7F5FVkFVczsXJhRDvEOIz4wnzCePbId+SWZDJvN3zyCnMYfrN0+kZ0fOs5AcqNf/Cx92H+bfPP+tzGwxni+kpGE7NH39oZfD33ygvL/686yquC1lGji2PcN9wujbsyn/a/Ichlww5OWi8fr0Ob7xhg54Edu21ulfQtWvFPvn/ggOpB2j1SSusNis/3fITN7e+uXjfjuQdPLjgQf46/BdO4kT38O50rN+RpOwkEjITqONWhwY+DTicfpg/DvxBsHcwwd7B7EjegU3ZCPMJ47oW19E2uC2CYLFZOJZ9jPiseMJ9w3mq+1Mm54KhVmF6CoZ/x+efw4MPoiIbET3udsYGr2d9zkJubHEjb/V7i2YBzUqXV0r3Dh59VJuDZs7U/vsOtPePWzwOVydXOtTvwNBfhvKD+gFPF09m75rNd1u+w8fNh/f7v09aXhpzd8/l601fE+oTSv069UnK1pOjBOGtvm/xYJcH8XbzJrsgm/jMeJoFNKuRvuYGg6MxPQVDaZTSvYOJEznYvTX9BqewvyCR1kGtebPvmwxuMbj8MZmZOonJlCm6Z/DDDzp8w3kiIz+DRfsWsXj/YjqFdmJ059EsP7Scvt/15dU+r/LQZQ9xzXfXsPboWgB83X0Z2mYor/R55bRmI4PhYsIMNBvOnoQEHQv/t9/4v8v9uK1vGlc268PT3Z/mmqbXlG45Z2XpYG3LlunkLUrBc8/BxIlnPS4Qmx7L4fTDxGfGE+gZSJ/GfRARbMrGhOUTeGP1GxRYC/By9SKnMIdOoZ3ILsgm35rPzrE78XDxIC0vjcnrJ+uMapFXmpzKBkMZaoT5SEQGAB+g03FOVUq9UWb/+0Bv+6oXEKyUMv501cGcOVjvvgtLdiZPDoSF1/gxe+C3DG4xuLwZ5fhx3SOIidHupLffrj2JelQ+kc2u47uYtnEa8/bMY9fxXaX2DWo+iNf6vMaLy19k3u55DG07lAeiHqB7eHd+2fELjy96nISsBGbdOqt4gpafhx/je43/14/BYLjYcVhPQUScgT1APyAOWA8MU0rtOEX5h4GOSqm7T3de01M4/9i+mILcP4aYMLj3VnduHvIc47qPKz+5KT9feyLddRccOgQ//aRTEZ4FSik+XPshzyx5BquyclXkVQxuPphWQa0IrRPKkgNLeHH5i2QVZOEszrzf/30euuyhUoopIz+DDQkbuLLRlcbubzBUkmrPvAZcDiwqsT4eGH+a8n8D/c50XpNk5/xi+fADpUAtaIa684dbVFx6XPlC+/Yp1aOHTugCStWtq9Rff531taKPRqsB3w9QvIQaPH2wSsxMrLBcXHqcevS3R9WyA8vO+hoGg6FiqGSSHUeajxoAsSXW44CuFRUUkUZAY2DZKfaPBkYDREScRQA1w6lRioKXXsBt4qvMaQk7PnqRb/q+VL7lffAg9O6tA9A98YR2L73iipMTzkqQlpfGT9t+IjErsdR2q7Ly+77fWR+/Hi9XLz4d9CljosacspXfwLcBkwZMOl93ajAYzgJHKoWK/vGnslUNBWYppawV7VRKTQGmgDYfnR/xLmLy8zlxx00E/rKQbzpAweRPeK7bg+XLHT6sFUJWlh5Q7tChwtPtT9nPqytfZca2GcWJWMrSJqgNHw38iOHth5swDAZDDcaRSiEOKJmxoyEQf4qyQ4GxDpTFYCdv326Sb+pP+NbDvDnQl8s+nk3vJn30pLPCQp0qsijY3KhRUFAAS5cWK4TcwlwyCzIJ9g5GKcXXm77m4d8eBuDO9ncyJmoMHeqXVx7G9m8w1A4cqRTWA81FpDFwFF3x3162kIi0BPyBfxwoy0VPTkE2KybcQ493Z1JXKd59pAv3vv6bTs5y/Lj2HsrKgjZt4NJLYfp0HY7ixx+xtW7FD5u/Y9bOWfyx/w9yLbmE+YQR5hNGdHw0V0VexbdDvi2XtctgMNQ+HKYUlFIWEXkIWIR2Sf1KKbVdRCaiBzzm2YsOA2bYB0IMDiAxbjdbh3RjQEwam1v6kTt1Mk/2HHqywFtvQU4OvPGGDlo3fbqemfzGGyh3d8YueJDPYj4jom4E93S8h8b+jdmYuJEdyTt44+o3GNd9XLWlGDQYDOcXM3ntAmf/4p9wuX04DVIt7Hl8BK3f/Kr05LKEBGjaFG69VWcuA53LwMMDpRSP/PYIH6//mGd6PMPrV79uzEAGQy2lsi6pJofeBUz0jPdocO1QXKw29s/+itbvfFN+tvFrr+mxhBdfPLnNQ88QvmfePXy8/mOevPxJoxAMhosEExDvAmXWzxO4+q6XiA90x+3vNbRsUmLwNzYWtm1j/6Y/iZj8Cauvbs6Sw9MIS9HjBMdzjvP8suf1Z6/nebn3y0YhGAwXCUYpXGAopXjn58f5z30fYPNwJ2hFND5N2p4sMG2aDm1dUEBTIMlHeOqydDauegNrCY/grg268vsdv9MxtGPV34TBYKg2jFK4gFBK8fziZ7l23AcEF7jisnoVLi3sCqGwEJ56Cj74gPSeUdzSZjuFoSH8+NBy1gc2wmqzkpyTTHxmPNkF2fSI6IGTGOuiwXCxYZTCBYJN2Xh2ybM4v/02PWLB9sM0nDpFwfz5OrfBggWQkkLWg/fRrMHP1K0Txl+j/iLUtwEAzk7O1K9Tn/p16lfznRgMhurEKIULgC1JW3hgwQNkr/ub6L+cULfdgtPQYfD00/D22xAQAIMHo4YO5c6MKWTty2PV7QtoYFcIBoPBUISxD9Ry3l79Np0+78ThxN38ubQBzvWCkQ8/hHvv1QrhgQcgKQm++YafIzKZs2sOE6+aSMt6LatbdIPBUAMxSqEWEx0fzbNLn+W6ltex78gQ/PcdRb78El55RQ8o/+9/8Mkn4OLCobRDPLTwIaLConj88serW3SDwVBDMeajWorFZuG+X+8jxDuE761D8Ph8FDz5JLRooXMcjBlD3BP38szs4aw4vIK4jDhcnVxZev1SXJzMz24wGCrG1A61lA/WfMCmxE0s7PYx3rc9Apddpiei3XsvuLkxf2gnRkxuT4G1gBsuuYGo0CiuaXoNbYLbVLfoBoOhBmPCXNRC9qfsp/1n7bk6sg9zP0tHtm7VGdFyclBt2/LnzZ24um0MXcK68MNNP9A8sHl1i2wwGKqZGpGj2XD+yczP5IYZN+Du7M6XtuuQlffD559DZCTq1lvJ93Tl1iYxPNTlId7r/x6uzq7VLbLBYKhFGKVQi7ApG3fOvpNdx3ex6D/zCbruEWjVCu6+G7V4MTJrFm9eCcOveoRJAyaZ0BQGg+GsMUqhFjFh+QTm7p7LpP6TuHrJfti7F379FQ4epODWm9gdDOkP3WcUgsFgOGeMUqglrD6ympdXvMzIS0fySKtRcH1zuPJK6NULS9cuZFlymPB4R2beNNkoBIPBcM6YeQq1gJzCHEbNHUVE3Qg+GvgR8vrrkJysJ6eNGQP79jF0qDOv3PODSXZjMBj+FaanUAt4funz7EvZx9IRS/GJOwbvvw8jRnAiL5XAGTN45UroM2oirYJaVbeoBoOhluPQnoKIDBCR3SKyT0SePUWZ20Rkh4hsF5HpjpSnNrLi8Ao+WPsBY7uMpU/jPjBuHBYXJ7o1/ZN1d/fnuCesuLkz47qPq25RDQbDBYDDlIKIOAOfAAOB1sAwEWldpkxzYDzQQynVBnjMUfLURjLyMxg5ZySN/RvzRt83YOlSmDOHiT0sNE0TBu6DjMfGsPCBVcb11GAwnBccaT66DNinlDoAICIzgBuAHSXK3Ad8opRKBVBKHXOgPLWOx35/jCPpR1h510rquHqT9cgYkv2FXwc1Ze2sAAjJp8l/3wUXj+oW1WAwXCA40nzUAIgtsR5n31aSFkALEVktImtEZEBFJxKR0SISLSLRycnJDhK3ZjF752ymbZrG+J7j6R7enZx5v1Bnxz4+uzaEpc1fwW31P/D88+DlVd2iGgyGCwhHKoWK/CLLxtRwAZoDVwHDgKki4lfuIKWmKKWilFJRQUFB513QmsaJnBOMnj+aTqGdePHKFwE49sITHPGF216ZQ8Ckz6F+fbjvvmqW1GAwXGg4UinEAeEl1hsC8RWUmauUKlRKHQR2o5XERc2zS54lNTeVaTdMw83ZjeQ/5hK5NZblN3eic7IzLFkCjz8OHsZsZDAYzi+OVArrgeYi0lhE3IChwLwyZeYAvQFEpB7anHTAgTLVeP6O/ZupG6fyeLfHaR/SHoC4/z7CCU+4YuK38OabULeunp9gMBgM5xmHKQWllAV4CFgE7ARmKqW2i8hEEbneXmwRcEJEdgB/Ak8ppU44SqaaTqG1kDHzxxDuG87/rvofAFuWzaDjuiNE39SNyBxX+OUXePBB8PWtZmkNBsOFiEMnrymlFgILy2x7scR3BTxhXy56Plr3EVuPbeX/bvs/vFy9mLRmEq7PP0kLF+jy2jfw8tvg5gaPPlrdohoMhgsUM6O5hpCUlcSEvyYwsNlAbmh5A7fMvIVft88meYcbMrg/AfkCX38No0dDSEh1i2swGC5QTOyjGsLzy54npzCH9/u/T0xCDLN3zWaax3/wyyjA/e774LnnwN0dXnihukU1GAwXMEYp1ABi4mP4auNXPNr1UVrWa8l3W77D3dmd22LyoV49CAiAWbPgqae0K6rBYDA4CKMUqhmlFI/+/ihB3kG8cMULFFoL+XHbjwxtOAC3Bb/B0KEwfrw2GT35ZHWLazAYLnCMUqhmlh9azurY1Uy8aiJ1Pery+77fOZ5znMfjwiE/HyIjYeVKeOklqFOnusU1GAwXOEYpVDOfRn9KgGcAIy4dAcB3W76jnlc92i3aBC1bwtSp+vOee6pZUoPBcDFglEI1Ep8Zz+yds7m7w914unqSlpfGvN3zGFtvEE6rVun8y7t2wRtvgKuJgmowGByPUQrVyBcxX2BVVsZEjSGnMIcJyyeQb83n3h2eusDq1dC9O9xwQ/UKajAYLhoqNU9BRJoCcUqpfBG5CmgPfKuUSnOkcBcyhdZCpmyYQr8m/Zi/Zz6vr3qdpOwkbmt9Kw2eWwqNG8PBgzBnDpicywaDoYqo7OS1X4AoEWkGfImOYTQdGOQowS50ftnxC/GZ8eRZ8vjjwB9cFXkVP9/6M70SXGHf5TrY3Y036p6CwWAwVBGVVQo2pZRFRG4EJimlPhKRjY4U7ELnicU6skereq14uffL9G7cW+9480E9fpCXZ1xQDQZDlVNZpVAoIsOAkcB19m1m5PMcWXd0HQlZCUSFRrHyrpVIkXkoPx9mzAB/fx0J1fQSDAZDFVPZgea7gMuBV5VSB0WkMfC948S6sBm3eBwAr1392kmFALBgAaSmwrFjMGqUGUswGAxVTqV6CkqpHcAjACLiD/gopd5wpGAXKvGZ8aw6sgpfd1/6Nulbeuf334O3N+TmwogR1SOgwWC4qKlUT0FElouIr4gEAJuBaSLynmNFuzB5dcWrKBTD2w0v3UtIS9M9BRHo1w8aNqw+IQ0Gw0VLZc1HdZVSGcBNwDSlVGeg7xmOMZQhLS+NrzZ9BcCYqDKZ02bPhoICyMqCu+6qBukMBoOh8krBRURCgduA+Q6U54Lm601fk2fJI9IvkrbBbUvv/PFH8PTUEVHNZDWDwVBNVFYpTESnztyvlFovIk2AvWc6SEQGiMhuEdknIs9WsH+UiCSLyCb7cu/ZiV97UErx8bqPARh16ajSpqPERFiyRI8lPP+8nqNgMBgM1UBlB5p/Bn4usX4AuPl0x4iIM/AJ0A+IA9aLyDz7oHVJflJKPXRWUtdClh9azv7U/TiLMyM7jCy986efQCkIC4OxY6tHQIPBYKDyA80NRWS2iBwTkSQR+UVEzjQSehmwTyl1QClVAMwALlq7yGsrXwPgkcseIdIvsvTOj3UPgrfe0tnVDAaDoZqorPloGjq0RRjQAPjVvu10NABiS6zH2beV5WYR2SIis0QkvKITichoEYkWkejk5ORKilxziM+IZ+nBpXi6ePJS75dK79y3Ty9hYTBsWLXIZzAYDEVUVikEKaWmKaUs9uVrIOgMx1Q080qVWf8ViFRKtQeWAN9UdCKl1BSlVJRSKioo6EyXrXk8vuhxFIrnez2Pr7tv6Z2TJunP8ePByQStNRgM1Utla6HjIjJcRJzty3DgxBmOiQNKtvwbAvElCyilTiil8u2rXwCdKylPrSG7IJv/2/V/1HGtw7M9y421w7x5em7CvRfsGLvBYKhFVFYp3I12R00EEoBb0KEvTsd6oLmINBYRN2Ao2gRVjN3NtYjrgZ2VlKfW8Pyy57HYLIzuPBpnJ+fSO9PTITYWWrQwHkcGg6FGUFnvoyPoSrsYEXkMmHSaYywi8hDaldUZ+EoptV1EJgLRSql5wCMicj1gAVKAUed0FzWUjPwMPo/5HEEY32t8+QJFA8z/+U/VCmYwGAynoLJRUiviCU6jFACUUguBhWW2vVji+3iggtrywuC9f94jz5JHz/Ce1POqV77A9On68/HHq1Ywg8FgOAX/ZmTThPA8DSm5Kbzz9zsAPNDlgfIFLBadf7lBA/Dzq2LpDAaDoWL+jVIo60lkKMEHaz4guzAbDxcPbmhZwfSMb74Bmw2uu678PoPBYKgmTms+EpFMKq78BfB0iEQXAFkFWXy8/mNcnVy5qdVNeLt5ly80dar+NNnVDAZDDeK0SkEp5VNVglxITN0wlZTcFABub3t7+QI2G2zYoLOrNWtWxdIZDAbDqfk3A82GCiiwFvDuP+9Sz6segnBN02vKF/rrLx0mu1+/qhfQYDAYToOZQnuemb51OnEZcaTlpXFHuztwda4glfXkyfrznnuqVjiDwWA4A0YpnEeUUrz999s09GmIxWYpHw1VF9JhskVgwICqF9JgMBhOg1EK55HVsavZkbwDV2dX2gW349KQS8sX2rkTUlP1LGZPM1ZvMBhqFkYpnEembpiKt6s3B9MOMvLSkaUT6RTxjT3m3403Vq1wBoPBUAmMUjhPpOWlMXP7TFoEtsBZnLmj/R0VF5w5U3/efNocRQaDwVAtGKVwnpi+dTq5llziM+O5puk11K9Tv3yhQ4f04ukJHTtWtYgGg8FwRoxSOE9M3TCVZv7NSMpOYuSlFQwww8kJa1deCc7OFZcxGAyGasQohfNATHwMGxM3EugViK+7L9e3vL58ocJCmDJFf7/hos1KajAYajhGKZwHvt/yPW7ObmxN2sptrW/D07UCr6K5c6EolWjfvlUroMFgMFQSoxT+JTZlY9bOWbQJakOOJYcRl46ouODkyXosoVEjaNq0aoU0GAyGSmKUwr9k/dH1xGXEUWAtoLFfY3pE9ChfaPduWLZMxzzq109PXDMYDIYaiFEK/5JZO2bh6uTK9uTt3Nn+Tpykgkf6+efg4gL5+cZ0ZDAYajQOVQoiMkBEdovIPhGpIGt9cblbRESJSJQj5TnfKKWYtXMWTf21OWh4++EVFYKfftIzmAH69KlCCQ0Gg+HscJhSEBFn4BNgINAaGCYirSso5wM8Aqx1lCyOYkPCBg6lHSLPmkdUWBTNA5uXL7R9O8THg9UKHTpAUFDVC2owGAyVxJE9hcuAfUqpA0qpAmAGUJEv5svAW0CeA2VxCL/s/AVnceZQ2iFua31bxYV+/11/HjhgTEcGg6HG40il0ACILbEeZ99WjIh0BMKVUvNPdyIRGS0i0SISnVzk1lnNKKWYtWMWTfybAHBbm9MohchIPU/BKAWDwVDDcaRSqMjFpji1p4g4Ae8DZ8xHqZSaopSKUkpFBdUQ88vGxI3sTdlLvjWfrg260sivUflCWVmwciXUqwdubtCrV9ULajAYDGeBI5VCHBBeYr0hEF9i3QdoCywXkUNAN2BebRlsnr51Oq5OrhxJP3LqXsLy5TrDWmIi9OwJXl5VKqPBYDCcLY5UCuuB5iLSWETcgKHAvKKdSql0pVQ9pVSkUioSWANcr5SKdqBM5wWrzcqP236kWYDOr3xL61sqLvj77+DhAXFxMHDLSR4AACAASURBVGxYFUpoMBgM54bDlIJSygI8BCwCdgIzlVLbRWSiiFQQHKj2sPLISuIz48kuzObyhpcTUTei4oKLFkFIiFYMt95atUIaDAbDOeDiyJMrpRYCC8tse/EUZa9ypCznk+lbp+Pp4smR9COMu3xcxYX27dOLlxcMGQJ161atkAaDwXAOmBnNZ0m+JZ9ZO2YRUicEHzefivMwA/zyi/7MyYERp4iHZDAYDDUMh/YULkQW7V9Eal4qGfkZjO0yFl933/KFlIJp0yAgQHsd9etX9YIaDAbDOWB6CmfJt5u/xcvVC6uy8nDXhysutHatDoKXng533KHjHhkMBkMtwCiFsyApK4m5u+eilGJwi8HF3kflmDZN9xCsVrjzzqoV0mAwGP4Fpgl7Fny7+VssNgsWm4VHuz5acaGcHJgxA3x9ITQULr20aoU0GAyGf4FRCpVEKcUXG77Aw8WDloEtubrx1RUXnD0bMjL096efrjoBDYZKUFhYSFxcHHl5tS7UmKGSeHh40LBhQ1xdXc/peKMUKsmKwyvYm7IXgFf6vIKcKlHOtGng5wdpaWbCmqHGEff/7d17WFVV/vjx90e84B3jiI5QAzWWIl9AJdQ63rJxREm8FfLVJxUvj5q3vvYbzSi1tKc0zdLG0TRq+jLydUozekRziPGSeQEVUBwvKRXCGBqiCArY+v2xDyfQg4ByOFzW63nOw9l7r732Wu7jWWevvfZnpafTsmVLPD09y/4Ma7WWUorLly+Tnp6Ol5fXPeWh7ylU0F8T/4ogBHYIZEjHIbYTJSdDXBw0aAADBoCHR/UWUtPKcePGDVxdXXWDUEeJCK6urvd1JagbhQrIzs/ms9TPUCje/uPbZf+Hevtt42G1X36BsTYm3NG0GkA3CHXb/Z5f3ShUQOSxSIp+LaKHew/6efaznejcOeMGc8eO0LQpjBhRrWXUNE2rCrpRqIB39r8DwOqg1WUnWrHCeB4hLQ1CQozRR5qmlXL58mX8/f3x9/enffv2uLu7W5cLCgoqlMeECRM4derUXdN88MEHREVFVUWRq1xERASrVq0qte6HH36gX79+eHt706VLF9asWeOg0ukbzeX6+vuvyczNpPvvuvO4++O2E/38M3z0EfTsCXv2wNSp1VtITaslXF1dOXbsGACLFi2iRYsWvPRS6fhhSimUUjRoYPs3a2RkZLnHeeGFF+6/sNWoUaNGrFq1Cn9/f65evUrXrl0ZOHAgjxbP7V6NdKNQjlmxswDYOHRj2YmWLYObN+HyZejSBfr0qabSadq9m7NjDsf+c6xK8/Rv78+qQavKT3ibs2fPMmzYMMxmMwcPHuSrr75i8eLFHDlyhPz8fEJDQ3ntNSOWptlsZs2aNfj4+GAymZg6dSqxsbE0a9aMbdu24ebmRkREBCaTiTlz5mA2mzGbzXzzzTfk5OQQGRnJE088wfXr13n++ec5e/Ys3t7enDlzhg0bNuDv71+qbAsXLmT79u3k5+djNptZu3YtIsLp06eZOnUqly9fxsnJiS1btuDp6cmbb77Jpk2baNCgAcHBwSxdurTc+nfo0IEOHToA0KpVKzp16sSFCxcc0ijo7qO7+Ffavzh1+RSdXDvh176Mh9DS0mD1ahgyBE6cgOnTQd/I07RKS01NZeLEiRw9ehR3d3feeustEhISSEpKYteuXaSmpt6xT05ODn379iUpKYlevXrx0Ucf2cxbKcWhQ4dYvnw5r7/+OgCrV6+mffv2JCUlMX/+fI4ePWpz39mzZ3P48GFSUlLIyclhh2Xe9bCwMF588UWSkpLYv38/bm5uxMTEEBsby6FDh0hKSmLu3HInlrzDuXPnOH78OI8/XkbPhJ3pK4W7mLNjDgCL+y8uO9GrrxpDUBs3hpYtdVgLrda4l1/09vTII4+U+iLctGkTGzdupKioiIyMDFJTU/H29i61T9OmTQkKCgKge/fu7N2712beIywDP7p3705aWhoA+/btY968eQD4+fnRpUsXm/vGxcWxfPlybty4waVLl+jevTs9e/bk0qVLPPPMM4DxwBjAP//5T8LDw2natCkADzzwQKX+Da5evcrIkSNZvXo1LVq0qNS+VUU3CmU4n32epItJNG/UnOGdhttOdPQoREXBzJnw17/C5MlGw6BpWqU1b97c+v7MmTO89957HDp0CBcXF8aOHWtz7H3jxo2t752cnCgqKrKZd5MmTe5Io5SymbakvLw8ZsyYwZEjR3B3dyciIsJaDltDP5VS9zwktKCggBEjRjB+/HiGDnXcPGS6+6gMr8a/CsDsHrNp5FTG4+Lz5hnhsZs1M+ZinjatGkuoaXXX1atXadmyJa1atSIzM5OdO3dW+THMZjObN28GICUlxWb3VH5+Pg0aNMBkMnHt2jU+t8yT0qZNG0wmEzExMYDxUGBeXh4DBw5k48aN5OfnA/DLL79UqCxKKcaPH4+/vz+zZ5cRV62a2LVREJFBInJKRM6KyHwb26eKSIqIHBORfSLibSuf6lZ4q5DNJzbj3NCZV/u+ajtRYiLs2mVcJaxZA8OHGzeZNU27b926dcPb2xsfHx8mT57Mk08+WeXHmDlzJhcuXMDX15cVK1bg4+ND69tmSHR1dWXcuHH4+PgwfPhwevToYd0WFRXFihUr8PX1xWw2k5WVRXBwMIMGDSIgIAB/f3/effddm8detGgRHh4eeHh44Onpye7du9m0aRO7du2yDtG1R0NYIcXDv6r6BTgB3wMPA42BJMD7tjStSrwfCuwoL9/u3bsre5v39TzFItTM7TPLTjRunFLNmys1bZpSDRoolZpq93Jp2v1K1Z9Tq8LCQpWfn6+UUur06dPK09NTFRYWOrhUVcPWeQYSVAW+u+15TyEQOKuUOgcgItFACGC9RlNKXS2RvjlQfiefnd0susnqw6txEieW/XGZ7UQ//wybNsHo0bBxI0yYAJ07V29BNU27L7m5uQwYMICioiKUUqxbt46GekIsuzYK7sBPJZbTgR63JxKRF4D/wbiaeMpWRiIyBZgC8NBDD1V5QUta+d1K8grzGNl5JM4NnW0n+vBD4x5CTo4x/HTRIruWSdO0qufi4kJiYqKji1Hj2POegq1b8HdcCSilPlBKPQLMAyJsZaSUWq+UClBKBbRt27aKi/mbvMI8lu41HjRZ0n+J7USFhbB2LQQGwpdfGvcUdDRUTdPqCHs2CunAgyWWPYCMu6SPBobZsTzlWrF/BdcLr9PPsx+d2naynWjLFrhwwXh6uX174zkFTdO0OsKejcJhoKOIeIlIY2A08GXJBCLSscTiEOCMHctzV1dvXuWtb98CYOXAlbYTZWfD3LlGY/D99/DuuzrwnaZpdYrd7ikopYpEZAawE2Mk0kdKqRMi8jrGXfAvgRki8jRQCGQD4+xVnvIs/3Y5eYV5POHxBF1/1/XOBEoZzyFcvAhNmsDTT8Nzz1V/QTVN0+zIrs8pKKW2K6UeVUo9opRaaln3mqVBQCk1WynVRSnlr5Tqr5Q6Yc/ylOVX9SvvH3ofgLf/+LbtRH//O/zf/4G/v3GTec0aHeNI0yqpX79+d4y/X7VqFdOnT7/rfsUhHzIyMhg1alSZeSckJNw1n1WrVpGXl2ddHjx4MFeuXKlI0avVv/71L4KDg+9YP2bMGB577DF8fHwIDw+nsLCwyo+tn2gGvvvpO67evMqjro9ifsh8Z4IffjAC3XXvDkeOGFcMjz1W/QXVtFouLCyM6OjoUuuio6MJq+B85h06dOCzzz675+Pf3ihs374dFxeXe86vuo0ZM4Z///vfpKSkkJ+fz4YNG6r8GHpQLrD6kDF5ztyeNiIa3roF48YZ3UceHpCaCi+/XM0l1LSq54jQ2aNGjSIiIoKbN2/SpEkT0tLSyMjIwGw2k5ubS0hICNnZ2RQWFrJkyRJCQkJK7Z+WlkZwcDDHjx8nPz+fCRMmkJqaSufOna2hJQCmTZvG4cOHyc/PZ9SoUSxevJj333+fjIwM+vfvj8lkIj4+Hk9PTxISEjCZTKxcudIaZXXSpEnMmTOHtLQ0goKCMJvN7N+/H3d3d7Zt22YNeFcsJiaGJUuWUFBQgKurK1FRUbRr147c3FxmzpxJQkICIsLChQsZOXIkO3bsYMGCBdy6dQuTyURcXFyF/n0HDx5sfR8YGEh6enqF9quMen+loJRi+5ntOIkTY/1szKu8YgXs3g3z5/82BLV9++ovqKbVAa6urgQGBlrDT0dHRxMaGoqI4OzszNatWzly5Ajx8fHMnTv3rkHr1q5dS7NmzUhOTuaVV14p9czB0qVLSUhIIDk5md27d5OcnMysWbPo0KED8fHxxMfHl8orMTGRyMhIDh48yIEDB/jwww+tobTPnDnDCy+8wIkTJ3BxcbHGPyrJbDZz4MABjh49yujRo1m2zHjw9Y033qB169akpKSQnJzMU089RVZWFpMnT+bzzz8nKSmJf/zjH5X+dywsLOTTTz9l0KBBld63PPX+SuHghYNcK7hG74d606xRs9Ibjx6FiAgYNcroNmrRAv78Z8cUVNOqmKNCZxd3IYWEhBAdHW39da6UYsGCBezZs4cGDRpw4cIFLl68SPsyfoTt2bOHWbOMSbB8fX3x9fW1btu8eTPr16+nqKiIzMxMUlNTS22/3b59+xg+fLg1UuuIESPYu3cvQ4cOxcvLyzrxTsnQ2yWlp6cTGhpKZmYmBQUFeHl5AUYo7ZLdZW3atCEmJoY+ffpY01Q2vDbA9OnT6dOnD7179670vuWp91cKb+0zhqHOe3Je6Q23bsHEiWAywdix8Pnn8OKL4OrqgFJqWt0xbNgw4uLirLOqdevWDTACzGVlZZGYmMixY8do166dzXDZJdkKU33+/Hneeecd4uLiSE5OZsiQIeXmc7crkuKw21B2eO6ZM2cyY8YMUlJSWLdunfV4ykYobVvrKmPx4sVkZWWxcmUZQ+fvU71vFP557p80bdiUwR0Hl94QGWlcKSxdCnPmwB/+oK8SNK0KtGjRgn79+hEeHl7qBnNOTg5ubm40atSI+Ph4fvjhh7vm06dPH6KiogA4fvw4ycnJgBF2u3nz5rRu3ZqLFy8SGxtr3adly5Zcu3bNZl5ffPEFeXl5XL9+na1bt1bqV3hOTg7u7u4AfPLJJ9b1AwcOZM2aNdbl7OxsevXqxe7duzl//jxQ8fDaABs2bGDnzp3W6T7toV43CnHn4rheeJ2nvJ4q3XLn5MCCBfDkk3DggDH66OOPocQkIJqm3buwsDCSkpIYPXq0dd2YMWNISEggICCAqKgoOnUqI6qAxbRp08jNzcXX15dly5YRGBgIGLOode3alS5duhAeHl4q7PaUKVMICgqif//+pfLq1q0b48ePJzAwkB49ejBp0iS6drXxvFIZFi1axLPPPkvv3r0xmUzW9REREWRnZ+Pj44Ofnx/x8fG0bduW9evXM2LECPz8/AgNDbWZZ1xcnDW8toeHB9999x1Tp07l4sWL9OrVC39/f+vUolVJ7nbZVBMFBASo8sYiV9Sg/x3Ezu93sj98P70e7PXbhpdegpUrjbmXZ8wwnmJ+550qOaamOdLJkyfprCP61nm2zrOIJCqlAsrbt17faN77416aN2r+W4Pw66/wySfw3nswfrzx97HH4I03HFpOTdO06lJvu4++/fFb8grz6PP7PsaKkyehb18ID4cePaBjRzhzxhiSetuYZE3TtLqq3l4pLP92OQBze801rhCeeQauXDEmzRk+HB59FPr3h8GDy8lJ0zSt7qi3jcI3ad/g3NCZAQ8PgB07jKinxbOpLVgAly7B8uU6vpGmafVKvew+SsxI5FrBNXp5WO4lrF0Lbm4wYgT89JMREnvMGCPWkaZpWj1SLxuFZd8aj6DPDJwJP/4IX31lPKjWuDHMszzEtnSpA0uoaZrmGPWuUVBKsfP7nTRq0Iihjw015ltWCqZMgW+/NbqQXnoJfv97RxdV0+qcy5cv4+/vj7+/P+3bt8fd3d26XFBQUKE8JkyYwKlTp+6a5oMPPrA+2KZVTr27p3Ak8wg5N3Po6dETp1u/woYNEBQEDz1kxDhydzeC32maVuVcXV05dsyIzLpo0SJatGjBSy+9VCqNUgqlVJlP7EZGRpZ7nBdeeOH+C1tP1btGoXgynee8nzOinv7nP8b8CH/7GyQmwqef6ieXtfphzhw4VrWhs/H3h1WVD7R39uxZhg0bhtls5uDBg3z11VcsXrzYGh8pNDSU1157DTAikq5ZswYfHx9MJhNTp04lNjaWZs2asW3bNtzc3IiIiMBkMjFnzhzMZjNms5lvvvmGnJwcIiMjeeKJJ7h+/TrPP/88Z8+exdvbmzNnzrBhwwZr8LtiCxcuZPv27eTn52M2m1m7di0iwunTp5k6dSqXL1/GycmJLVu24OnpyZtvvmkNQxEcHMzSWtYVbdfuIxEZJCKnROSsiNzx81tE/kdEUkUkWUTiRMSufTY3i26y5eQWAOP5hPXrjTkS+vWDV14xnk/47/+2ZxE0TStDamoqEydO5OjRo7i7u/PWW2+RkJBAUlISu3btIjU19Y59cnJy6Nu3L0lJSfTq1csacfV2SikOHTrE8uXLraEhVq9eTfv27UlKSmL+/PnWUNm3mz17NocPHyYlJYWcnBxr2O+wsDBefPFFkpKS2L9/P25ubsTExBAbG8uhQ4dISkpi7lwbc7TUcHa7UhARJ+AD4I9AOnBYRL5USpU8s0eBAKVUnohMA5YBtgOBVIGY0zHkFuTiJE745LWAXbvg1VeNxiEjA6KiwE5BpjStxrmHX/T29Mgjj/D4449blzdt2sTGjRspKioiIyOD1NRUvL29S+3TtGlTgoKCACOs9d69e23mPWLECGua4tDX+/btY55lYImfnx9dunSxuW9cXBzLly/nxo0bXLp0ie7du9OzZ08uXbrEM888A4CzszNghMoODw+3TsJzL2GxHc2e3UeBwFml1DkAEYkGQgBro6CUKjnTxQHAxiw3VefjYx/TxKkJ3m29afI3y02o554zrhSeftr4q2maQzQv0W175swZ3nvvPQ4dOoSLiwtjx461Gf66cePG1vdlhbWG38Jfl0xTkbhveXl5zJgxgyNHjuDu7k5ERIS1HLbCX99vWOyawJ4/i92Bn0osp1vWlWUiEGtrg4hMEZEEEUnIysq6p8JkXssk9mwsgvB4u27w0UcwcCBs3Wo8qFbL+v00rS67evUqLVu2pFWrVmRmZrJz584qP4bZbGbz5s0ApKSk2Oyeys/Pp0GDBphMJq5du2adda1NmzaYTCZiYmIAuHHjBnl5eQwcOJCNGzdapwatTFjsmsKejYKt5tJm0ywiY4EAYLmt7Uqp9UqpAKVUQNu2be+pMJ8mf8qv6ldu3LrBsDRnuHABwsKM6KdDh4Il7K6maY7XrVs3vL298fHxYfLkyaXCX1eVmTNncuHCBXx9fVmxYgU+Pj60bt26VBpXV1fGjRuHj48Pw4cPp0ePHtZtUVFRrFixAl9fX8xmM1lZWQQHBzNo0CACAgLw9/fn3XffrfJy213x8K+qfgG9gJ0lll8GXraR7mngJOBWkXy7d++u7sX3v3yvJn85WbEI9cuf+irl5qZURIRSoNSxY/eUp6bVNqmpqY4uQo1RWFio8vPzlVJKnT59Wnl6eqrCwkIHl6pq2DrPQIKqwHesPe8pHAY6iogXcAEYDZQa2iMiXYF1wCCl1M92LAsPt3mYB5o+QNubDXGJ+9YYhvqXvxiB8Pz87HloTdNqoNzcXAYMGEBRURFKKdatW0fDhvVulP4d7PYvoJQqEpEZwE7ACfhIKXVCRF7HaLG+xOguagH8w3Jz5kel1FB7lSkxM5GpGR2Qoh+hYUP45Rd4+WV7HU7TtBrMxcWFxMRERxejxrFrs6iU2g5sv23dayXeP23P4992XBIzElme2hIefBA2b4Y+faBXr/J31jRNqyfqzaD8tCtpFF3J5r+OZUDnzsaNZh3OQtM0rZR60ygkZiYSfBqcCouMGdX8/GDQIEcXS9M0rUapN43CuexzPHdSUG3bwvnzMGGCnkBH0zTtNvWmUfiz33RCzjdBih+Ttzwar2la9enXr98dD6KtWrWK6dOn33W/Fi1aAJCRkcGoUaPKzDshIeGu+axatYq8vDzr8uDBg7ly5UpFil5v1JtGgdhY5MYNKCwELy/o2NHRJdK0eicsLIzo6OhS66KjowkLC6vQ/h06dOCzzz675+Pf3ihs374dFxeXe86vLqo/g3KLioyRRklJ8PzzuutI0xwQOnvUqFFERERw8+ZNmjRpQlpaGhkZGZjNZnJzcwkJCSE7O5vCwkKWLFlCSEhIqf3T0tIIDg7m+PHj5OfnM2HCBFJTU+ncubM1tATAtGnTOHz4MPn5+YwaNYrFixfz/vvvk5GRQf/+/TGZTMTHx+Pp6UlCQgImk4mVK1dao6xOmjSJOXPmkJaWRlBQEGazmf379+Pu7s62bdusAe+KxcTEsGTJEgoKCnB1dSUqKop27dqRm5vLzJkzSUhIQERYuHAhI0eOZMeOHSxYsIBbt25hMpmIi4urwpNwf+pPoxAWBr/7HfTvD3/6k6NLo2n1kqurK4GBgezYsYOQkBCio6MJDQ1FRHB2dmbr1q20atWKS5cu0bNnT4YOHVpmgLm1a9fSrFkzkpOTSU5Oplu3btZtS5cu5YEHHuDWrVsMGDCA5ORkZs2axcqVK4mPj8dkMpXKKzExkcjISA4ePIhSih49etC3b1/atGnDmTNn2LRpEx9++CHPPfccn3/+OWPHlo7daTabOXDgACLChg0bWLZsGStWrOCNN96gdevWpKSkAJCdnU1WVhaTJ09mz549eHl51bj4SPWnUQDYscN4aO2ppxxdEk1zPAeFzi7uQipuFIp/nSulWLBgAXv27KFBgwZcuHCBixcv0r59e5v57Nmzh1mzZgHg6+uLr6+vddvmzZtZv349RUVFZGZmkpqaWmr77fbt28fw4cOtkVpHjBjB3r17GTp0KF5eXtaJd0qG3i4pPT2d0NBQMjMzKSgowMvLCzBCaZfsLmvTpg0xMTH06dPHmqamhdeuP/cUwGgUzGZo2dLRJdG0emvYsGHExcVZZ1Ur/oUfFRVFVlYWiYmJHDt2jHbt2tkMl12SrauI8+fP88477xAXF0dycjJDhgwpNx91lzDaxWG3oezw3DNnzmTGjBmkpKSwbt066/GUjVDattbVJPWnUcjMNO4n6GcTNM2hWrRoQb9+/QgPDy91gzknJwc3NzcaNWpEfHw8P/zww13z6dOnD1FRxrwox48fJzk5GTDCbjdv3pzWrVtz8eJFYmN/i8jfsmVLrl27ZjOvL774gry8PK5fv87WrVvp3bt3heuUk5ODu7sxM8Ann3xiXT9w4EDWrFljXc7OzqZXr17s3r2b8+fPAzUvvHb9aRS+/tr4q+8naJrDhYWFkZSUxOjRo63rxowZQ0JCAgEBAURFRdGpU6e75jFt2jRyc3Px9fVl2bJlBFrC3/v5+dG1a1e6dOlCeHh4qbDbU6ZMISgoiP79+5fKq1u3bowfP57AwEB69OjBpEmT6Nq1a4Xrs2jRIp599ll69+5d6n5FREQE2dnZ+Pj44OfnR3x8PG3btmX9+vWMGDECPz8/QkPtNtnkPZG7XTbVRAEBAaq8scg2bdsGkZGwZYueclOrt06ePEnnzp0dXQzNzmydZxFJVEoFlLdv/bnRHBJivDRN07Qy6Z/MmqZpmpVuFDStnqltXcZa5dzv+dWNgqbVI87Ozly+fFk3DHWUUorLly/j7Ox8z3nUn3sKmqbh4eFBeno6WVlZji6KZifOzs54eHjc8/52bRREZBDwHsZ0nBuUUm/dtr0PsArwBUYrpe490pWmaeVq1KiR9UlaTbPFbt1HIuIEfAAEAd5AmIh435bsR2A88Hd7lUPTNE2rOHteKQQCZ5VS5wBEJBoIAVKLEyil0izbfrVjOTRN07QKsueNZnfgpxLL6ZZ1lSYiU0QkQUQSdF+opmma/djzSsFWxKd7GvKglFoPrAcQkSwRuXtQlDuZgEv3cuwaSNelZtJ1qbnqUn3upy6/r0giezYK6cCDJZY9gIz7zVQp1bay+4hIQkUe764NdF1qJl2Xmqsu1ac66mLP7qPDQEcR8RKRxsBo4Es7Hk/TNE27T3ZrFJRSRcAMYCdwEtislDohIq+LyFAAEXlcRNKBZ4F1InLCXuXRNE3TymfX5xSUUtuB7bete63E+8MY3Ur2tr4ajlFddF1qJl2Xmqsu1cfudal1obM1TdM0+9GxjzRN0zQr3ShomqZpVnW6URCRQSJySkTOish8R5enMkTkQRGJF5GTInJCRGZb1j8gIrtE5IzlbxtHl7WiRMRJRI6KyFeWZS8ROWipy/9ZRqnVCiLiIiKfici/LeeoV209NyLyouUzdlxENomIc205NyLykYj8LCLHS6yzeR7E8L7l+yBZRLo5ruR3KqMuyy2fsWQR2SoiLiW2vWypyykRqbJ5hutso1DB2Es1WREwVynVGegJvGAp/3wgTinVEYizLNcWszFGohV7G3jXUpdsYKJDSnVv3gN2KKU6AX4Y9ap150ZE3IFZQIBSygcjeOVoas+5+RgYdNu6ss5DENDR8poCrK2mMlbUx9xZl12Aj1LKFzgNvAxg+S4YDXSx7PMXy3fefauzjQIlYi8ppQqA4thLtYJSKlMpdcTy/hrGl447Rh0+sST7BBjmmBJWjoh4AEOADZZlAZ4CiiPj1qa6tAL6ABsBlFIFSqkr1NJzgzEKsamINASaAZnUknOjlNoD/HLb6rLOQwjwN2U4ALiIyO+qp6Tls1UXpdTXluH9AAf4bbRmCBCtlLqplDoPnMX4zrtvdblRqLLYS44mIp5AV+Ag0E4plQlGwwG4Oa5klbIK+DNQHPzQFbhS4gNfm87Pw0AWOKrUMAAABA5JREFUEGnpDtsgIs2phedGKXUBeAcjYnEmkAMkUnvPDZR9Hmr7d0I4EGt5b7e61OVGocpiLzmSiLQAPgfmKKWuOro890JEgoGflVKJJVfbSFpbzk9DoBuwVinVFbhOLegqssXS3x4CeAEdgOYY3Sy3qy3n5m5q7WdORF7B6FKOKl5lI1mV1KUuNwp2ib1UnUSkEUaDEKWU2mJZfbH4ktfy92dHla8SngSGikgaRjfeUxhXDi6WLguoXecnHUhXSh20LH+G0UjUxnPzNHBeKZWllCoEtgBPUHvPDZR9Hmrld4KIjAOCgTHqtwfL7FaXutwo1OrYS5Y+943ASaXUyhKbvgTGWd6PA7ZVd9kqSyn1slLKQynliXEevlFKjQHigVGWZLWiLgBKqf8AP4nIY5ZVAzDmCal15waj26iniDSzfOaK61Irz41FWefhS+B5yyiknkBOcTdTTSXG7JXzgKFKqbwSm74ERotIExHxwrh5fqhKDqqUqrMvYDDGHfvvgVccXZ5Klt2McTmYDByzvAZj9MXHAWcsfx9wdFkrWa9+wFeW9w9bPshngX8ATRxdvkrUwx9IsJyfL4A2tfXcAIuBfwPHgU+BJrXl3ACbMO6FFGL8ep5Y1nnA6HL5wPJ9kIIx4srhdSinLmcx7h0Ufwf8tUT6Vyx1OQUEVVU5dJgLTdM0zaoudx9pmqZplaQbBU3TNM1KNwqapmmalW4UNE3TNCvdKGiapmlWulHQNAsRuSUix0q8quwpZRHxLBn9UtNqKrtOx6lptUy+Usrf0YXQNEfSVwqaVg4RSRORt0XkkOX1B8v634tInCXWfZyIPGRZ384S+z7J8nrCkpWTiHxombvgaxFpakk/S0RSLflEO6iamgboRkHTSmp6W/dRaIltV5VSgcAajLhNWN7/TRmx7qOA9y3r3wd2K6X8MGIinbCs7wh8oJTqAlwBRlrWzwe6WvKZaq/KaVpF6CeaNc1CRHKVUi1srE8DnlJKnbMEKfyPUspVRC4Bv1NKFVrWZyqlTCKSBXgopW6WyMMT2KWMiV8QkXlAI6XUEhHZAeRihMv4QimVa+eqalqZ9JWCplWMKuN9WWlsuVni/S1+u6c3BCMmT3cgsUR0Uk2rdrpR0LSKCS3x9zvL+/0YUV8BxgD7LO/jgGlgnZe6VVmZikgD4EGlVDzGJEQuwB1XK5pWXfQvEk37TVMROVZieYdSqnhYahMROYjxQyrMsm4W8JGI/D+MmdgmWNbPBtaLyESMK4JpGNEvbXEC/ldEWmNE8XxXGVN7appD6HsKmlYOyz2FAKXUJUeXRdPsTXcfaZqmaVb6SkHTNE2z0lcKmqZpmpVuFDRN0zQr3ShomqZpVrpR0DRN06x0o6BpmqZZ/X9NqDbC03ixzgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a12ffb630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = L2_model_dict['acc'] \n",
    "val_acc_values = L2_model_dict['val_acc']\n",
    "model_acc = model_val_dict['acc']\n",
    "model_val_acc = model_val_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L2')\n",
    "plt.plot(epochs, val_acc_values, 'g', label='Validation acc L2')\n",
    "plt.plot(epochs, model_acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, model_val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of L2 regularization are quite disappointing here. We notice the discrepancy between validation and training accuracy seems to have decreased slightly, but the end result is definitely not getting better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.  L1 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at L1 regularization. Will this work better?\n",
    "\n",
    "In the cell below: \n",
    "\n",
    "* Recreate the same model we did above, but this time, set the `kernel_regularizer` to `regularizers.l1(0.005)` inside both hidden layers. \n",
    "* Compile and fit the model exactly as we did for our L2 Regularization experiment (`120` epochs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 15.9955 - acc: 0.1745 - val_loss: 15.5842 - val_acc: 0.1910\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 15.2297 - acc: 0.1989 - val_loss: 14.8376 - val_acc: 0.2210\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 14.4914 - acc: 0.2165 - val_loss: 14.1122 - val_acc: 0.2340\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 13.7732 - acc: 0.2319 - val_loss: 13.4047 - val_acc: 0.2540\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 13.0731 - acc: 0.2571 - val_loss: 12.7143 - val_acc: 0.2690\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 12.3902 - acc: 0.2823 - val_loss: 12.0410 - val_acc: 0.3080\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 11.7261 - acc: 0.3251 - val_loss: 11.3869 - val_acc: 0.3520\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 11.0818 - acc: 0.3640 - val_loss: 10.7545 - val_acc: 0.4020\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 10.4589 - acc: 0.3991 - val_loss: 10.1433 - val_acc: 0.4260\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 9.8582 - acc: 0.4337 - val_loss: 9.5551 - val_acc: 0.4550\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 9.2793 - acc: 0.4649 - val_loss: 8.9886 - val_acc: 0.4750\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 8.7229 - acc: 0.4945 - val_loss: 8.4447 - val_acc: 0.4820\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 8.1892 - acc: 0.5145 - val_loss: 7.9243 - val_acc: 0.5010\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 7.6783 - acc: 0.5379 - val_loss: 7.4267 - val_acc: 0.5140\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 7.1894 - acc: 0.5565 - val_loss: 6.9503 - val_acc: 0.5430\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 6.7234 - acc: 0.5841 - val_loss: 6.4972 - val_acc: 0.5770\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 6.2805 - acc: 0.6043 - val_loss: 6.0676 - val_acc: 0.5880\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 5.8602 - acc: 0.6161 - val_loss: 5.6601 - val_acc: 0.6040\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 5.4629 - acc: 0.6340 - val_loss: 5.2770 - val_acc: 0.6020\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 5.0888 - acc: 0.6407 - val_loss: 4.9150 - val_acc: 0.6250\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 4.7375 - acc: 0.6521 - val_loss: 4.5781 - val_acc: 0.6330\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 4.4088 - acc: 0.6627 - val_loss: 4.2627 - val_acc: 0.6410\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 4.1028 - acc: 0.6691 - val_loss: 3.9683 - val_acc: 0.6500\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 3.8191 - acc: 0.6748 - val_loss: 3.6978 - val_acc: 0.6580\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 3.5581 - acc: 0.6787 - val_loss: 3.4499 - val_acc: 0.6590\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 3.3194 - acc: 0.6823 - val_loss: 3.2252 - val_acc: 0.6470\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 3.1030 - acc: 0.6821 - val_loss: 3.0192 - val_acc: 0.6560\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 2.9079 - acc: 0.6844 - val_loss: 2.8352 - val_acc: 0.6650\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 2.7336 - acc: 0.6860 - val_loss: 2.6729 - val_acc: 0.6620\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 2.5804 - acc: 0.6841 - val_loss: 2.5305 - val_acc: 0.6620\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 2.4483 - acc: 0.6841 - val_loss: 2.4094 - val_acc: 0.6610\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 2.3358 - acc: 0.6869 - val_loss: 2.3077 - val_acc: 0.6630\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 2.2419 - acc: 0.6849 - val_loss: 2.2235 - val_acc: 0.6650\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 2.1662 - acc: 0.6845 - val_loss: 2.1565 - val_acc: 0.6600\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 2.1075 - acc: 0.6820 - val_loss: 2.1056 - val_acc: 0.6630\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 2.0633 - acc: 0.6843 - val_loss: 2.0706 - val_acc: 0.6610\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 2.0316 - acc: 0.6840 - val_loss: 2.0404 - val_acc: 0.6670\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 2.0056 - acc: 0.6852 - val_loss: 2.0186 - val_acc: 0.6650\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.9837 - acc: 0.6829 - val_loss: 1.9985 - val_acc: 0.6630\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.9636 - acc: 0.6852 - val_loss: 1.9775 - val_acc: 0.6700\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.9454 - acc: 0.6833 - val_loss: 1.9620 - val_acc: 0.6660\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.9277 - acc: 0.6867 - val_loss: 1.9432 - val_acc: 0.6740\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.9119 - acc: 0.6861 - val_loss: 1.9295 - val_acc: 0.6730\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.8966 - acc: 0.6863 - val_loss: 1.9162 - val_acc: 0.6690\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.8816 - acc: 0.6875 - val_loss: 1.8999 - val_acc: 0.6850\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.8677 - acc: 0.6896 - val_loss: 1.8876 - val_acc: 0.6770\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.8544 - acc: 0.6895 - val_loss: 1.8768 - val_acc: 0.6770\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.8420 - acc: 0.6876 - val_loss: 1.8616 - val_acc: 0.6830\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.8286 - acc: 0.6895 - val_loss: 1.8514 - val_acc: 0.6770\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.8168 - acc: 0.6896 - val_loss: 1.8395 - val_acc: 0.6780\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.8048 - acc: 0.6904 - val_loss: 1.8283 - val_acc: 0.6820\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.7935 - acc: 0.6928 - val_loss: 1.8163 - val_acc: 0.6810\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.7822 - acc: 0.6919 - val_loss: 1.8059 - val_acc: 0.6820\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.7711 - acc: 0.6939 - val_loss: 1.7930 - val_acc: 0.6820\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.7604 - acc: 0.6951 - val_loss: 1.7843 - val_acc: 0.6830\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.7499 - acc: 0.6928 - val_loss: 1.7715 - val_acc: 0.6850\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.7395 - acc: 0.6939 - val_loss: 1.7614 - val_acc: 0.6850\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.7292 - acc: 0.6956 - val_loss: 1.7512 - val_acc: 0.6860\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.7195 - acc: 0.6965 - val_loss: 1.7442 - val_acc: 0.6850\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.7095 - acc: 0.6972 - val_loss: 1.7320 - val_acc: 0.6840\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.7004 - acc: 0.6952 - val_loss: 1.7232 - val_acc: 0.6910\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.6908 - acc: 0.6983 - val_loss: 1.7193 - val_acc: 0.6830\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.6820 - acc: 0.6981 - val_loss: 1.7077 - val_acc: 0.6890\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.6726 - acc: 0.6981 - val_loss: 1.6954 - val_acc: 0.6890\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.6640 - acc: 0.6988 - val_loss: 1.6871 - val_acc: 0.6890\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.6558 - acc: 0.6991 - val_loss: 1.6789 - val_acc: 0.6740\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.6469 - acc: 0.6989 - val_loss: 1.6725 - val_acc: 0.6790\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.6387 - acc: 0.6997 - val_loss: 1.6624 - val_acc: 0.6910\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.6301 - acc: 0.7004 - val_loss: 1.6585 - val_acc: 0.7000\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.6221 - acc: 0.7012 - val_loss: 1.6486 - val_acc: 0.6940\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.6142 - acc: 0.7025 - val_loss: 1.6378 - val_acc: 0.6900\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.6062 - acc: 0.7027 - val_loss: 1.6358 - val_acc: 0.6830\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.5984 - acc: 0.7027 - val_loss: 1.6258 - val_acc: 0.6980\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.5906 - acc: 0.7024 - val_loss: 1.6176 - val_acc: 0.6980\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.5833 - acc: 0.7040 - val_loss: 1.6095 - val_acc: 0.6870\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.5758 - acc: 0.7027 - val_loss: 1.6029 - val_acc: 0.6950\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.5683 - acc: 0.7025 - val_loss: 1.5945 - val_acc: 0.6970\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.5611 - acc: 0.7032 - val_loss: 1.5895 - val_acc: 0.6900\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.5540 - acc: 0.7039 - val_loss: 1.5862 - val_acc: 0.6940\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.5472 - acc: 0.7061 - val_loss: 1.5738 - val_acc: 0.7000\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.5396 - acc: 0.7072 - val_loss: 1.5665 - val_acc: 0.6990\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.5328 - acc: 0.7061 - val_loss: 1.5610 - val_acc: 0.7020\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.5258 - acc: 0.7087 - val_loss: 1.5562 - val_acc: 0.6930\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.5190 - acc: 0.7068 - val_loss: 1.5488 - val_acc: 0.6960\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.5122 - acc: 0.7077 - val_loss: 1.5392 - val_acc: 0.6980\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.5058 - acc: 0.7092 - val_loss: 1.5363 - val_acc: 0.6910\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.4992 - acc: 0.7079 - val_loss: 1.5262 - val_acc: 0.7010\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.4926 - acc: 0.7097 - val_loss: 1.5193 - val_acc: 0.7040\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.4861 - acc: 0.7097 - val_loss: 1.5147 - val_acc: 0.6990\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.4797 - acc: 0.7120 - val_loss: 1.5080 - val_acc: 0.6890\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.4734 - acc: 0.7125 - val_loss: 1.5051 - val_acc: 0.7050\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.4675 - acc: 0.7107 - val_loss: 1.4971 - val_acc: 0.7070\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.4613 - acc: 0.7112 - val_loss: 1.4889 - val_acc: 0.7070\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.4552 - acc: 0.7136 - val_loss: 1.4855 - val_acc: 0.7070\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.4493 - acc: 0.7129 - val_loss: 1.4785 - val_acc: 0.7060\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.4435 - acc: 0.7136 - val_loss: 1.4736 - val_acc: 0.7070\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.4377 - acc: 0.7148 - val_loss: 1.4676 - val_acc: 0.7030\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.4319 - acc: 0.7145 - val_loss: 1.4600 - val_acc: 0.7090\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.4262 - acc: 0.7155 - val_loss: 1.4558 - val_acc: 0.6980\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.4207 - acc: 0.7169 - val_loss: 1.4490 - val_acc: 0.7070\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.4148 - acc: 0.7172 - val_loss: 1.4425 - val_acc: 0.7060\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.4095 - acc: 0.7169 - val_loss: 1.4390 - val_acc: 0.7040\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.4043 - acc: 0.7157 - val_loss: 1.4328 - val_acc: 0.7090\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.3986 - acc: 0.7181 - val_loss: 1.4272 - val_acc: 0.7010\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.3933 - acc: 0.7176 - val_loss: 1.4223 - val_acc: 0.7080\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.3879 - acc: 0.7183 - val_loss: 1.4182 - val_acc: 0.7110\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.3830 - acc: 0.7189 - val_loss: 1.4146 - val_acc: 0.7030\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.3779 - acc: 0.7196 - val_loss: 1.4085 - val_acc: 0.6990\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.3723 - acc: 0.7203 - val_loss: 1.4022 - val_acc: 0.7020\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.3677 - acc: 0.7196 - val_loss: 1.3973 - val_acc: 0.7100\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.3627 - acc: 0.7213 - val_loss: 1.3910 - val_acc: 0.7110\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.3577 - acc: 0.7217 - val_loss: 1.3913 - val_acc: 0.7040\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.3520 - acc: 0.7219 - val_loss: 1.3813 - val_acc: 0.7130\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.3479 - acc: 0.7201 - val_loss: 1.3767 - val_acc: 0.7040\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.3428 - acc: 0.7233 - val_loss: 1.3723 - val_acc: 0.7050\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.3377 - acc: 0.7220 - val_loss: 1.3679 - val_acc: 0.7000\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.3332 - acc: 0.7225 - val_loss: 1.3670 - val_acc: 0.7110\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.3286 - acc: 0.7240 - val_loss: 1.3608 - val_acc: 0.7140\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.3237 - acc: 0.7225 - val_loss: 1.3553 - val_acc: 0.7120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.3192 - acc: 0.7245 - val_loss: 1.3495 - val_acc: 0.7050\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the cell below to get and visualize the model's `.history`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a19df9f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g.', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy with L1 regularization')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how The training and validation accuracy don't diverge as much as before! Unfortunately, the validation accuracy doesn't reach rates much higher than 70%. It does seem like we can still improve the model by training much longer.\n",
    "\n",
    "To complete our comparison, let's use `model.evaluate()` again on the appropriate variables to compare results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 41us/step\n",
      "1500/1500 [==============================] - 0s 43us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "\n",
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.3186310468037923, 0.72266666663487755]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train # Expected Output: [1.3186310468037923, 0.72266666663487755]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.3541648308436076, 0.70800000031789145]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test # Expected Output: [1.3541648308436076, 0.70800000031789145]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is about the best we've seen so far, but we were training for quite a while! Let's see if dropout regularization can do even better and/or be more efficient!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Dropout Regularization\n",
    "\n",
    "Dropout Regularization is accomplished by adding in an additional `Dropout` layer wherever we want to use it, and providing a percentage value for how likely any given neuron is to get \"dropped out\" during this layer. \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Import `Dropout` from `keras.layers`\n",
    "* Recreate the same network we have above, but this time without any L1 or L2 regularization\n",
    "* Add a `Dropout` layer between hidden layer 1 and hidden layer 2.  This should have a dropout chance of `0.3`.\n",
    "* Add a `Dropout` layer between hidden layer 2 and the output layer.  This should have a dropout chance of `0.3`.\n",
    "* Compile the model with the exact same hyperparameters as all other models we've built. \n",
    "* Fit the model with the same hyperparameters we've used above.  But this time, train the model for `200` epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.9565 - acc: 0.1480 - val_loss: 1.9257 - val_acc: 0.1600\n",
      "Epoch 2/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.9375 - acc: 0.1700 - val_loss: 1.9152 - val_acc: 0.1820\n",
      "Epoch 3/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.9266 - acc: 0.1797 - val_loss: 1.9065 - val_acc: 0.2010\n",
      "Epoch 4/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.9158 - acc: 0.1959 - val_loss: 1.8977 - val_acc: 0.2260\n",
      "Epoch 5/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.9072 - acc: 0.2027 - val_loss: 1.8877 - val_acc: 0.2490\n",
      "Epoch 6/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.9005 - acc: 0.2133 - val_loss: 1.8774 - val_acc: 0.2710\n",
      "Epoch 7/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.8874 - acc: 0.2288 - val_loss: 1.8657 - val_acc: 0.2810\n",
      "Epoch 8/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.8830 - acc: 0.2351 - val_loss: 1.8537 - val_acc: 0.3010\n",
      "Epoch 9/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.8667 - acc: 0.2444 - val_loss: 1.8395 - val_acc: 0.3080\n",
      "Epoch 10/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.8563 - acc: 0.2495 - val_loss: 1.8241 - val_acc: 0.3230\n",
      "Epoch 11/200\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.8453 - acc: 0.2623 - val_loss: 1.8077 - val_acc: 0.3410\n",
      "Epoch 12/200\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.8261 - acc: 0.2741 - val_loss: 1.7875 - val_acc: 0.3570\n",
      "Epoch 13/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.8147 - acc: 0.2844 - val_loss: 1.7671 - val_acc: 0.3660\n",
      "Epoch 14/200\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.7957 - acc: 0.2927 - val_loss: 1.7453 - val_acc: 0.3810\n",
      "Epoch 15/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.7767 - acc: 0.3100 - val_loss: 1.7231 - val_acc: 0.3940\n",
      "Epoch 16/200\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.7652 - acc: 0.3229 - val_loss: 1.6996 - val_acc: 0.4170\n",
      "Epoch 17/200\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.7440 - acc: 0.3249 - val_loss: 1.6753 - val_acc: 0.4300\n",
      "Epoch 18/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.7296 - acc: 0.3308 - val_loss: 1.6513 - val_acc: 0.4420\n",
      "Epoch 19/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.7082 - acc: 0.3473 - val_loss: 1.6257 - val_acc: 0.4560\n",
      "Epoch 20/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.6868 - acc: 0.3529 - val_loss: 1.5988 - val_acc: 0.4670\n",
      "Epoch 21/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.6688 - acc: 0.3656 - val_loss: 1.5730 - val_acc: 0.4910\n",
      "Epoch 22/200\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.6469 - acc: 0.3717 - val_loss: 1.5445 - val_acc: 0.4970\n",
      "Epoch 23/200\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.6181 - acc: 0.3853 - val_loss: 1.5169 - val_acc: 0.5090\n",
      "Epoch 24/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.5946 - acc: 0.4053 - val_loss: 1.4894 - val_acc: 0.5190\n",
      "Epoch 25/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.5661 - acc: 0.4128 - val_loss: 1.4594 - val_acc: 0.5290\n",
      "Epoch 26/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.5482 - acc: 0.4276 - val_loss: 1.4301 - val_acc: 0.5450\n",
      "Epoch 27/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.5247 - acc: 0.4288 - val_loss: 1.4038 - val_acc: 0.5580\n",
      "Epoch 28/200\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.5093 - acc: 0.4368 - val_loss: 1.3766 - val_acc: 0.5670\n",
      "Epoch 29/200\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.4812 - acc: 0.4420 - val_loss: 1.3505 - val_acc: 0.5770\n",
      "Epoch 30/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.4579 - acc: 0.4599 - val_loss: 1.3246 - val_acc: 0.5900\n",
      "Epoch 31/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.4406 - acc: 0.4580 - val_loss: 1.3018 - val_acc: 0.5960\n",
      "Epoch 32/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.4202 - acc: 0.4687 - val_loss: 1.2810 - val_acc: 0.6120\n",
      "Epoch 33/200\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.4028 - acc: 0.4863 - val_loss: 1.2569 - val_acc: 0.6170\n",
      "Epoch 34/200\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.3794 - acc: 0.4952 - val_loss: 1.2353 - val_acc: 0.6280\n",
      "Epoch 35/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.3618 - acc: 0.4972 - val_loss: 1.2136 - val_acc: 0.6300\n",
      "Epoch 36/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.3577 - acc: 0.4973 - val_loss: 1.1966 - val_acc: 0.6330\n",
      "Epoch 37/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.3339 - acc: 0.5120 - val_loss: 1.1777 - val_acc: 0.6400\n",
      "Epoch 38/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.2992 - acc: 0.5237 - val_loss: 1.1557 - val_acc: 0.6410\n",
      "Epoch 39/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.3012 - acc: 0.5199 - val_loss: 1.1386 - val_acc: 0.6500\n",
      "Epoch 40/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.2801 - acc: 0.5317 - val_loss: 1.1217 - val_acc: 0.6490\n",
      "Epoch 41/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.2621 - acc: 0.5375 - val_loss: 1.1038 - val_acc: 0.6550\n",
      "Epoch 42/200\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.2423 - acc: 0.5471 - val_loss: 1.0887 - val_acc: 0.6560\n",
      "Epoch 43/200\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.2296 - acc: 0.5519 - val_loss: 1.0738 - val_acc: 0.6590\n",
      "Epoch 44/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.2236 - acc: 0.5491 - val_loss: 1.0592 - val_acc: 0.6660\n",
      "Epoch 45/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.2054 - acc: 0.5516 - val_loss: 1.0459 - val_acc: 0.6600\n",
      "Epoch 46/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1904 - acc: 0.5595 - val_loss: 1.0310 - val_acc: 0.6720\n",
      "Epoch 47/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.1758 - acc: 0.5675 - val_loss: 1.0202 - val_acc: 0.6700\n",
      "Epoch 48/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1651 - acc: 0.5761 - val_loss: 1.0047 - val_acc: 0.6730\n",
      "Epoch 49/200\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.1572 - acc: 0.5683 - val_loss: 0.9951 - val_acc: 0.6750\n",
      "Epoch 50/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.1481 - acc: 0.5841 - val_loss: 0.9842 - val_acc: 0.6850\n",
      "Epoch 51/200\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.1148 - acc: 0.5900 - val_loss: 0.9689 - val_acc: 0.6850\n",
      "Epoch 52/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.1101 - acc: 0.5976 - val_loss: 0.9571 - val_acc: 0.6890\n",
      "Epoch 53/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0976 - acc: 0.5947 - val_loss: 0.9472 - val_acc: 0.6870\n",
      "Epoch 54/200\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.1005 - acc: 0.5987 - val_loss: 0.9393 - val_acc: 0.6890\n",
      "Epoch 55/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0906 - acc: 0.6012 - val_loss: 0.9297 - val_acc: 0.6900\n",
      "Epoch 56/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0798 - acc: 0.6064 - val_loss: 0.9257 - val_acc: 0.6900\n",
      "Epoch 57/200\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.0677 - acc: 0.6143 - val_loss: 0.9150 - val_acc: 0.6960\n",
      "Epoch 58/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0570 - acc: 0.6133 - val_loss: 0.9039 - val_acc: 0.7000\n",
      "Epoch 59/200\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.0506 - acc: 0.6172 - val_loss: 0.8972 - val_acc: 0.7070\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0373 - acc: 0.6247 - val_loss: 0.8915 - val_acc: 0.7080\n",
      "Epoch 61/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0373 - acc: 0.6256 - val_loss: 0.8822 - val_acc: 0.7040\n",
      "Epoch 62/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0231 - acc: 0.6301 - val_loss: 0.8761 - val_acc: 0.7100\n",
      "Epoch 63/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0108 - acc: 0.6273 - val_loss: 0.8667 - val_acc: 0.7080\n",
      "Epoch 64/200\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.0020 - acc: 0.6340 - val_loss: 0.8596 - val_acc: 0.7120\n",
      "Epoch 65/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9989 - acc: 0.6344 - val_loss: 0.8548 - val_acc: 0.7160\n",
      "Epoch 66/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9969 - acc: 0.6359 - val_loss: 0.8478 - val_acc: 0.7130\n",
      "Epoch 67/200\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9899 - acc: 0.6403 - val_loss: 0.8414 - val_acc: 0.7140\n",
      "Epoch 68/200\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9741 - acc: 0.6421 - val_loss: 0.8373 - val_acc: 0.7130\n",
      "Epoch 69/200\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9690 - acc: 0.6392 - val_loss: 0.8289 - val_acc: 0.7210\n",
      "Epoch 70/200\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9568 - acc: 0.6505 - val_loss: 0.8282 - val_acc: 0.7110\n",
      "Epoch 71/200\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9582 - acc: 0.6601 - val_loss: 0.8176 - val_acc: 0.7190\n",
      "Epoch 72/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9517 - acc: 0.6519 - val_loss: 0.8130 - val_acc: 0.7230\n",
      "Epoch 73/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9404 - acc: 0.6591 - val_loss: 0.8110 - val_acc: 0.7170\n",
      "Epoch 74/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9331 - acc: 0.6643 - val_loss: 0.8052 - val_acc: 0.7200\n",
      "Epoch 75/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9309 - acc: 0.6608 - val_loss: 0.8026 - val_acc: 0.7220\n",
      "Epoch 76/200\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9197 - acc: 0.6624 - val_loss: 0.7939 - val_acc: 0.7260\n",
      "Epoch 77/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9209 - acc: 0.6641 - val_loss: 0.7896 - val_acc: 0.7290\n",
      "Epoch 78/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9155 - acc: 0.6596 - val_loss: 0.7876 - val_acc: 0.7260\n",
      "Epoch 79/200\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8921 - acc: 0.6808 - val_loss: 0.7804 - val_acc: 0.7250\n",
      "Epoch 80/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8929 - acc: 0.6771 - val_loss: 0.7775 - val_acc: 0.7290\n",
      "Epoch 81/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8975 - acc: 0.6697 - val_loss: 0.7734 - val_acc: 0.7290\n",
      "Epoch 82/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8922 - acc: 0.6720 - val_loss: 0.7707 - val_acc: 0.7290\n",
      "Epoch 83/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8830 - acc: 0.6799 - val_loss: 0.7665 - val_acc: 0.7290\n",
      "Epoch 84/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8745 - acc: 0.6807 - val_loss: 0.7633 - val_acc: 0.7290\n",
      "Epoch 85/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8747 - acc: 0.6791 - val_loss: 0.7593 - val_acc: 0.7320\n",
      "Epoch 86/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8726 - acc: 0.6852 - val_loss: 0.7545 - val_acc: 0.7310\n",
      "Epoch 87/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8731 - acc: 0.6787 - val_loss: 0.7525 - val_acc: 0.7320\n",
      "Epoch 88/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8628 - acc: 0.6856 - val_loss: 0.7512 - val_acc: 0.7330\n",
      "Epoch 89/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8515 - acc: 0.6869 - val_loss: 0.7447 - val_acc: 0.7330\n",
      "Epoch 90/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8459 - acc: 0.6969 - val_loss: 0.7398 - val_acc: 0.7330\n",
      "Epoch 91/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8395 - acc: 0.6916 - val_loss: 0.7413 - val_acc: 0.7330\n",
      "Epoch 92/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8503 - acc: 0.6871 - val_loss: 0.7361 - val_acc: 0.7360\n",
      "Epoch 93/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8288 - acc: 0.6968 - val_loss: 0.7326 - val_acc: 0.7350\n",
      "Epoch 94/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8308 - acc: 0.6973 - val_loss: 0.7334 - val_acc: 0.7360\n",
      "Epoch 95/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8236 - acc: 0.7009 - val_loss: 0.7267 - val_acc: 0.7370\n",
      "Epoch 96/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8245 - acc: 0.6952 - val_loss: 0.7246 - val_acc: 0.7380\n",
      "Epoch 97/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8112 - acc: 0.7037 - val_loss: 0.7221 - val_acc: 0.7360\n",
      "Epoch 98/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8073 - acc: 0.7124 - val_loss: 0.7187 - val_acc: 0.7360\n",
      "Epoch 99/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8112 - acc: 0.7016 - val_loss: 0.7182 - val_acc: 0.7400\n",
      "Epoch 100/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8042 - acc: 0.6997 - val_loss: 0.7165 - val_acc: 0.7400\n",
      "Epoch 101/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.7997 - acc: 0.7083 - val_loss: 0.7129 - val_acc: 0.7340\n",
      "Epoch 102/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7959 - acc: 0.7115 - val_loss: 0.7118 - val_acc: 0.7400\n",
      "Epoch 103/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7862 - acc: 0.7143 - val_loss: 0.7090 - val_acc: 0.7360\n",
      "Epoch 104/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.7971 - acc: 0.7091 - val_loss: 0.7061 - val_acc: 0.7400\n",
      "Epoch 105/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.7836 - acc: 0.7091 - val_loss: 0.7038 - val_acc: 0.7390\n",
      "Epoch 106/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.7754 - acc: 0.7168 - val_loss: 0.7014 - val_acc: 0.7390\n",
      "Epoch 107/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.7682 - acc: 0.7224 - val_loss: 0.7004 - val_acc: 0.7420\n",
      "Epoch 108/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.7769 - acc: 0.7204 - val_loss: 0.6986 - val_acc: 0.7390\n",
      "Epoch 109/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.7762 - acc: 0.7135 - val_loss: 0.6985 - val_acc: 0.7430\n",
      "Epoch 110/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7649 - acc: 0.7220 - val_loss: 0.6945 - val_acc: 0.7400\n",
      "Epoch 111/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.7626 - acc: 0.7260 - val_loss: 0.6928 - val_acc: 0.7450\n",
      "Epoch 112/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7551 - acc: 0.7236 - val_loss: 0.6912 - val_acc: 0.7410\n",
      "Epoch 113/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7657 - acc: 0.7207 - val_loss: 0.6912 - val_acc: 0.7450\n",
      "Epoch 114/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.7430 - acc: 0.7280 - val_loss: 0.6879 - val_acc: 0.7490\n",
      "Epoch 115/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7472 - acc: 0.7205 - val_loss: 0.6856 - val_acc: 0.7470\n",
      "Epoch 116/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7427 - acc: 0.7275 - val_loss: 0.6846 - val_acc: 0.7440\n",
      "Epoch 117/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7426 - acc: 0.7311 - val_loss: 0.6837 - val_acc: 0.7460\n",
      "Epoch 118/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7419 - acc: 0.7268 - val_loss: 0.6812 - val_acc: 0.7520\n",
      "Epoch 119/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7253 - acc: 0.7331 - val_loss: 0.6786 - val_acc: 0.7490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7218 - acc: 0.7344 - val_loss: 0.6786 - val_acc: 0.7510\n",
      "Epoch 121/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.7263 - acc: 0.7283 - val_loss: 0.6779 - val_acc: 0.7450\n",
      "Epoch 122/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7254 - acc: 0.7288 - val_loss: 0.6771 - val_acc: 0.7470\n",
      "Epoch 123/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7164 - acc: 0.7408 - val_loss: 0.6753 - val_acc: 0.7490\n",
      "Epoch 124/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7183 - acc: 0.7379 - val_loss: 0.6750 - val_acc: 0.7510\n",
      "Epoch 125/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.7114 - acc: 0.7403 - val_loss: 0.6743 - val_acc: 0.7510\n",
      "Epoch 126/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.7198 - acc: 0.7424 - val_loss: 0.6720 - val_acc: 0.7560\n",
      "Epoch 127/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.7106 - acc: 0.7393 - val_loss: 0.6695 - val_acc: 0.7550\n",
      "Epoch 128/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.7080 - acc: 0.7392 - val_loss: 0.6687 - val_acc: 0.7570\n",
      "Epoch 129/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.7053 - acc: 0.7396 - val_loss: 0.6667 - val_acc: 0.7550\n",
      "Epoch 130/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7030 - acc: 0.7417 - val_loss: 0.6662 - val_acc: 0.7530\n",
      "Epoch 131/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7003 - acc: 0.7409 - val_loss: 0.6655 - val_acc: 0.7550\n",
      "Epoch 132/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.6934 - acc: 0.7431 - val_loss: 0.6653 - val_acc: 0.7530\n",
      "Epoch 133/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.6912 - acc: 0.7521 - val_loss: 0.6645 - val_acc: 0.7550\n",
      "Epoch 134/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.6882 - acc: 0.7504 - val_loss: 0.6646 - val_acc: 0.7540\n",
      "Epoch 135/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.6850 - acc: 0.7515 - val_loss: 0.6605 - val_acc: 0.7580\n",
      "Epoch 136/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.6971 - acc: 0.7495 - val_loss: 0.6627 - val_acc: 0.7560\n",
      "Epoch 137/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.6783 - acc: 0.7524 - val_loss: 0.6608 - val_acc: 0.7540\n",
      "Epoch 138/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.6805 - acc: 0.7471 - val_loss: 0.6588 - val_acc: 0.7530\n",
      "Epoch 139/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.6815 - acc: 0.7472 - val_loss: 0.6580 - val_acc: 0.7540\n",
      "Epoch 140/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.6686 - acc: 0.7576 - val_loss: 0.6562 - val_acc: 0.7560\n",
      "Epoch 141/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.6693 - acc: 0.7592 - val_loss: 0.6554 - val_acc: 0.7570\n",
      "Epoch 142/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.6664 - acc: 0.7616 - val_loss: 0.6545 - val_acc: 0.7600\n",
      "Epoch 143/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.6622 - acc: 0.7559 - val_loss: 0.6534 - val_acc: 0.7590\n",
      "Epoch 144/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.6635 - acc: 0.7555 - val_loss: 0.6529 - val_acc: 0.7620\n",
      "Epoch 145/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.6587 - acc: 0.7564 - val_loss: 0.6532 - val_acc: 0.7570\n",
      "Epoch 146/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.6552 - acc: 0.7513 - val_loss: 0.6506 - val_acc: 0.7570\n",
      "Epoch 147/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.6426 - acc: 0.7651 - val_loss: 0.6506 - val_acc: 0.7620\n",
      "Epoch 148/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.6516 - acc: 0.7633 - val_loss: 0.6515 - val_acc: 0.7560\n",
      "Epoch 149/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.6506 - acc: 0.7627 - val_loss: 0.6505 - val_acc: 0.7590\n",
      "Epoch 150/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.6410 - acc: 0.7708 - val_loss: 0.6500 - val_acc: 0.7610\n",
      "Epoch 151/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.6523 - acc: 0.7627 - val_loss: 0.6486 - val_acc: 0.7610\n",
      "Epoch 152/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.6367 - acc: 0.7680 - val_loss: 0.6472 - val_acc: 0.7580\n",
      "Epoch 153/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.6408 - acc: 0.7671 - val_loss: 0.6461 - val_acc: 0.7620\n",
      "Epoch 154/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.6296 - acc: 0.7699 - val_loss: 0.6470 - val_acc: 0.7640\n",
      "Epoch 155/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.6410 - acc: 0.7652 - val_loss: 0.6476 - val_acc: 0.7610\n",
      "Epoch 156/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.6374 - acc: 0.7685 - val_loss: 0.6466 - val_acc: 0.7580\n",
      "Epoch 157/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.6294 - acc: 0.7679 - val_loss: 0.6455 - val_acc: 0.7590\n",
      "Epoch 158/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.6336 - acc: 0.7676 - val_loss: 0.6453 - val_acc: 0.7630\n",
      "Epoch 159/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.6230 - acc: 0.7695 - val_loss: 0.6445 - val_acc: 0.7580\n",
      "Epoch 160/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.6345 - acc: 0.7663 - val_loss: 0.6475 - val_acc: 0.7550\n",
      "Epoch 161/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.6127 - acc: 0.7735 - val_loss: 0.6460 - val_acc: 0.7570\n",
      "Epoch 162/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.6202 - acc: 0.7689 - val_loss: 0.6431 - val_acc: 0.7640\n",
      "Epoch 163/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.6172 - acc: 0.7697 - val_loss: 0.6423 - val_acc: 0.7620\n",
      "Epoch 164/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.6158 - acc: 0.7769 - val_loss: 0.6428 - val_acc: 0.7630\n",
      "Epoch 165/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.6061 - acc: 0.7779 - val_loss: 0.6421 - val_acc: 0.7570\n",
      "Epoch 166/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.6031 - acc: 0.7777 - val_loss: 0.6405 - val_acc: 0.7620\n",
      "Epoch 167/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.6065 - acc: 0.7796 - val_loss: 0.6405 - val_acc: 0.7590\n",
      "Epoch 168/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.6075 - acc: 0.7777 - val_loss: 0.6406 - val_acc: 0.7590\n",
      "Epoch 169/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.6088 - acc: 0.7797 - val_loss: 0.6411 - val_acc: 0.7580\n",
      "Epoch 170/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.6128 - acc: 0.7719 - val_loss: 0.6401 - val_acc: 0.7590\n",
      "Epoch 171/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.6024 - acc: 0.7752 - val_loss: 0.6398 - val_acc: 0.7580\n",
      "Epoch 172/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.6075 - acc: 0.7740 - val_loss: 0.6398 - val_acc: 0.7600\n",
      "Epoch 173/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.6048 - acc: 0.7761 - val_loss: 0.6408 - val_acc: 0.7610\n",
      "Epoch 174/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.5910 - acc: 0.7907 - val_loss: 0.6400 - val_acc: 0.7580\n",
      "Epoch 175/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.5899 - acc: 0.7828 - val_loss: 0.6379 - val_acc: 0.7580\n",
      "Epoch 176/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.5907 - acc: 0.7873 - val_loss: 0.6385 - val_acc: 0.7560\n",
      "Epoch 177/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.5909 - acc: 0.7816 - val_loss: 0.6377 - val_acc: 0.7590\n",
      "Epoch 178/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.5903 - acc: 0.7813 - val_loss: 0.6366 - val_acc: 0.7580\n",
      "Epoch 179/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.5872 - acc: 0.7877 - val_loss: 0.6376 - val_acc: 0.7550\n",
      "Epoch 180/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.5864 - acc: 0.7847 - val_loss: 0.6376 - val_acc: 0.7560\n",
      "Epoch 181/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.5933 - acc: 0.7879 - val_loss: 0.6366 - val_acc: 0.7570\n",
      "Epoch 182/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.5746 - acc: 0.7871 - val_loss: 0.6375 - val_acc: 0.7560\n",
      "Epoch 183/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.5679 - acc: 0.7909 - val_loss: 0.6372 - val_acc: 0.7560\n",
      "Epoch 184/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.5718 - acc: 0.7883 - val_loss: 0.6355 - val_acc: 0.7570\n",
      "Epoch 185/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.5763 - acc: 0.7913 - val_loss: 0.6375 - val_acc: 0.7550\n",
      "Epoch 186/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.5596 - acc: 0.8005 - val_loss: 0.6357 - val_acc: 0.7570\n",
      "Epoch 187/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.5671 - acc: 0.7912 - val_loss: 0.6350 - val_acc: 0.7580\n",
      "Epoch 188/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.5632 - acc: 0.7991 - val_loss: 0.6341 - val_acc: 0.7600\n",
      "Epoch 189/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.5550 - acc: 0.7949 - val_loss: 0.6333 - val_acc: 0.7590\n",
      "Epoch 190/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.5683 - acc: 0.7900 - val_loss: 0.6327 - val_acc: 0.7600\n",
      "Epoch 191/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.5586 - acc: 0.7947 - val_loss: 0.6334 - val_acc: 0.7590\n",
      "Epoch 192/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.5655 - acc: 0.7933 - val_loss: 0.6335 - val_acc: 0.7560\n",
      "Epoch 193/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.5554 - acc: 0.7945 - val_loss: 0.6327 - val_acc: 0.7570\n",
      "Epoch 194/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.5542 - acc: 0.7975 - val_loss: 0.6348 - val_acc: 0.7580\n",
      "Epoch 195/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.5508 - acc: 0.7981 - val_loss: 0.6349 - val_acc: 0.7580\n",
      "Epoch 196/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.5409 - acc: 0.8012 - val_loss: 0.6324 - val_acc: 0.7620\n",
      "Epoch 197/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.5592 - acc: 0.7972 - val_loss: 0.6324 - val_acc: 0.7590\n",
      "Epoch 198/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.5487 - acc: 0.7977 - val_loss: 0.6332 - val_acc: 0.7570\n",
      "Epoch 199/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.5437 - acc: 0.7964 - val_loss: 0.6316 - val_acc: 0.7640\n",
      "Epoch 200/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.5494 - acc: 0.7993 - val_loss: 0.6310 - val_acc: 0.7590\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "dropout_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=200,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's check the results from `model.evaluate` to see how this change has affected our training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 43us/step\n",
      "1500/1500 [==============================] - 0s 45us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.36925017188787462, 0.88026666666666664]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train # Expected Results: [0.36925017188787462, 0.88026666666666664]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.69210424280166627, 0.74333333365122478]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test # Expected Results: [0.69210424280166627, 0.74333333365122478]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that the validation performance has improved again! However, the variance did become higher again, compared to L1-regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.  More Training Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another solution to high variance is to just get more data. We actually *have* more data, but took a subset of 10,000 units before. Let's now quadruple our data set, and see what happens. Note that we are really just lucky here, and getting more data isn't always possible, but this is a useful exercise in order to understand the power of big data sets.\n",
    "\n",
    "Run the cell below to preprocess our entire dataset, instead of just working with a subset of the data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "random.seed(123)\n",
    "df = df.sample(40000)\n",
    "df.index = range(40000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]\n",
    "\n",
    "#one-hot encoding of the complaints\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "sequences = tokenizer.texts_to_sequences(complaints)\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)\n",
    "\n",
    "#one-hot encoding of products\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "list(le.classes_)\n",
    "product_cat = le.transform(product) \n",
    "product_onehot = to_categorical(product_cat)\n",
    "\n",
    "# train test split\n",
    "test_index = random.sample(range(1,40000), 4000)\n",
    "test = one_hot_results[test_index]\n",
    "train = np.delete(one_hot_results, test_index, 0)\n",
    "label_test = product_onehot[test_index]\n",
    "label_train = np.delete(product_onehot, test_index, 0)\n",
    "\n",
    "#Validation set\n",
    "random.seed(123)\n",
    "val = train[:3000]\n",
    "train_final = train[3000:]\n",
    "label_val = label_train[:3000]\n",
    "label_train_final = label_train[3000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, build the first model that we built, without any regularization or dropout layers included. \n",
    "\n",
    "Train this model for 120 epochs.  All other hyperparameters should stay the same.  Store the fitted model inside of `moredata_model`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33000 samples, validate on 3000 samples\n",
      "Epoch 1/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 1.9255 - acc: 0.1838 - val_loss: 1.8950 - val_acc: 0.2300\n",
      "Epoch 2/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 1.8546 - acc: 0.2767 - val_loss: 1.8070 - val_acc: 0.3283\n",
      "Epoch 3/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 1.7242 - acc: 0.4103 - val_loss: 1.6337 - val_acc: 0.4763\n",
      "Epoch 4/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 1.5148 - acc: 0.5402 - val_loss: 1.4048 - val_acc: 0.5850\n",
      "Epoch 5/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 1.2823 - acc: 0.6213 - val_loss: 1.1900 - val_acc: 0.6370\n",
      "Epoch 6/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 1.0915 - acc: 0.6647 - val_loss: 1.0293 - val_acc: 0.6730\n",
      "Epoch 7/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.9572 - acc: 0.6901 - val_loss: 0.9224 - val_acc: 0.6880\n",
      "Epoch 8/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.8664 - acc: 0.7100 - val_loss: 0.8512 - val_acc: 0.6993\n",
      "Epoch 9/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.8035 - acc: 0.7236 - val_loss: 0.7997 - val_acc: 0.7173\n",
      "Epoch 10/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.7581 - acc: 0.7343 - val_loss: 0.7620 - val_acc: 0.7280\n",
      "Epoch 11/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.7232 - acc: 0.7425 - val_loss: 0.7326 - val_acc: 0.7343\n",
      "Epoch 12/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.6957 - acc: 0.7507 - val_loss: 0.7095 - val_acc: 0.7380\n",
      "Epoch 13/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.6730 - acc: 0.7570 - val_loss: 0.6909 - val_acc: 0.7477\n",
      "Epoch 14/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.6542 - acc: 0.7625 - val_loss: 0.6748 - val_acc: 0.7510\n",
      "Epoch 15/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.6380 - acc: 0.7677 - val_loss: 0.6612 - val_acc: 0.7557\n",
      "Epoch 16/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.6238 - acc: 0.7718 - val_loss: 0.6523 - val_acc: 0.7577\n",
      "Epoch 17/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.6111 - acc: 0.7749 - val_loss: 0.6427 - val_acc: 0.7613\n",
      "Epoch 18/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.5999 - acc: 0.7800 - val_loss: 0.6310 - val_acc: 0.7603\n",
      "Epoch 19/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.5893 - acc: 0.7828 - val_loss: 0.6221 - val_acc: 0.7670\n",
      "Epoch 20/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.5800 - acc: 0.7858 - val_loss: 0.6161 - val_acc: 0.7727\n",
      "Epoch 21/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.5709 - acc: 0.7899 - val_loss: 0.6092 - val_acc: 0.7750\n",
      "Epoch 22/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.5629 - acc: 0.7927 - val_loss: 0.6046 - val_acc: 0.7703\n",
      "Epoch 23/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.5554 - acc: 0.7966 - val_loss: 0.5978 - val_acc: 0.7780\n",
      "Epoch 24/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.5483 - acc: 0.7982 - val_loss: 0.5935 - val_acc: 0.7803\n",
      "Epoch 25/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.5415 - acc: 0.8010 - val_loss: 0.5909 - val_acc: 0.7817\n",
      "Epoch 26/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.5351 - acc: 0.8040 - val_loss: 0.5842 - val_acc: 0.7847\n",
      "Epoch 27/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.5290 - acc: 0.8057 - val_loss: 0.5808 - val_acc: 0.7887\n",
      "Epoch 28/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.5229 - acc: 0.8077 - val_loss: 0.5809 - val_acc: 0.7813\n",
      "Epoch 29/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.5179 - acc: 0.8102 - val_loss: 0.5748 - val_acc: 0.7923\n",
      "Epoch 30/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.5123 - acc: 0.8127 - val_loss: 0.5707 - val_acc: 0.7983\n",
      "Epoch 31/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.5075 - acc: 0.8152 - val_loss: 0.5697 - val_acc: 0.7943\n",
      "Epoch 32/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.5027 - acc: 0.8165 - val_loss: 0.5674 - val_acc: 0.7957\n",
      "Epoch 33/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.4978 - acc: 0.8182 - val_loss: 0.5637 - val_acc: 0.8013\n",
      "Epoch 34/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.4933 - acc: 0.8204 - val_loss: 0.5616 - val_acc: 0.7993\n",
      "Epoch 35/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.4888 - acc: 0.8211 - val_loss: 0.5612 - val_acc: 0.8000\n",
      "Epoch 36/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.4846 - acc: 0.8232 - val_loss: 0.5582 - val_acc: 0.8013\n",
      "Epoch 37/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.4803 - acc: 0.8262 - val_loss: 0.5579 - val_acc: 0.8033\n",
      "Epoch 38/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.4763 - acc: 0.8269 - val_loss: 0.5551 - val_acc: 0.8053\n",
      "Epoch 39/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.4725 - acc: 0.8287 - val_loss: 0.5551 - val_acc: 0.8017\n",
      "Epoch 40/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.4689 - acc: 0.8302 - val_loss: 0.5539 - val_acc: 0.8053\n",
      "Epoch 41/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.4654 - acc: 0.8318 - val_loss: 0.5524 - val_acc: 0.8063\n",
      "Epoch 42/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.4619 - acc: 0.8336 - val_loss: 0.5509 - val_acc: 0.8080\n",
      "Epoch 43/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.4584 - acc: 0.8348 - val_loss: 0.5512 - val_acc: 0.8067\n",
      "Epoch 44/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.4552 - acc: 0.8365 - val_loss: 0.5504 - val_acc: 0.8053\n",
      "Epoch 45/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.4518 - acc: 0.8374 - val_loss: 0.5514 - val_acc: 0.8053\n",
      "Epoch 46/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.4487 - acc: 0.8392 - val_loss: 0.5479 - val_acc: 0.8123\n",
      "Epoch 47/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.4456 - acc: 0.8407 - val_loss: 0.5474 - val_acc: 0.8077\n",
      "Epoch 48/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.4424 - acc: 0.8423 - val_loss: 0.5455 - val_acc: 0.8127\n",
      "Epoch 49/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.4395 - acc: 0.8424 - val_loss: 0.5486 - val_acc: 0.8113\n",
      "Epoch 50/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.4368 - acc: 0.8434 - val_loss: 0.5443 - val_acc: 0.8117\n",
      "Epoch 51/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.4341 - acc: 0.8450 - val_loss: 0.5445 - val_acc: 0.8150\n",
      "Epoch 52/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.4307 - acc: 0.8462 - val_loss: 0.5440 - val_acc: 0.8137\n",
      "Epoch 53/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.4284 - acc: 0.8476 - val_loss: 0.5454 - val_acc: 0.8123\n",
      "Epoch 54/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.4258 - acc: 0.8486 - val_loss: 0.5428 - val_acc: 0.8153\n",
      "Epoch 55/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.4232 - acc: 0.8501 - val_loss: 0.5432 - val_acc: 0.8147\n",
      "Epoch 56/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.4206 - acc: 0.8502 - val_loss: 0.5428 - val_acc: 0.8120\n",
      "Epoch 57/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.4181 - acc: 0.8518 - val_loss: 0.5457 - val_acc: 0.8110\n",
      "Epoch 58/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.4157 - acc: 0.8521 - val_loss: 0.5434 - val_acc: 0.8127\n",
      "Epoch 59/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.4131 - acc: 0.8533 - val_loss: 0.5439 - val_acc: 0.8147\n",
      "Epoch 60/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.4108 - acc: 0.8550 - val_loss: 0.5438 - val_acc: 0.8130\n",
      "Epoch 61/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.4085 - acc: 0.8554 - val_loss: 0.5429 - val_acc: 0.8110\n",
      "Epoch 62/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.4065 - acc: 0.8571 - val_loss: 0.5444 - val_acc: 0.8107\n",
      "Epoch 63/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.4041 - acc: 0.8565 - val_loss: 0.5431 - val_acc: 0.8083\n",
      "Epoch 64/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.4020 - acc: 0.8582 - val_loss: 0.5450 - val_acc: 0.8103\n",
      "Epoch 65/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3998 - acc: 0.8584 - val_loss: 0.5436 - val_acc: 0.8123\n",
      "Epoch 66/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3977 - acc: 0.8593 - val_loss: 0.5443 - val_acc: 0.8103\n",
      "Epoch 67/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3960 - acc: 0.8601 - val_loss: 0.5444 - val_acc: 0.8093\n",
      "Epoch 68/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3935 - acc: 0.8610 - val_loss: 0.5457 - val_acc: 0.8103\n",
      "Epoch 69/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3915 - acc: 0.8623 - val_loss: 0.5445 - val_acc: 0.8103\n",
      "Epoch 70/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3895 - acc: 0.8633 - val_loss: 0.5447 - val_acc: 0.8110\n",
      "Epoch 71/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3878 - acc: 0.8635 - val_loss: 0.5450 - val_acc: 0.8120\n",
      "Epoch 72/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3856 - acc: 0.8643 - val_loss: 0.5474 - val_acc: 0.8120\n",
      "Epoch 73/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3838 - acc: 0.8651 - val_loss: 0.5469 - val_acc: 0.8100\n",
      "Epoch 74/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3819 - acc: 0.8648 - val_loss: 0.5467 - val_acc: 0.8100\n",
      "Epoch 75/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3801 - acc: 0.8665 - val_loss: 0.5477 - val_acc: 0.8130\n",
      "Epoch 76/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3783 - acc: 0.8671 - val_loss: 0.5466 - val_acc: 0.8107\n",
      "Epoch 77/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3765 - acc: 0.8683 - val_loss: 0.5498 - val_acc: 0.8087\n",
      "Epoch 78/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.3745 - acc: 0.8684 - val_loss: 0.5479 - val_acc: 0.8110\n",
      "Epoch 79/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3730 - acc: 0.8686 - val_loss: 0.5487 - val_acc: 0.8097\n",
      "Epoch 80/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3712 - acc: 0.8702 - val_loss: 0.5499 - val_acc: 0.8100\n",
      "Epoch 81/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3694 - acc: 0.8702 - val_loss: 0.5500 - val_acc: 0.8097\n",
      "Epoch 82/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3681 - acc: 0.8706 - val_loss: 0.5506 - val_acc: 0.8057\n",
      "Epoch 83/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3660 - acc: 0.8717 - val_loss: 0.5532 - val_acc: 0.8107\n",
      "Epoch 84/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.3646 - acc: 0.8708 - val_loss: 0.5506 - val_acc: 0.8063\n",
      "Epoch 85/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.3629 - acc: 0.8725 - val_loss: 0.5551 - val_acc: 0.8060\n",
      "Epoch 86/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3614 - acc: 0.8726 - val_loss: 0.5551 - val_acc: 0.8067\n",
      "Epoch 87/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3597 - acc: 0.8734 - val_loss: 0.5524 - val_acc: 0.8047\n",
      "Epoch 88/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3583 - acc: 0.8745 - val_loss: 0.5541 - val_acc: 0.8090\n",
      "Epoch 89/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3568 - acc: 0.8739 - val_loss: 0.5550 - val_acc: 0.8060\n",
      "Epoch 90/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.3552 - acc: 0.8741 - val_loss: 0.5568 - val_acc: 0.8083\n",
      "Epoch 91/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3538 - acc: 0.8756 - val_loss: 0.5556 - val_acc: 0.8077\n",
      "Epoch 92/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.3523 - acc: 0.8756 - val_loss: 0.5560 - val_acc: 0.8100\n",
      "Epoch 93/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3506 - acc: 0.8768 - val_loss: 0.5600 - val_acc: 0.8080\n",
      "Epoch 94/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3491 - acc: 0.8776 - val_loss: 0.5611 - val_acc: 0.8067\n",
      "Epoch 95/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3478 - acc: 0.8780 - val_loss: 0.5583 - val_acc: 0.8077\n",
      "Epoch 96/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3465 - acc: 0.8795 - val_loss: 0.5584 - val_acc: 0.8077\n",
      "Epoch 97/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3445 - acc: 0.8789 - val_loss: 0.5619 - val_acc: 0.8037\n",
      "Epoch 98/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3435 - acc: 0.8790 - val_loss: 0.5630 - val_acc: 0.8037\n",
      "Epoch 99/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3423 - acc: 0.8791 - val_loss: 0.5624 - val_acc: 0.8063\n",
      "Epoch 100/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3405 - acc: 0.8802 - val_loss: 0.5632 - val_acc: 0.8047\n",
      "Epoch 101/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3391 - acc: 0.8818 - val_loss: 0.5638 - val_acc: 0.8063\n",
      "Epoch 102/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3384 - acc: 0.8813 - val_loss: 0.5632 - val_acc: 0.8073\n",
      "Epoch 103/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3370 - acc: 0.8820 - val_loss: 0.5654 - val_acc: 0.8027\n",
      "Epoch 104/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3354 - acc: 0.8820 - val_loss: 0.5707 - val_acc: 0.8030\n",
      "Epoch 105/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3346 - acc: 0.8831 - val_loss: 0.5665 - val_acc: 0.8057\n",
      "Epoch 106/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3330 - acc: 0.8831 - val_loss: 0.5668 - val_acc: 0.8073\n",
      "Epoch 107/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3318 - acc: 0.8832 - val_loss: 0.5688 - val_acc: 0.8023\n",
      "Epoch 108/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3303 - acc: 0.8846 - val_loss: 0.5689 - val_acc: 0.8053\n",
      "Epoch 109/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3292 - acc: 0.8844 - val_loss: 0.5733 - val_acc: 0.8030\n",
      "Epoch 110/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3278 - acc: 0.8854 - val_loss: 0.5737 - val_acc: 0.8020\n",
      "Epoch 111/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3266 - acc: 0.8859 - val_loss: 0.5779 - val_acc: 0.8013\n",
      "Epoch 112/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3255 - acc: 0.8864 - val_loss: 0.5744 - val_acc: 0.8057\n",
      "Epoch 113/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3241 - acc: 0.8859 - val_loss: 0.5741 - val_acc: 0.8017\n",
      "Epoch 114/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3229 - acc: 0.8872 - val_loss: 0.5760 - val_acc: 0.8037\n",
      "Epoch 115/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3216 - acc: 0.8881 - val_loss: 0.5793 - val_acc: 0.8010\n",
      "Epoch 116/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3205 - acc: 0.8874 - val_loss: 0.5769 - val_acc: 0.8027\n",
      "Epoch 117/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3194 - acc: 0.8886 - val_loss: 0.5840 - val_acc: 0.8017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3181 - acc: 0.8882 - val_loss: 0.5783 - val_acc: 0.8027\n",
      "Epoch 119/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.3170 - acc: 0.8886 - val_loss: 0.5829 - val_acc: 0.8010\n",
      "Epoch 120/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3158 - acc: 0.8901 - val_loss: 0.5795 - val_acc: 0.8037\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "moredata_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, finally, let's check the results returned from `model.evaluate()` to see how this model stacks up with the other techniques we've used.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 1s 38us/step\n",
      "4000/4000 [==============================] - 0s 41us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.31160746300942971, 0.89160606060606062]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train # Expected Output:  [0.31160746300942971, 0.89160606060606062]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.56076071488857271, 0.8145]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test # Expected Output: [0.56076071488857271, 0.8145]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same amount of epochs, we were able to get a fairly similar validation accuracy of 89.1%. Our test set accuracy went up from ~75% to a staggering 81.45% though, without any other regularization technique. You can still consider early stopping, L1, L2 and dropout here. It's clear that having more data has a strong impact on model performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Consumer_complaints.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://catalog.data.gov/dataset/consumer-complaint-database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
